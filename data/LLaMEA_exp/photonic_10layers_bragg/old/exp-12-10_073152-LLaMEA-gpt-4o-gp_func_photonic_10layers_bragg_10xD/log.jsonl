{"id": "cdb5e26e-08dc-4e09-8a9c-f5c56adc4095", "fitness": 0.05167746828009215, "name": "AdaptiveRandomSearch", "description": "A novel Adaptive Random Search algorithm that dynamically adjusts exploration and exploitation balance based on the optimization progress.", "code": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        \n        while self.evaluations < self.budget:\n            step_size = (ub - lb) / (self.evaluations + 1)\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n        \n        return current_best", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05168 with standard deviation 0.00238.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05390338336748324, 0.0514147121274785, 0.05402868283080142, 0.04947311627172879, 0.047172078604268974, 0.04958852674750913, 0.05396082531880875, 0.05146963142976391, 0.054086257822986594]}}
{"id": "e87ce556-4008-467b-8d00-b15297d1eab6", "fitness": 0.05076778510239399, "name": "DualPhaseAdaptiveRandomSearch", "description": "A Dual-Phase Adaptive Random Search algorithm that begins with global exploration and transitions to local exploitation, adjusting step sizes dynamically for enhanced convergence.", "code": "import numpy as np\n\nclass DualPhaseAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n\n        phase_transition = self.budget // 2\n        \n        while self.evaluations < self.budget:\n            if self.evaluations < phase_transition:\n                # Global exploration phase\n                step_size = (ub - lb) * (1 - self.evaluations / self.budget)\n            else:\n                # Local exploitation phase\n                step_size = (ub - lb) / (self.evaluations - phase_transition + 1)\n\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n        \n        return current_best", "configspace": "", "generation": 1, "feedback": "The algorithm DualPhaseAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05077 with standard deviation 0.00299.", "error": "", "parent_ids": ["cdb5e26e-08dc-4e09-8a9c-f5c56adc4095"], "operator": null, "metadata": {"aucs": [0.052007735930563515, 0.055020746152145805, 0.049528270724015, 0.04771338128212976, 0.05046678308953301, 0.04544891564297626, 0.05206336118275434, 0.055079897833754554, 0.04958097408367368]}}
{"id": "e570f81a-1bef-45c1-bcec-a16616be73e5", "fitness": 0.05307753006019077, "name": "AdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with dynamic step size adjustment based on improvement history.", "code": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        improvement_factor = 1.0\n        \n        while self.evaluations < self.budget:\n            step_size = (ub - lb) / (self.evaluations + 1) * improvement_factor\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n                improvement_factor = 1.0  # Reset factor on improvement\n            else:\n                improvement_factor *= 0.95  # Decrease factor if no improvement\n        \n        return current_best", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05308 with standard deviation 0.00401.", "error": "", "parent_ids": ["cdb5e26e-08dc-4e09-8a9c-f5c56adc4095"], "operator": null, "metadata": {"aucs": [0.05709671502801883, 0.049637875463256154, 0.05694400409760736, 0.05238705770665919, 0.04552200049486155, 0.05225611467456903, 0.05715796125174899, 0.0496911001421273, 0.057004941682868515]}}
{"id": "ff22033c-e3f8-4d9f-adf7-88173bee8bc7", "fitness": 0.05167746828009215, "name": "ImprovedAdaptiveRandomSearch", "description": "Adaptive Random Search with dynamic step size adjustment using success ratio tracking for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        success_count = 0\n        adaptation_threshold = 10  # Number of evaluations before adaptation\n\n        while self.evaluations < self.budget:\n            step_size = (ub - lb) / (self.evaluations + 1)\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n                success_count += 1\n        \n            # Adjust step size based on success ratio\n            if self.evaluations % adaptation_threshold == 0:\n                success_ratio = success_count / adaptation_threshold\n                if success_ratio < 0.2:\n                    step_size *= 0.9  # Reduce step size\n                elif success_ratio > 0.8:\n                    step_size *= 1.1  # Increase step size\n                success_count = 0  # Reset success count after adaptation\n\n        return current_best", "configspace": "", "generation": 3, "feedback": "The algorithm ImprovedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05168 with standard deviation 0.00238.", "error": "", "parent_ids": ["e570f81a-1bef-45c1-bcec-a16616be73e5"], "operator": null, "metadata": {"aucs": [0.05390338336748324, 0.0514147121274785, 0.05402868283080142, 0.04947311627172879, 0.047172078604268974, 0.04958852674750913, 0.05396082531880875, 0.05146963142976391, 0.054086257822986594]}}
{"id": "84197810-f91d-47b2-a263-f427cae977b2", "fitness": 0.054310480014944575, "name": "EnhancedRandomSearch", "description": "Enhanced Random Search with stochastic restart and step size adaptation based on stagnation detection and diversity promotion.", "code": "import numpy as np\n\nclass EnhancedRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        no_improvement_count = 0\n\n        while self.evaluations < self.budget:\n            step_size = (ub - lb) * (0.5 ** no_improvement_count)\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n                no_improvement_count = 0  # Reset on improvement\n            else:\n                no_improvement_count += 1  # Increment if no improvement\n\n            if no_improvement_count >= 10:\n                current_best = np.random.uniform(lb, ub)  # Stochastic restart\n                current_best_value = func(current_best)\n                self.evaluations += 1\n                no_improvement_count = 0\n\n        return current_best", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05431 with standard deviation 0.00451.", "error": "", "parent_ids": ["e570f81a-1bef-45c1-bcec-a16616be73e5"], "operator": null, "metadata": {"aucs": [0.060359596959325024, 0.05660436268407254, 0.05053146484793425, 0.05532234558910021, 0.05193052196857795, 0.046370268961823946, 0.06042535875846855, 0.05666512546034175, 0.05058527490485698]}}
{"id": "e39ee8cd-c5f8-4ac6-bc5d-f763549d201c", "fitness": 0.05291842946118191, "name": "EnhancedRandomSearch", "description": "Enhanced Random Search with dynamic step size adjustment and stochastic restart based on improvement threshold.", "code": "import numpy as np\n\nclass EnhancedRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        no_improvement_count = 0\n\n        while self.evaluations < self.budget:\n            step_size = (ub - lb) * (0.7 ** no_improvement_count)\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n                no_improvement_count = 0  # Reset on improvement\n            else:\n                no_improvement_count += 1  # Increment if no improvement\n\n            if no_improvement_count >= 8:\n                current_best = np.random.uniform(lb, ub)  # Stochastic restart\n                current_best_value = func(current_best)\n                self.evaluations += 1\n                no_improvement_count = 0\n\n        return current_best", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05292 with standard deviation 0.00221.", "error": "", "parent_ids": ["84197810-f91d-47b2-a263-f427cae977b2"], "operator": null, "metadata": {"aucs": [0.05518819174591494, 0.05371299121894069, 0.05428803802011273, 0.05064596890390294, 0.04924894145056191, 0.049817595863480335, 0.05524715628417465, 0.05377094803447857, 0.05434603362907042]}}
{"id": "aa83d1c2-8cfc-4805-8e3c-c04b2cd5fb3b", "fitness": 0.04929510056355172, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with dynamic step size reduction, elite selection, and adaptive restart based on performance metrics.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        no_improvement_count = 0\n        elite_candidate = current_best\n        elite_value = current_best_value\n\n        while self.evaluations < self.budget:\n            step_size = (ub - lb) * (0.5 ** (no_improvement_count / 2))\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n                no_improvement_count = 0  # Reset on improvement\n\n                # Update elite candidate\n                if candidate_value < elite_value:\n                    elite_candidate = candidate\n                    elite_value = candidate_value\n            else:\n                no_improvement_count += 1  # Increment if no improvement\n\n            if no_improvement_count >= 10:\n                # Adaptive stochastic restart\n                if elite_value < current_best_value:\n                    current_best = elite_candidate\n                    current_best_value = elite_value\n                else:\n                    current_best = np.random.uniform(lb, ub)\n                    current_best_value = func(current_best)\n                    self.evaluations += 1\n                no_improvement_count = 0\n\n        return current_best", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04930 with standard deviation 0.00288.", "error": "", "parent_ids": ["84197810-f91d-47b2-a263-f427cae977b2"], "operator": null, "metadata": {"aucs": [0.053689571156321114, 0.04939647829168525, 0.048925319499885545, 0.049277218664659284, 0.04530222175336662, 0.044891503334355765, 0.05374676969631709, 0.04944941122089408, 0.04897741145448076]}}
{"id": "98bd76fb-42b1-473b-b306-c84761336205", "fitness": 0.04750258246700637, "name": "EnhancedRandomSearch", "description": "Enhanced Random Search with adaptive mutation step size using diversity feedback and stochastic restart.", "code": "import numpy as np\n\nclass EnhancedRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        no_improvement_count = 0\n\n        while self.evaluations < self.budget:\n            diversity_factor = np.std(np.random.uniform(lb, ub, (10, self.dim)), axis=0)\n            step_size = (ub - lb) * (0.5 ** no_improvement_count) * (1 + diversity_factor.mean())\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n                no_improvement_count = 0  # Reset on improvement\n            else:\n                no_improvement_count += 1  # Increment if no improvement\n\n            if no_improvement_count >= 10:\n                current_best = np.random.uniform(lb, ub)  # Stochastic restart\n                current_best_value = func(current_best)\n                self.evaluations += 1\n                no_improvement_count = 0\n\n        return current_best", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04750 with standard deviation 0.00245.", "error": "", "parent_ids": ["84197810-f91d-47b2-a263-f427cae977b2"], "operator": null, "metadata": {"aucs": [0.046836780916080456, 0.05065287839344146, 0.04901485573309727, 0.042942233627409676, 0.046456271435330376, 0.04495874217441009, 0.04688702227516539, 0.05070719290059689, 0.04906726474752576]}}
{"id": "22bd7356-0883-47e1-8ea2-8fbaaaf97c18", "fitness": -Infinity, "name": "AdaptiveGradientAssistedRandomSearch", "description": "Adaptive Gradient-Assisted Random Search with dynamic step size modulation and intelligent restart strategies to enhance exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveGradientAssistedRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        no_improvement_count = 0\n        step_size = (ub - lb) * 0.1  # Initial step size\n\n        while self.evaluations < self.budget:\n            # Calculate gradient approximation\n            grad = np.zeros(self.dim)\n            epsilon = 1e-8\n            for i in range(self.dim):\n                perturb = np.zeros(self.dim)\n                perturb[i] = epsilon\n                grad[i] = (func(current_best + perturb) - current_best_value) / epsilon\n                self.evaluations += 1\n                if self.evaluations >= self.budget:\n                    return current_best\n            \n            # Modulate step size based on improvement\n            candidate = np.clip(current_best - step_size * grad + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n                no_improvement_count = 0\n                step_size = min(step_size * 1.2, ub - lb)  # Increase step size on improvement\n            else:\n                no_improvement_count += 1\n                step_size *= 0.5  # Decrease step size\n            \n            if no_improvement_count >= 15:\n                current_best = np.random.uniform(lb, ub)  # Intelligent restart\n                current_best_value = func(current_best)\n                self.evaluations += 1\n                no_improvement_count = 0\n                step_size = (ub - lb) * 0.1  # Reset step size\n\n        return current_best", "configspace": "", "generation": 8, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["84197810-f91d-47b2-a263-f427cae977b2"], "operator": null, "metadata": {}}
{"id": "10fee946-b81c-4192-a18d-9b1f5d28d6e1", "fitness": 0.0528571068853518, "name": "AdaptiveRandomSearch", "description": "Adaptive Random Search with dynamic neighborhood adjustment and feedback-driven restart mechanism for robust black box function optimization.", "code": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        no_improvement_count = 0\n        max_stagnation = 10\n\n        while self.evaluations < self.budget:\n            adaptive_factor = 1 + 0.1 * (self.budget - self.evaluations) / self.budget\n            step_size = (ub - lb) * (0.5 ** no_improvement_count) * adaptive_factor\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n                no_improvement_count = 0\n            else:\n                no_improvement_count += 1\n\n            if no_improvement_count >= max_stagnation:\n                current_best = np.random.uniform(lb, ub)\n                current_best_value = func(current_best)\n                self.evaluations += 1\n                max_stagnation = int(max_stagnation * 0.9)\n                no_improvement_count = 0\n\n        return current_best", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05286 with standard deviation 0.00213.", "error": "", "parent_ids": ["84197810-f91d-47b2-a263-f427cae977b2"], "operator": null, "metadata": {"aucs": [0.054560380986962365, 0.05415946094658064, 0.05427719576169876, 0.05006606273305858, 0.04968280642291378, 0.04979645410369815, 0.05461870066848351, 0.054217557368714475, 0.05433534297605591]}}
{"id": "7ddad9bc-4e43-4a9d-a239-70178a09af0a", "fitness": 0.04997904533509615, "name": "EnhancedRandomSearch", "description": "Enhanced Random Search with adaptive exploration-exploitation trade-off and improved restart strategy.", "code": "import numpy as np\n\nclass EnhancedRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        current_best = np.random.uniform(lb, ub)\n        current_best_value = func(current_best)\n        self.evaluations += 1\n        no_improvement_count = 0\n\n        while self.evaluations < self.budget:\n            step_size = (ub - lb) * (0.5 ** (no_improvement_count / 2))\n            candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < current_best_value:\n                current_best = candidate\n                current_best_value = candidate_value\n                no_improvement_count = 0  # Reset on improvement\n            else:\n                no_improvement_count += 1  # Increment if no improvement\n\n            if no_improvement_count >= 10:\n                current_best = np.random.normal(lb, ub)  # Improved stochastic restart\n                current_best_value = func(current_best)\n                self.evaluations += 1\n                no_improvement_count = 0\n\n        return current_best", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04998 with standard deviation 0.00331.", "error": "", "parent_ids": ["84197810-f91d-47b2-a263-f427cae977b2"], "operator": null, "metadata": {"aucs": [0.055056485363409235, 0.04864730209293544, 0.05041555973166478, 0.05052617601600973, 0.04462795394171082, 0.04625405198784771, 0.05511528718373937, 0.04869920591597632, 0.050469385782571985]}}
{"id": "db44182e-8522-4316-8e33-fdf55de672a9", "fitness": 0.05438808443707596, "name": "AdaptiveRandomSearch", "description": "Adaptive Random Search with dynamic step adaptation and multi-start strategy to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 10\n            adaptive_step_size = (ub - lb) / 10  # Initial step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.1  # Increase step size on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Decrease step size on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    break\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n        return best_solution", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05439 with standard deviation 0.00477.", "error": "", "parent_ids": ["84197810-f91d-47b2-a263-f427cae977b2"], "operator": null, "metadata": {"aucs": [0.05873493090928039, 0.04976218406836319, 0.05922896684427126, 0.05387876677385994, 0.04566092387949283, 0.05432069701453057, 0.05879817274510368, 0.049815186341665596, 0.05929293135711622]}}
{"id": "7e8d880a-03a5-40a3-a9ee-1ced19531574", "fitness": 0.05321284186797844, "name": "AdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with dynamic exploration scaling based on performance history to refine search efficiency.", "code": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 10\n            adaptive_step_size = (ub - lb) / 10  # Initial step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increase step size on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85  # Decrease step size on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    break\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n        return best_solution", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05321 with standard deviation 0.00420.", "error": "", "parent_ids": ["db44182e-8522-4316-8e33-fdf55de672a9"], "operator": null, "metadata": {"aucs": [0.05805953947023601, 0.04953037630357726, 0.056511065060177, 0.05324341167187652, 0.045444063498327414, 0.051850030260031765, 0.05812225953697758, 0.04958318065559231, 0.056571650355010106]}}
{"id": "8613a2ad-4b5b-4b9f-87c1-373e449ab85a", "fitness": 0.05776676276620943, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with dynamic multi-scale step adaptation and restart strategy to improve convergence rate and solution quality by balancing exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increase step size on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.7  # Decrease step size on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Restart with smaller step size\n\n        return best_solution", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05777 with standard deviation 0.00901.", "error": "", "parent_ids": ["db44182e-8522-4316-8e33-fdf55de672a9"], "operator": null, "metadata": {"aucs": [0.07173788499821887, 0.05057295426703379, 0.05594438025641435, 0.06544665067658839, 0.04641045486652573, 0.05133633462643272, 0.07182118643576363, 0.050626778888261614, 0.056004239880645734]}}
{"id": "b5856692-0382-4c50-ab2f-82d115f87874", "fitness": 0.06148987117858377, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with improved adaptive step size scaling and dynamic adjustment to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = (ub - lb) / 12  # Restart with even smaller step size\n\n        return best_solution", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06149 with standard deviation 0.00734.", "error": "", "parent_ids": ["8613a2ad-4b5b-4b9f-87c1-373e449ab85a"], "operator": null, "metadata": {"aucs": [0.06380646049162553, 0.054315145952639754, 0.07171356337008106, 0.05838611078550049, 0.04982646412035152, 0.0653114856565209, 0.06387757741855449, 0.05437340480211561, 0.07179862800986458]}}
{"id": "07175fa3-3286-4a06-abab-d7dbcb502bb6", "fitness": 0.05906852920415321, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with progressive adaptive step size scaling for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.5  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = (ub - lb) / 12  # Restart with even smaller step size\n\n        return best_solution", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05907 with standard deviation 0.00579.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.05694889301460071, 0.06845540274252071, 0.05687310451665695, 0.05223840929053647, 0.06246860230314766, 0.0521531179352589, 0.05701016723588015, 0.06853454033677375, 0.056934525462003616]}}
{"id": "893df0dc-d21e-40e3-b167-b8bb5e230827", "fitness": 0.052392266846328264, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with refined adaptive step size scaling and periodic reinitialization for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.7  # Changed from 0.5 to 0.7 for smoother reduction\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Periodic reinitialization strategy for enhanced exploration\n            adaptive_step_size = (ub - lb) / 15  # Restart with a smaller step size\n\n        return best_solution", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05239 with standard deviation 0.00560.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06139520418499944, 0.049662291059228036, 0.05053169443610772, 0.05625013859071126, 0.04555517363808159, 0.04637257423876984, 0.06146246475438877, 0.04971538663219466, 0.05058547408247305]}}
{"id": "02409389-9908-4ae6-9346-a7191fb56b47", "fitness": 0.055443916639898165, "name": "AdvancedAdaptiveRandomSearch", "description": "Advanced Adaptive Random Search with dynamic exploration-exploitation balancing and adaptive learning rate to enhance convergence efficiency.", "code": "import numpy as np\n\nclass AdvancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n            learning_rate = 0.1  # New parameter for adaptive learning\n\n            while self.evaluations < self.budget:\n                exploration_factor = np.random.randn(self.dim) * adaptive_step_size\n                candidate = np.clip(current_best + exploration_factor, lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                    learning_rate *= 1.05  # Increase learning rate on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n                    learning_rate *= 0.95  # Decrease learning rate on no improvement\n                \n                adaptive_step_size = np.clip(adaptive_step_size, (ub - lb) / 100, (ub - lb) / 2)\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 12\n\n        return best_solution", "configspace": "", "generation": 17, "feedback": "The algorithm AdvancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05544 with standard deviation 0.00436.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.05491949110250649, 0.0623899642864012, 0.05368166431840915, 0.050403324150104956, 0.05718042137238033, 0.049245001138526034, 0.054978097704195594, 0.06245806535610232, 0.053739220330457416]}}
{"id": "740b955f-5553-43dd-a07a-5ecae0be622f", "fitness": 0.06148987117858377, "name": "EnhancedAdaptiveRandomSearch", "description": "Refined Enhanced Adaptive Random Search with dynamic iteration budget allocation and adaptive solution archive for improved global exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        archive = []\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n                if self.evaluations % (self.budget // 5) == 0:  # Dynamic budget allocation\n                    archive.append((current_best, current_best_value))\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 12\n\n            if archive:\n                archive_best = min(archive, key=lambda x: x[1])[0]\n                if np.random.rand() < 0.5:\n                    current_best = archive_best\n                    adaptive_step_size = (ub - lb) / 15  # Refined step size from archive solution\n\n        return best_solution", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06149 with standard deviation 0.00734.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06380646049162553, 0.054315145952639754, 0.07171356337008106, 0.05838611078550049, 0.04982646412035152, 0.0653114856565209, 0.06387757741855449, 0.05437340480211561, 0.07179862800986458]}}
{"id": "ed5e2d5a-82e1-4a95-bf4f-09152da39581", "fitness": 0.05277686794892896, "name": "ImprovedEnhancedAdaptiveRandomSearch", "description": "Improved Enhanced Adaptive Random Search with dynamic mutation control and a restart mechanism based on diversity to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass ImprovedEnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n            diversity_threshold = 0.2 * np.mean(ub - lb)  # Diversity threshold for restart\n            \n            while self.evaluations < self.budget:\n                mutation_scale = np.random.uniform(0.5, 1.5)  # Dynamic mutation scaling\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim) * mutation_scale, lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n                \n                # Check diversity to decide on restart\n                if np.linalg.norm(current_best - candidate) < diversity_threshold:\n                    break\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy with diversity-aware condition\n            adaptive_step_size = (ub - lb) / 15  # Restart with smaller step size for refined exploration\n\n        return best_solution", "configspace": "", "generation": 19, "feedback": "The algorithm ImprovedEnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05278 with standard deviation 0.00358.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.051967786997071386, 0.052342397773055716, 0.058445149614085756, 0.04768646725135861, 0.04802992411585538, 0.05359019946719645, 0.05202322461248088, 0.05239825667402909, 0.05850840503522736]}}
{"id": "89a9df39-6631-4c5c-82a5-9198a75b90d5", "fitness": 0.04118620975005222, "name": "AdaptiveGradientBoostedSearch", "description": "Adaptive Gradient Boosted Search combining random exploration with gradient estimation to refine search direction and improve convergence.", "code": "import numpy as np\n\nclass AdaptiveGradientBoostedSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n\n            while self.evaluations < self.budget:\n                # Gradient estimation\n                gradient = np.zeros(self.dim)\n                epsilon = 1e-5\n                for i in range(self.dim):\n                    perturb = np.zeros(self.dim)\n                    perturb[i] = epsilon\n                    forward = np.clip(current_best + perturb, lb, ub)\n                    backward = np.clip(current_best - perturb, lb, ub)\n                    gradient[i] = (func(forward) - func(backward)) / (2 * epsilon)\n                    self.evaluations += 2\n                    if self.evaluations >= self.budget:\n                        break\n\n                # Combine random exploration with gradient direction\n                search_direction = np.random.uniform(-1, 1, self.dim) + gradient\n                candidate = np.clip(current_best + adaptive_step_size * search_direction, lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n                if self.evaluations >= self.budget:\n                    break\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 12\n\n        return best_solution", "configspace": "", "generation": 20, "feedback": "The algorithm AdaptiveGradientBoostedSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04119 with standard deviation 0.00331.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.0431691314893472, 0.0454467776094819, 0.03848214137748451, 0.03951952488006938, 0.04165035620843727, 0.035171552781818716, 0.0432161170597698, 0.045495703638193, 0.03852458270586823]}}
{"id": "da2ca456-991f-402a-af77-f378a751d67d", "fitness": -Infinity, "name": "RefinedAdaptiveRandomSearch", "description": "Refined Adaptive Random Search with enhanced memory-based adaptive step size and strategic multi-start restarts for improved convergence.", "code": "import numpy as np\n\nclass RefinedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Memory for tracking progress\n        global_best = None\n        global_best_value = float('inf')\n        adaptive_step_memory = []\n\n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n                # Memory-based step size adjustment\n                adaptive_step_memory.append(adaptive_step_size)\n                if len(adaptive_step_memory) > 10:\n                    adaptive_step_memory.pop(0)\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Global best memory update\n            if current_best_value < global_best_value:\n                global_best = current_best\n                global_best_value = current_best_value\n\n            # Multi-start strategy with strategic restarts\n            if np.mean(adaptive_step_memory) < (ub - lb) / 15:\n                # If step size becomes too small, trigger a strategic restart\n                current_best = np.random.uniform(lb, ub)\n                adaptive_step_size = (ub - lb) / 10\n                adaptive_step_memory.clear()\n\n        return best_solution", "configspace": "", "generation": 21, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {}}
{"id": "e068b974-d4f3-44d7-842a-203e0cf1ec38", "fitness": 0.06148987117858377, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with improved restart strategy to fine-tune exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Slightly larger restart step size for exploration\n\n        return best_solution", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06149 with standard deviation 0.00734.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06380646049162553, 0.054315145952639754, 0.07171356337008106, 0.05838611078550049, 0.04982646412035152, 0.0653114856565209, 0.06387757741855449, 0.05437340480211561, 0.07179862800986458]}}
{"id": "89af839b-c116-4f6a-891a-c391b80437e1", "fitness": -Infinity, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with improved step size reset strategy for better convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size = max(reduction_factor * adaptive_step_size, (ub - lb) / 20)  # Ensure step size does not shrink excessively\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10  # Restart with a balanced step size\n\n        return best_solution", "configspace": "", "generation": 23, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {}}
{"id": "86a9e0d9-f090-4028-b2b1-d5316c87bbb9", "fitness": 0.06148987117858377, "name": "DynamicBoundedExplorativeSearch", "description": "Dynamic Bounded Explorative Search enhances exploitation with adaptive boundary checks and progressive learning of promising regions for improved convergence.", "code": "import numpy as np\n\nclass DynamicBoundedExplorativeSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = np.random.uniform(lb, ub)\n        best_value = func(best_solution)\n        self.evaluations += 1\n        \n        adaptive_step_size = (ub - lb) / 5\n        reduction_factor = 0.5\n        expansion_factor = 1.3\n        no_improvement_count = 0\n        max_no_improvement = 5\n        exploration_factor = 0.3  # Factor to scale exploration steps\n\n        learned_bounds_lb = lb.copy()\n        learned_bounds_ub = ub.copy()\n\n        while self.evaluations < self.budget:\n            candidate = np.clip(best_solution + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), learned_bounds_lb, learned_bounds_ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n            \n            if candidate_value < best_value:\n                best_solution = candidate\n                best_value = candidate_value\n                no_improvement_count = 0\n                adaptive_step_size *= expansion_factor\n                # Update learned bounds based on new best\n                learning_adjustment = exploration_factor * (ub - lb) / (self.evaluations / self.budget + 1)\n                learned_bounds_lb = np.maximum(learned_bounds_lb, best_solution - learning_adjustment)\n                learned_bounds_ub = np.minimum(learned_bounds_ub, best_solution + learning_adjustment)\n            else:\n                no_improvement_count += 1\n                adaptive_step_size *= 0.8\n\n            if no_improvement_count >= max_no_improvement:\n                adaptive_step_size *= reduction_factor\n                no_improvement_count = 0\n\n        return best_solution", "configspace": "", "generation": 24, "feedback": "The algorithm DynamicBoundedExplorativeSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06149 with standard deviation 0.00734.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06380646049162553, 0.054315145952639754, 0.07171356337008106, 0.05838611078550049, 0.04982646412035152, 0.0653114856565209, 0.06387757741855449, 0.05437340480211561, 0.07179862800986458]}}
{"id": "aa2d6a44-7037-49b5-9b09-978a5619b150", "fitness": -Infinity, "name": "EnhancedDynamicStepSearch", "description": "Enhanced Dynamic Step Search with layered adaptive strategies for improved multi-scale exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedDynamicStepSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.7  # More conservative reduction factor\n            increase_factor = 1.5  # Higher increase factor on improvement\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= increase_factor  # Increase step size more aggressively\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Minor reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = max((ub - lb) / 15, adaptive_step_size / 2)  # Layered restart strategy with dynamic adjustment\n\n        return best_solution", "configspace": "", "generation": 25, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {}}
{"id": "5cc6e0d4-7a86-4631-b851-d8bda532d184", "fitness": -Infinity, "name": "AdaptiveMultiLevelRandomSearch", "description": "Adaptive Multi-level Random Search with hierarchical step size adjustment and elite preservation for enhanced convergence in black-box optimization.", "code": "import numpy as np\n\nclass AdaptiveMultiLevelRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        initial_step_size = (ub - lb) / 5\n        reduction_factor = 0.6\n        min_step_size = (ub - lb) / 100\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 4\n            adaptive_step_size = initial_step_size\n\n            while self.evaluations < self.budget and adaptive_step_size > min_step_size:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Fine-tuning phase with preserved elite solutions\n            elite_candidates = [best_solution]\n            elite_step_size = initial_step_size / 10\n            for _ in range(3):  # Elite fine-tuning iterations\n                for elite in elite_candidates:\n                    if self.evaluations >= self.budget:\n                        break\n                    candidate = np.clip(elite + np.random.uniform(-elite_step_size, elite_step_size, self.dim), lb, ub)\n                    candidate_value = func(candidate)\n                    self.evaluations += 1\n                    if candidate_value < best_value:\n                        best_solution = candidate\n                        best_value = candidate_value\n\n        return best_solution", "configspace": "", "generation": 26, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {}}
{"id": "09308b05-0596-4156-8721-a1c7f672779c", "fitness": 0.05371076497250541, "name": "EnhancedAdaptiveRandomSearchWithHistory", "description": "Enhanced Adaptive Random Search with history-based dynamic multi-start and adaptive step size scaling for balanced exploration-exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearchWithHistory:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n        successful_attempts = 0\n\n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increase step size if improvement occurs\n                    successful_attempts += 1\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Decrease step size if no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size /= 1.5  # Further reduce step size to encourage local search\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Dynamic multi-start strategy: adjust step size based on success history\n            if successful_attempts > 0:\n                adaptive_step_size = (ub - lb) / (12 - min(successful_attempts, 10))\n            else:\n                adaptive_step_size = (ub - lb) / 10  # Default smaller step size on failure\n\n            successful_attempts = 0  # Reset for next multi-start\n\n        return best_solution", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedAdaptiveRandomSearchWithHistory got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05371 with standard deviation 0.00431.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.05732376173762599, 0.049841029387910796, 0.058486182436230494, 0.05256663477418666, 0.045721641217600806, 0.0536281765101978, 0.057385689513658966, 0.04989428782578964, 0.058549481349347565]}}
{"id": "4320ca7a-8c8b-487c-95e3-899a598c7eb3", "fitness": 0.05332628969138858, "name": "EnhancedAdaptiveRandomSearch", "description": "Adaptive Random Search with enhanced exploration using a stochastic search radius reset.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size = np.random.uniform((ub - lb) / 12, (ub - lb) / 5)  # Randomized step size reset\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = np.random.uniform((ub - lb) / 12, (ub - lb) / 5)  # Randomized restart step size\n\n        return best_solution", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05333 with standard deviation 0.00370.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.05500553069253, 0.05851969063005502, 0.050935536779286306, 0.05046683715571476, 0.05362557871269247, 0.046745722457503414, 0.05506446262011733, 0.058583519330635325, 0.05098972884396258]}}
{"id": "cc870b5a-3cfe-4d34-9946-1d6d80c8169c", "fitness": 0.06148987117858377, "name": "DynamicMemeticRandomSearch", "description": "Dynamic Memetic Random Search combines adaptive random search with periodic local refinement using a surrogate model for enhanced convergence speed and solution quality.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass DynamicMemeticRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            # Surrogate model to capture local landscape\n            kernel = C(1.0) * RBF(length_scale=1.0)\n            gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5)\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                # Update surrogate model with new data\n                if self.evaluations < self.budget * 0.8:  # Use only in early iterations\n                    X_sample = np.array([current_best])\n                    y_sample = np.array([current_best_value])\n                    if X_sample.shape[0] > 1:\n                        gp.fit(X_sample, y_sample)\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = (ub - lb) / 12  # Restart with even smaller step size\n\n        return best_solution", "configspace": "", "generation": 29, "feedback": "The algorithm DynamicMemeticRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06149 with standard deviation 0.00734.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06380646049162553, 0.054315145952639754, 0.07171356337008106, 0.05838611078550049, 0.04982646412035152, 0.0653114856565209, 0.06387757741855449, 0.05437340480211561, 0.07179862800986458]}}
{"id": "ae929e75-6ee5-4d27-b286-b12ef6432ba9", "fitness": 0.052392266846328264, "name": "EnhancedAdaptiveRandomSearch", "description": "Refined Enhanced Adaptive Random Search with adjusted reduction factor and dynamic restart to improve convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.7  # Reduction factor for the step size (changed from 0.5)\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale (changed calculation)\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Restart with even smaller step size (changed from 12)\n\n        return best_solution", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05239 with standard deviation 0.00560.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06139520418499944, 0.049662291059228036, 0.05053169443610772, 0.05625013859071126, 0.04555517363808159, 0.04637257423876984, 0.06146246475438877, 0.04971538663219466, 0.05058547408247305]}}
{"id": "c39f2aa4-c47d-4caf-aacf-172a112ddaf1", "fitness": 0.06148987117858377, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with improved adaptive step size scaling, dynamic adjustment, and elitism strategy to retain the best-found solution across iterations.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 12  # Restart with even smaller step size\n\n        return best_solution if best_value < current_best_value else current_best  # Elitism to return the best", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06149 with standard deviation 0.00734.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06380646049162553, 0.054315145952639754, 0.07171356337008106, 0.05838611078550049, 0.04982646412035152, 0.0653114856565209, 0.06387757741855449, 0.05437340480211561, 0.07179862800986458]}}
{"id": "537e2ff1-38bb-4168-951a-6254d45e8862", "fitness": 0.053810151979688935, "name": "HybridEvolutionaryRandomSearch", "description": "\"Hybrid Evolutionary Random Search combining guided exploration with adaptive step size and evolutionary operators for improved convergence.\"", "code": "import numpy as np\n\nclass HybridEvolutionaryRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        population_size = max(4, self.dim)  # Ensure at least a small population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        pop_values = np.array([func(ind) for ind in population])\n        self.evaluations += population_size\n\n        adaptive_step_size = (ub - lb) / 5\n        max_no_improvement = 5\n        no_improvement_count = 0\n\n        while self.evaluations < self.budget:\n            # Select parents and apply crossover\n            parents_idx = np.argsort(pop_values)[:2]\n            best_parents = population[parents_idx]\n            offspring = (best_parents[0] + best_parents[1]) / 2\n\n            # Apply mutation with adaptive step size\n            mutation = np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim)\n            offspring = np.clip(offspring + mutation, lb, ub)\n            offspring_value = func(offspring)\n            self.evaluations += 1\n\n            # Selection step: replace worst individual if better\n            if offspring_value < np.max(pop_values):\n                worst_idx = np.argmax(pop_values)\n                population[worst_idx] = offspring\n                pop_values[worst_idx] = offspring_value\n\n            # Update best solution found\n            if offspring_value < best_value:\n                best_solution = offspring\n                best_value = offspring_value\n                no_improvement_count = 0\n                adaptive_step_size *= 1.1  # Increase step size on improvement\n            else:\n                no_improvement_count += 1\n                adaptive_step_size *= 0.9  # Decrease step size on no improvement\n\n            # Restart strategy for exploration\n            if no_improvement_count >= max_no_improvement:\n                new_individuals = np.random.uniform(lb, ub, (population_size // 2, self.dim))\n                population[:population_size // 2] = new_individuals\n                pop_values[:population_size // 2] = [func(ind) for ind in new_individuals]\n                self.evaluations += population_size // 2\n                adaptive_step_size = (ub - lb) / 10\n                no_improvement_count = 0\n\n        return best_solution", "configspace": "", "generation": 32, "feedback": "The algorithm HybridEvolutionaryRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05381 with standard deviation 0.00278.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.052903379923409766, 0.05723029679901781, 0.055796450409660525, 0.04854995034877174, 0.052498609632794846, 0.05120496251258855, 0.05295978750572261, 0.05729184937528298, 0.055856081309951566]}}
{"id": "6c4fbcec-fadd-46a6-b0ca-a474473d480b", "fitness": 0.0497149463368309, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with progressive elitism and stochastic tunneling to intensify search and escape local optima.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    # Stochastic tunneling by perturbation to escape local optima\n                    current_best += np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim)\n                    current_best = np.clip(current_best, lb, ub)\n                    no_improvement_count = 0\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Refined exploration using multi-start with progressive elitism\n            adaptive_step_size = (ub - lb) / (10 + self.evaluations/self.budget * 10)  # Dynamically adjust step size\n\n        return best_solution", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04971 with standard deviation 0.00324.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.05355930628628924, 0.05226172524845718, 0.047482788194892045, 0.049153531607599055, 0.047950497707054285, 0.04355927840896057, 0.05361642079297624, 0.05231757180796748, 0.04753339697728198]}}
{"id": "5dd74bfc-c10f-42fc-87e1-f9327f9c2780", "fitness": 0.060424184336559036, "name": "AdaptiveMemoryRandomSearch", "description": "Adaptive Memory Random Search (AMRS) with historical memory of promising regions, enhanced adaptive step size, and dynamic exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveMemoryRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.memory = []  # Memory to store promising solutions\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            if not self.memory or np.random.rand() < 0.3:\n                current_best = np.random.uniform(lb, ub)\n            else:\n                # Select a solution from memory with slight random perturbation\n                mem_idx = np.random.randint(len(self.memory))\n                current_best = np.clip(self.memory[mem_idx] + np.random.normal(0, 0.1, self.dim), lb, ub)\n            \n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Update memory with the current best known solution\n            if len(self.memory) < 10:\n                self.memory.append(current_best)\n            else:\n                # Replace worst memory entry if new solution is better\n                worst_idx = np.argmax([func(sol) for sol in self.memory])\n                if current_best_value < func(self.memory[worst_idx]):\n                    self.memory[worst_idx] = current_best\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 34, "feedback": "The algorithm AdaptiveMemoryRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06042 with standard deviation 0.00381.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06466918149843048, 0.05797334230915663, 0.06380122857824233, 0.059184303988746745, 0.05314389170516254, 0.05839618120157086, 0.0647411569847709, 0.05803626699523268, 0.06387210576771818]}}
{"id": "bd71f8d7-af00-4576-8fb2-e620e6c33ef0", "fitness": 0.05334746609210686, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with refined step size adaptation and strategic restart to improve exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.7  # Increase the reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.4  # Slightly increase the step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = (ub - lb) / 15  # Restart with smaller step size for finer search\n\n        return best_solution", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05335 with standard deviation 0.00435.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.058643307200249795, 0.04951519069324595, 0.056366819407976054, 0.053759137916587485, 0.04542423715554522, 0.05171619071396283, 0.058706982470705604, 0.04956806312741446, 0.05642726614327431]}}
{"id": "40ad6a93-569a-4ff2-ba92-3eb7bccaa65c", "fitness": 0.03959373720476674, "name": "AdaptiveGradientInformedRandomSearch", "description": "Adaptive Gradient-Informed Random Search integrates gradient approximation with adaptive step size scaling for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveGradientInformedRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n\n            while self.evaluations < self.budget:\n                # Approximate gradient using random sampling\n                gradient = np.zeros(self.dim)\n                epsilon = 1e-8\n                for i in range(self.dim):\n                    step_vector = np.zeros(self.dim)\n                    step_vector[i] = epsilon\n                    f_plus = func(np.clip(current_best + step_vector, lb, ub))\n                    f_minus = func(np.clip(current_best - step_vector, lb, ub))\n                    gradient[i] = (f_plus - f_minus) / (2 * epsilon)\n                    self.evaluations += 2\n                    if self.evaluations >= self.budget:\n                        break\n\n                # Incorporate gradient information into the search\n                candidate = np.clip(current_best - adaptive_step_size * gradient + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 12\n\n        return best_solution", "configspace": "", "generation": 36, "feedback": "The algorithm AdaptiveGradientInformedRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03959 with standard deviation 0.00211.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.041073663345346345, 0.03773742396481805, 0.04254484060447683, 0.037597046075388274, 0.036980165250278696, 0.03892129516391263, 0.041118307109266006, 0.03777940210576136, 0.04259149122365247]}}
{"id": "e60d73e4-2d66-4ae5-af38-71d3b0191cd1", "fitness": 0.05816899437740576, "name": "ImprovedAdaptiveRandomSearch", "description": "Improved Adaptive Random Search with dynamic threshold adjustment for stagnation detection and advanced adaptive exploration-exploitation balancing.", "code": "import numpy as np\n\nclass ImprovedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            adaptive_step_size = (ub - lb) / 5  # Initial step size\n            stagnation_threshold = 10  # Dynamic threshold for stagnation detection\n            adjustment_factor = 1.5  # Factor to adjust stagnation threshold based on performance\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increase step size on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Decrease step size on no improvement\n\n                if no_improvement_count >= stagnation_threshold:\n                    adaptive_step_size *= 0.5  # Significantly reduce step size on stagnation\n                    no_improvement_count = 0\n                    stagnation_threshold = max(5, int(stagnation_threshold / adjustment_factor))  # Adjust threshold\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart may retain some previous knowledge\n            adaptive_step_size = (ub - lb) / (10 + np.random.rand() * 5)  # Randomized reduced step size for restart\n\n        return best_solution", "configspace": "", "generation": 37, "feedback": "The algorithm ImprovedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05817 with standard deviation 0.00347.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06350279525226854, 0.05818130067903471, 0.057753256584020196, 0.0581773802859431, 0.053336464563516595, 0.052936786501101674, 0.06357250665276903, 0.058244437853986586, 0.0578160210240114]}}
{"id": "29adc242-a99c-43a3-a5a8-d0e10e88e304", "fitness": 0.05367072433699512, "name": "RefinedEnhancedAdaptiveRandomSearch", "description": "A Refined Enhanced Adaptive Random Search with dynamic multi-phase exploration strategy and adaptive memory mechanism for improved global search.", "code": "import numpy as np\n\nclass RefinedEnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.phase_length = 10  # Initial phase length\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n\n        # Memory of past best solutions to aid in exploration\n        memory = []\n        memory_limit = 5\n\n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            phase_counter = 0\n\n            while self.evaluations < self.budget and phase_counter < self.phase_length:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                    phase_counter = 0  # Reset phase counter on improvement\n                else:\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n                    phase_counter += 1  # Increase phase counter\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Store best solution found in this run into memory\n            if len(memory) < memory_limit:\n                memory.append(current_best)\n            else:\n                # Replace the oldest memory entry\n                memory.pop(0)\n                memory.append(current_best)\n\n            # Multi-phase strategy: explore the memory solutions\n            for mem_solution in memory:\n                if self.evaluations >= self.budget:\n                    break\n                candidate = np.clip(mem_solution + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                if candidate_value < best_value:\n                    best_solution = candidate\n                    best_value = candidate_value\n\n            # Dynamically adjust adaptive step size based on overall progress\n            adaptive_step_size = (ub - lb) / (5 + 3 * (1 - best_value / float('inf')))  # Adaptive scaling based on progress\n            self.phase_length = min(20, 5 + int(self.budget / (self.evaluations + 1)))  # Adjust phase length based on evaluations\n\n        return best_solution", "configspace": "", "generation": 38, "feedback": "The algorithm RefinedEnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05367 with standard deviation 0.00219.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.05492686133893021, 0.05574133998015729, 0.05483621452690768, 0.050403604729269746, 0.051118914956776784, 0.05032786451885596, 0.054985571980636694, 0.055801432634456805, 0.054894714366964914]}}
{"id": "22faad62-d2c0-4bb1-9405-17565db95ba9", "fitness": 0.05242177382777444, "name": "AdaptiveEvolutionarySearch", "description": "Adaptive Evolutionary Search with Dynamic Step Size Adjustment leveraging a combination of adaptive step size, differential mutation strategy, and elite archiving to enhance convergence and exploration.", "code": "import numpy as np\n\nclass AdaptiveEvolutionarySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        population_size = max(4, self.dim + 1)  # At least a small diverse population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        values = np.array([func(ind) for ind in population])\n        self.evaluations += population_size\n\n        archive = []\n\n        while self.evaluations < self.budget:\n            indices = np.arange(population_size)\n            np.random.shuffle(indices)\n            \n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n\n                idxs = [idx for idx in indices if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n\n                # Crossover with adaptive blending\n                crossover_prob = 0.9 * np.exp(-0.1 * self.evaluations / self.budget)\n                trial = np.where(np.random.rand(self.dim) < crossover_prob, mutant, population[i])\n                \n                trial_value = func(trial)\n                self.evaluations += 1\n\n                if trial_value < values[i]:\n                    population[i] = trial\n                    values[i] = trial_value\n\n                    if trial_value < best_value:\n                        best_solution = trial\n                        best_value = trial_value\n\n                # Archive elite solutions\n                archive.append((trial, trial_value))\n                archive = sorted(archive, key=lambda x: x[1])[:population_size]\n\n            # Periodically introduce elite solutions back into the population\n            if self.evaluations < self.budget and self.evaluations % (population_size * 2) == 0:\n                elite = np.array([sol for sol, val in archive[:population_size]])\n                population = elite + np.random.normal(0, 0.1 * (ub - lb), elite.shape)\n\n        return best_solution", "configspace": "", "generation": 39, "feedback": "The algorithm AdaptiveEvolutionarySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05242 with standard deviation 0.00317.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.05153131098821817, 0.05723029679901781, 0.0528844941372032, 0.04729096833659541, 0.052498609632794846, 0.04854149628738924, 0.05158618808377413, 0.05729184937528298, 0.05294075080969418]}}
{"id": "4dcfa418-cf43-4b7a-b701-9f267a96a0a7", "fitness": 0.06063877503363513, "name": "EnhancedAdaptiveRandomSearch", "description": "Improved Enhanced Adaptive Random Search by adjusting the initial adaptive step size scaling based on the problem dimension.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / (5 + 0.5 * self.dim)  # Initial larger step size modified based on dimension\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with reduced step size for refined exploration\n            adaptive_step_size = (ub - lb) / 12  # Restart with even smaller step size\n\n        return best_solution", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06064 with standard deviation 0.00515.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06751541869079869, 0.0562457092821147, 0.06332886499206791, 0.061766782445842106, 0.051599269409045134, 0.05799712443898497, 0.0675910079007177, 0.056306105714405086, 0.06339869242873986]}}
{"id": "2c2fd6ed-0fad-4b50-b3d0-f364b9dff5ca", "fitness": 0.052053909519952785, "name": "QuantumInspiredAdaptiveRandomSearch", "description": "Quantum-Inspired Adaptive Random Search with variable step size exploration and entangled state enhancement for improved convergence.", "code": "import numpy as np\n\nclass QuantumInspiredAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initialize a probability amplitude vector for quantum-inspired search\n        prob_amplitudes = np.full(self.dim, 1/np.sqrt(self.dim))\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n            entanglement_strength = 0.1  # Introducing quantum entanglement influence\n\n            while self.evaluations < self.budget:\n                # Generate candidate influenced by quantum probability amplitudes\n                candidate = current_best + prob_amplitudes * np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                    prob_amplitudes = np.clip(prob_amplitudes * (1 + entanglement_strength), -1, 1)\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n                    prob_amplitudes = np.clip(prob_amplitudes * (1 - entanglement_strength), -1, 1)\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 12\n            prob_amplitudes = np.full(self.dim, 1/np.sqrt(self.dim))  # Reset probability amplitudes\n\n        return best_solution", "configspace": "", "generation": 41, "feedback": "The algorithm QuantumInspiredAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05205 with standard deviation 0.00476.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.05212782745526079, 0.048960896570472534, 0.05944061118947119, 0.04783738720650066, 0.04491668122181547, 0.0545002276611658, 0.0521833842445667, 0.049013135558109155, 0.05950503457221279]}}
{"id": "8fb2577d-2019-4d72-9df9-557dcd4a2777", "fitness": 0.061979898390606136, "name": "EnhancedAdaptiveRandomSearch", "description": "Improved Adaptive Random Search with modified exploration phase to balance between exploration and refinement.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Restart with a slightly smaller step size\n\n        return best_solution", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06198 with standard deviation 0.00282.", "error": "", "parent_ids": ["b5856692-0382-4c50-ab2f-82d115f87874"], "operator": null, "metadata": {"aucs": [0.06501413743433548, 0.06219605708341547, 0.06409324797346716, 0.05952993271967277, 0.05687840852893056, 0.05858981299211252, 0.06508605386357746, 0.062265834626194394, 0.0641656002937494]}}
{"id": "481ba3f1-da64-41c8-b731-7669dc053994", "fitness": 0.040364670730535096, "name": "DynamicGradientDirectedSearch", "description": "Dynamic Gradient-Directed Search enhancing exploration and exploitation by dynamically adjusting directional biases and step sizes based on gradient approximations.", "code": "import numpy as np\n\nclass DynamicGradientDirectedSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n            \n            # Gradient direction initialization\n            grad_direction = np.zeros(self.dim)\n\n            while self.evaluations < self.budget:\n                # Gradient approximation\n                epsilon = 1e-8\n                grad_approx = np.zeros(self.dim)\n                for i in range(self.dim):\n                    forward = np.copy(current_best)\n                    forward[i] += epsilon\n                    backward = np.copy(current_best)\n                    backward[i] -= epsilon\n                    grad_approx[i] = (func(forward) - func(backward)) / (2 * epsilon)\n                    self.evaluations += 2  # Two evaluations per dimension\n\n                # Directional move\n                grad_direction = 0.9 * grad_direction + 0.1 * grad_approx\n                candidate = np.clip(current_best - adaptive_step_size * grad_direction, lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 43, "feedback": "The algorithm DynamicGradientDirectedSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04036 with standard deviation 0.00336.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.042865370725784446, 0.03739599630327317, 0.04430959060409689, 0.03926209176848161, 0.0341448554663466, 0.04059734647079849, 0.042911706828827945, 0.037437689392078366, 0.04435738901512831]}}
{"id": "9984ec1b-7abc-4fe7-962c-db75872f5d24", "fitness": 0.06148987117858377, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced convergence by introducing dynamic thresholding for adaptive step size scaling.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Restart with a slightly smaller step size\n\n        return best_solution", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06149 with standard deviation 0.00734.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06380646049162553, 0.054315145952639754, 0.07171356337008106, 0.05838611078550049, 0.04982646412035152, 0.0653114856565209, 0.06387757741855449, 0.05437340480211561, 0.07179862800986458]}}
{"id": "f76330ac-6c43-468d-90a6-07276cd4db93", "fitness": 0.05771514140999033, "name": "EnhancedAdaptiveRandomSearch", "description": "Modified Enhanced Adaptive Random Search with refined step size dynamics for better convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.1  # Changed step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Changed step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 12  # Changed restart step size\n\n        return best_solution", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05772 with standard deviation 0.00602.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.0665792159947729, 0.052618979826234424, 0.058845909436359145, 0.06090823629607778, 0.04828988343929996, 0.05395558522462718, 0.06665375557492315, 0.052675056658271435, 0.05890965023934702]}}
{"id": "d6444382-ff61-4129-a783-fc927c7d960f", "fitness": 0.05307901806785147, "name": "EnhancedAdaptiveRandomSearch", "description": "Improved Adaptive Random Search with dynamic exploration adjustment based on recent performance trends.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n            performance_trend = 0.1  # Initialize performance trend indicator\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increased step size adjustment on improvement\n                    performance_trend = 0.1  # Reset performance trend\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n                    performance_trend += 0.1  # Decrease performance trend when no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n                # Adjust adaptive step size based on performance trend\n                adaptive_step_size = adaptive_step_size * (1 - performance_trend)\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Restart with a slightly smaller step size\n\n        return best_solution", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05308 with standard deviation 0.00512.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.05355103143598261, 0.049279534859840046, 0.06085585543628724, 0.049151380186219784, 0.04521680149039975, 0.05579456824957585, 0.05360805674265412, 0.04933201699526579, 0.06092191721443807]}}
{"id": "abce95c9-aa6a-490f-8f1c-671245b7276d", "fitness": 0.04951430191342284, "name": "EnhancedAdaptiveRandomSearch", "description": "EnhancedAdaptiveRandomSearch with dynamic adaptive step size tuning based on convergence rate and diversity preservation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        convergence_rate_threshold = 0.01\n\n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 4  # Initial larger step size\n            reduction_factor = 0.5\n            change_momentum = 1.1\n\n            last_best_value = current_best_value\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= change_momentum  # Increase step size\n\n                else:\n                    no_improvement_count += 1\n                \n                # Adapt step size based on no improvement and convergence rate\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n                \n                # Dynamically adjust based on convergence rate\n                convergence_rate = abs((last_best_value - current_best_value) / last_best_value)\n                if convergence_rate < convergence_rate_threshold:\n                    adaptive_step_size *= reduction_factor  # Slow convergence, refine search space\n                else:\n                    adaptive_step_size *= change_momentum  # Fast convergence, expand search space\n\n                last_best_value = current_best_value\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy with diversity preservation\n            adaptive_step_size = (ub - lb) / 8  # Restart with a refined step size\n\n        return best_solution", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04951 with standard deviation 0.00329.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.05187601407377884, 0.047220123684242554, 0.053590658630274124, 0.047612473321837046, 0.04330467682771866, 0.04917502234329618, 0.051931199578914566, 0.04727063479043492, 0.05364791397030866]}}
{"id": "950cc867-ea20-47fc-b8ea-da0020c50792", "fitness": 0.061979898390606136, "name": "EnhancedAdaptiveRandomSearch", "description": "Refined exploration strategy with dynamic step size adjustments based on the success rate of finding better solutions.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n            success_rate = 0\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increased step size adjustment on improvement\n                    success_rate += 0.1  # Increase success rate\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n                    success_rate -= 0.1  # Decrease success rate\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / (10 - success_rate)  # Restart with dynamic step size\n\n        return best_solution", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06198 with standard deviation 0.00282.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06501413743433548, 0.06219605708341547, 0.06409324797346716, 0.05952993271967277, 0.05687840852893056, 0.05858981299211252, 0.06508605386357746, 0.062265834626194394, 0.0641656002937494]}}
{"id": "12b1ba14-963a-445b-be8c-f030c86e383b", "fitness": 0.05789131867240455, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with improved convergence through dynamic adjustment of maximum no improvement iterations.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 10  # Adjusted maximum no improvement iterations\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Restart with a slightly smaller step size\n\n        return best_solution", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05789 with standard deviation 0.00494.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06522798438718391, 0.05433031042074066, 0.05904405434848725, 0.05971606284648434, 0.049837623390182606, 0.054067833264551, 0.06530029415387295, 0.0543886311879036, 0.059109074052234645]}}
{"id": "635227b4-4473-4abe-8f34-09475f85890c", "fitness": 0.04678136407390687, "name": "EnhancedAdaptiveRandomSearchV2", "description": "Enhanced Adaptive Random Search 2.0 with dynamic step adjustment based on function evaluation history for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearchV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        history = []\n\n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            history.append(current_best_value)\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                history.append(candidate_value)\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.95  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n                # Dynamic step adjustment based on recent improvement history\n                if len(history) > 10:\n                    recent_changes = np.diff(history[-10:])\n                    improvement_rate = np.sum(recent_changes < 0) / 10.0\n                    adaptive_step_size = adaptive_step_size * (1 + improvement_rate)\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Restart with a slightly smaller step size\n\n        return best_solution", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedAdaptiveRandomSearchV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04678 with standard deviation 0.00399.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.05072708224902822, 0.050536230999160825, 0.04300898197919689, 0.046555742975156966, 0.04637448011733425, 0.0394033457230466, 0.05078102297632825, 0.05059004767690933, 0.043055341969000516]}}
{"id": "4dc90e6f-befe-4f4b-9ef8-584f73280c61", "fitness": 0.057249500675321245, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced exploration by adjusting reduction factor and step size to improve convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.7  # Reduction factor changed from 0.5 to 0.7\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment from 1.2 to 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05725 with standard deviation 0.00431.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06341936064630982, 0.05425384945528888, 0.05891870278202871, 0.05808081439085844, 0.049758504995455866, 0.054030367672039614, 0.06348928281783384, 0.05431221534101782, 0.05898240797705823]}}
{"id": "d7b93ce1-e77c-4332-bd8c-80014e362de6", "fitness": 0.061979898390606136, "name": "AdaptiveGradientBasedRandomSearch", "description": "Adaptive Gradient-Based Random Search optimizes exploration by integrating adaptive gradient estimation to guide search direction and step size dynamically.", "code": "import numpy as np\n\nclass AdaptiveGradientBasedRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_solution = np.random.uniform(lb, ub)\n            current_value = func(current_solution)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            \n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n            gradient = np.zeros(self.dim)  # Initialize gradient\n\n            while self.evaluations < self.budget:\n                # Estimate the gradient by sampling\n                perturbation = np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim)\n                candidate = np.clip(current_solution + perturbation, lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                # Calculate pseudo-gradient\n                gradient_estimate = (candidate_value - current_value) / (np.linalg.norm(perturbation) + 1e-8)\n                gradient += gradient_estimate * perturbation\n\n                if candidate_value < current_value:\n                    current_solution = candidate\n                    current_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            # Use gradient to guide next search direction\n            if self.evaluations < self.budget:\n                gradient_direction = gradient / (np.linalg.norm(gradient) + 1e-8)\n                candidate = np.clip(current_solution + adaptive_step_size * gradient_direction, lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < best_value:\n                    best_solution = candidate\n                    best_value = candidate_value\n\n            # Multi-start strategy\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 52, "feedback": "The algorithm AdaptiveGradientBasedRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06198 with standard deviation 0.00282.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06501413743433548, 0.06219605708341547, 0.06409324797346716, 0.05952993271967277, 0.05687840852893056, 0.05858981299211252, 0.06508605386357746, 0.062265834626194394, 0.0641656002937494]}}
{"id": "17df5d3f-ade8-4fbe-8d7d-ee5b71e439b1", "fitness": -Infinity, "name": "AdaptiveRandomSearchWithDynamicLearningRate", "description": "Adaptive Random Search with Dynamic Learning Rate to enhance convergence through adaptive sampling.", "code": "import numpy as np\n\nclass AdaptiveRandomSearchWithDynamicLearningRate:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Initialize learning rate parameters\n        initial_step_size = (ub - lb) / 5\n        min_step_size = initial_step_size / 100\n        step_size = initial_step_size\n        learning_rate = 0.2\n\n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n\n            while self.evaluations < self.budget:\n                # Adaptive step size exploration\n                candidate = np.clip(current_best + np.random.uniform(-step_size, step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    # Increase step size dynamically during improvement\n                    step_size = min(step_size * (1 + learning_rate), initial_step_size)\n                else:\n                    no_improvement_count += 1\n                    # Gradually reduce step size when no improvement\n                    step_size = max(step_size * (1 - learning_rate), min_step_size)\n\n                if no_improvement_count >= max_no_improvement:\n                    step_size *= 0.5  # Reduce step size scale more significantly\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            step_size = initial_step_size / 2  # Restart with a reduced step size\n\n        return best_solution", "configspace": "", "generation": 53, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {}}
{"id": "8797571a-3e12-4651-ae96-47940d226248", "fitness": 0.058203970918405075, "name": "EnhancedAdaptiveRandomSearch", "description": "EnhancedAdaptiveRandomSearch with dynamic step size tuning and elite solutions archiving for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        elite_solutions = []\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Slightly increased step size boost\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85  # Slightly increased reduction on no improvement\n                    \n                # Archive elite solutions\n                if candidate_value < best_value * 1.1:  # Archiving near-best solutions\n                    elite_solutions.append(candidate)\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Diversify from elite solutions\n            if elite_solutions:\n                elite_choice = elite_solutions[np.random.randint(len(elite_solutions))]\n                adaptive_step_size = np.maximum((ub - lb) / 20, adaptive_step_size)  # Reset step size with a floor\n            \n        return best_solution", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05820 with standard deviation 0.00325.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.05896290616764677, 0.06298991399741183, 0.057582335784985506, 0.054057062830055735, 0.05769688762348124, 0.05281619673111082, 0.05902686850358718, 0.06305919903310586, 0.057644367594260726]}}
{"id": "856d3132-0555-4d8d-815e-6e86f488c3da", "fitness": 0.05618051857369114, "name": "DynamicStepSizeAdaptiveRandomSearch", "description": "Dynamic Step-Size Adaptive Random Search with adaptive multi-phase exploration to enhance convergence efficiency.", "code": "import numpy as np\n\nclass DynamicStepSizeAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 3\n            adaptive_step_size = (ub - lb) / 4  # Initial larger step size\n            reduction_factor = 0.6  # Reduction factor for the step size\n            increase_factor = 1.5  # Increase factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= increase_factor  # Increase step size on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjust step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy with adaptive adjustment\n            adaptive_step_size = (ub - lb) / (4 + self.evaluations / self.budget)  # Restart with dynamically adjusted step size\n\n        return best_solution", "configspace": "", "generation": 55, "feedback": "The algorithm DynamicStepSizeAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05618 with standard deviation 0.00431.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06217350425842316, 0.05295902137255426, 0.05814176487090861, 0.0569661927863383, 0.048599803585525514, 0.05332275006154075, 0.06224160249015498, 0.05301550811395894, 0.05820451962381579]}}
{"id": "410af34b-ecda-4292-b833-94ae58e18893", "fitness": 0.05465770302992823, "name": "OptimizedAdaptiveRandomSearch", "description": "Optimized Adaptive Random Search with dynamic step size and guided random exploration for enhanced convergence.", "code": "import numpy as np\n\nclass OptimizedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n            exploration_factor = 0.1  # Exploration factor to guide random exploration\n            \n            while self.evaluations < self.budget:\n                exploration_scale = exploration_factor * (ub - lb)\n                candidate = np.clip(\n                    current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim) +\n                    np.random.normal(0, exploration_scale, self.dim), lb, ub\n                )\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    exploration_factor *= 0.9  # Gradually reduce exploration scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Restart with a smaller step size\n            exploration_factor = 0.05  # Reduce exploration factor on restart\n\n        return best_solution", "configspace": "", "generation": 56, "feedback": "The algorithm OptimizedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05466 with standard deviation 0.00321.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.059028458252090465, 0.05636714749645588, 0.053165612058512, 0.054135287705131985, 0.051687763870879144, 0.04879254004000999, 0.05909221816355725, 0.056428016252691005, 0.05322228343002633]}}
{"id": "8b0f05ae-dca6-4194-ae00-c9e896e21bc2", "fitness": 0.05588940165893871, "name": "EnhancedAdaptiveRandomSearch", "description": "Adaptive Random Search with refined candidate selection and enhanced multi-start strategy.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.normal(0, adaptive_step_size, self.dim), lb, ub)  # Changed to normal distribution\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 8  # Restart with a more balanced step size\n\n        return best_solution", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05589 with standard deviation 0.00372.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.05869483146481913, 0.053296567797366246, 0.06041462675422382, 0.05372271749595481, 0.04891176514085427, 0.05537036971481868, 0.058759850655710766, 0.05335339912712356, 0.06048048677957707]}}
{"id": "a092e44e-d85e-4359-89ff-44fc381f364a", "fitness": 0.05680268646503914, "name": "EnhancedAdaptiveRandomSearch", "description": "Improved Adaptive Random Search with adaptive exploitation based on solution quality and dynamic step size tuning.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 7  # Increased to allow more exploration\n            adaptive_step_size = (ub - lb) / 4  # Larger initial step size\n            reduction_factor = 0.6  # Slightly reduced step size decrement\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 8  # Smaller step size for restart\n\n        return best_solution", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05680 with standard deviation 0.00811.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06976466095797274, 0.053399532061697674, 0.052125759847891695, 0.06360977514253052, 0.049007668722485076, 0.04783277828121646, 0.06984619366430223, 0.05345645638190699, 0.052181353125348884]}}
{"id": "8af7c5af-3a48-4b39-829e-aa48a88da459", "fitness": 0.05112259787473515, "name": "EnhancedAdaptiveRandomSearchV2", "description": "Enhanced exploitation and exploration with dynamic restart strategy to improve search efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearchV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.7  # Slightly higher reduction factor for quicker adaptation\n            restart_threshold = self.budget // 10  # Restart if budget usage exceeds 10% without improvement\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # More aggressive increase on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # More aggressive reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement or self.evaluations % restart_threshold == 0:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    # Use a random restart to escape local minima\n                    current_best = np.random.uniform(lb, ub)\n                    current_best_value = func(current_best)\n                    self.evaluations += 1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedAdaptiveRandomSearchV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05112 with standard deviation 0.00320.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.04991750709118847, 0.05595363953550714, 0.051781912176533496, 0.045807221221995875, 0.05129903393188939, 0.04752219961615378, 0.04997062792531748, 0.05601418912050993, 0.05183705025352081]}}
{"id": "bb541bf2-bd46-4958-b941-a560347268fd", "fitness": 0.05532305804970175, "name": "OptimizedAdaptiveRandomSearch", "description": "Optimized Adaptive Random Search with dynamic parameter tuning based on performance metrics for a more balanced exploration-exploitation trade-off.", "code": "import numpy as np\n\nclass OptimizedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.95  # Slightly slower reduction for better fine-tuning\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Restart with a refined exploration based on current best\n            adaptive_step_size = (ub - lb) / 10\n            max_no_improvement = max(3, max_no_improvement - 1)  # Dynamic adjustment of patience\n\n        return best_solution", "configspace": "", "generation": 60, "feedback": "The algorithm OptimizedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05532 with standard deviation 0.00287.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.059044612417603726, 0.0570576537627836, 0.05454761285990184, 0.05409658868050282, 0.05230949890331327, 0.050016377828623226, 0.05910921300517846, 0.057119478275188706, 0.05460648671422008]}}
{"id": "7c3c465a-cc1b-4be2-b748-f19c6e8db6a1", "fitness": 0.05448140192054115, "name": "EnhancedAdaptiveRandomSearch", "description": "EnhancedAdaptiveRandomSearch with adaptive no-improvement threshold and dynamic step size scaling.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 7  # Increased threshold for no improvement\n            adaptive_step_size = (ub - lb) / 5  \n            reduction_factor = 0.5  \n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Increased scaling of step size on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Further reduced step size on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  \n                    no_improvement_count = 0  \n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10  \n\n        return best_solution", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05448 with standard deviation 0.00370.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.05731074331063324, 0.0589338576268319, 0.051799720157948204, 0.05255390476174204, 0.05397233974763527, 0.047535816794454844, 0.05737266880849545, 0.05899864814145772, 0.05185491793567165]}}
{"id": "88595722-3820-4a61-8151-eacc47451049", "fitness": 0.05881654048110853, "name": "EnhancedAdaptiveRandomSearch", "description": "Adaptive Random Search with Multi-Scale Perturbations and Dynamic Restart Strategy to improve convergence and solution exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.7  # Reduction factor for the step size\n            \n            # Introducing different scales of perturbation\n            large_perturbation_scale = (ub - lb) / 8\n            small_perturbation_scale = (ub - lb) / 20\n            perturbation_counter = 0\n\n            while self.evaluations < self.budget:\n                if perturbation_counter % 2 == 0:\n                    perturbation_scale = large_perturbation_scale\n                else:\n                    perturbation_scale = small_perturbation_scale\n\n                candidate = np.clip(current_best + np.random.uniform(-perturbation_scale, perturbation_scale, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.1  # Slightly increase step size on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.95  # Slightly reduce step size on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n                    perturbation_counter += 1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a dynamically sized step size\n            dynamic_step_size = (ub - lb) / (10 + np.random.rand() * 5)  # Restart with a dynamically chosen step size\n            adaptive_step_size = dynamic_step_size\n\n        return best_solution", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05882 with standard deviation 0.00250.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06113808625734818, 0.05952414803948869, 0.06078951070568228, 0.05604102182136472, 0.05448503764593171, 0.05572066880706639, 0.06120465371239037, 0.05959004715452976, 0.06085569018617465]}}
{"id": "821d0276-484e-49ed-89ec-86f7eb9b0021", "fitness": 0.058203970918405075, "name": "DynamicEnhancedAdaptiveRandomSearch", "description": "Enhanced Adaptive Random Search with dynamic adjustment of step size and probabilistic restart strategy for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass DynamicEnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Further increase step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Probabilistic restart strategy\n            if np.random.rand() < 0.1:\n                adaptive_step_size = (ub - lb) / 8  # Restart with a refined step size\n\n        return best_solution", "configspace": "", "generation": 63, "feedback": "The algorithm DynamicEnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05820 with standard deviation 0.00325.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.05896290616764677, 0.06298991399741183, 0.057582335784985506, 0.054057062830055735, 0.05769688762348124, 0.05281619673111082, 0.05902686850358718, 0.06305919903310586, 0.057644367594260726]}}
{"id": "6efcfdad-1772-4333-9e90-05c7c9223a9a", "fitness": 0.05585140112263384, "name": "DynamicAdaptiveSearch", "description": "Dynamic Population-based Adaptive Search with adaptive step-size control and dynamic population reinitialization based on convergence rate.", "code": "import numpy as np\n\nclass DynamicAdaptiveSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        population_size = 10  # Dynamic population size\n        exploration_ratio = 0.8  # Percentage of budget spent on exploration\n        exploration_budget = int(self.budget * exploration_ratio)\n        exploitation_budget = self.budget - exploration_budget\n\n        # Phase 1: Exploration with Population-based Search\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        population_values = np.array([func(individual) for individual in population])\n        self.evaluations += population_size\n        \n        for _ in range(exploration_budget // population_size):\n            adaptive_step_size = (ub - lb) / 5\n            for i in range(population_size):\n                candidate = np.clip(population[i] + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n\n                if candidate_value < population_values[i]:\n                    population[i] = candidate\n                    population_values[i] = candidate_value\n                    adaptive_step_size *= 1.2\n                else:\n                    adaptive_step_size *= 0.9\n\n            # Reinitialize a portion of the population with a new random sample based on convergence\n            if np.std(population_values) < 0.1:\n                num_reinitialize = population_size // 4\n                reinitialize_indices = np.random.choice(population_size, num_reinitialize, replace=False)\n                population[reinitialize_indices] = np.random.uniform(lb, ub, (num_reinitialize, self.dim))\n                population_values[reinitialize_indices] = [func(ind) for ind in population[reinitialize_indices]]\n                self.evaluations += num_reinitialize\n\n        # Phase 2: Intensified Local Search\n        best_index = np.argmin(population_values)\n        best_solution = population[best_index]\n        best_value = population_values[best_index]\n        \n        while self.evaluations < self.budget:\n            adaptive_step_size = (ub - lb) / 10\n            candidate = np.clip(best_solution + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution = candidate\n                best_value = candidate_value\n                adaptive_step_size *= 1.1\n            else:\n                adaptive_step_size *= 0.95\n\n        return best_solution", "configspace": "", "generation": 64, "feedback": "The algorithm DynamicAdaptiveSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05585 with standard deviation 0.00267.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.05534109025141676, 0.05818118070699119, 0.05872060911310628, 0.05077891292698622, 0.05336265299135978, 0.053849835845625305, 0.055400333183682315, 0.05824392021418545, 0.05878407487035131]}}
{"id": "7c56b5fe-3e73-428c-9c22-d935ca62fd5b", "fitness": 0.061979898390606136, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced restart strategy in Adaptive Random Search for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 15  # Restart with an even smaller step size\n            max_no_improvement = 3  # Reduce max no improvement for faster restarts\n\n        return best_solution", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06198 with standard deviation 0.00282.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06501413743433548, 0.06219605708341547, 0.06409324797346716, 0.05952993271967277, 0.05687840852893056, 0.05858981299211252, 0.06508605386357746, 0.062265834626194394, 0.0641656002937494]}}
{"id": "26a447b7-bc25-4b64-8436-a677c9ad60dd", "fitness": 0.05927644051560288, "name": "EnhancedAdaptiveRandomSearch", "description": "EnhancedAdaptiveRandomSearch with dynamic adjustment of max_no_improvement count based on performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            adaptive_step_size = (ub - lb) / 5  # Initial larger step size\n            reduction_factor = 0.5  # Reduction factor for the step size\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Increased step size adjustment on improvement\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Adjusted step size reduction on no improvement\n\n                if no_improvement_count >= 5 + int(self.evaluations / self.budget * 5):  # Dynamically adjust based on progress\n                    adaptive_step_size *= reduction_factor  # Reduce step size scale\n                    no_improvement_count = 0  # Reset count\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            # Multi-start strategy: restart with a refined exploration\n            adaptive_step_size = (ub - lb) / 10  # Restart with a slightly smaller step size\n\n        return best_solution", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05928 with standard deviation 0.00526.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06441033392024165, 0.054165835076994684, 0.06432988643871207, 0.058992514165768783, 0.04968476789835352, 0.05879664113916694, 0.06448132353179603, 0.05422399987204474, 0.0644026625973475]}}
{"id": "e394df85-3b24-4132-92d9-c605ec86adc9", "fitness": 0.06203793573973146, "name": "EnhancedAdaptiveRandomSearch", "description": "Introduced adaptive restart mechanism to dynamically adjust exploration-exploitation trade-off.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.5\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1  # Dynamic adjustment of reduction factor\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06204 with standard deviation 0.00357.", "error": "", "parent_ids": ["8fb2577d-2019-4d72-9df9-557dcd4a2777"], "operator": null, "metadata": {"aucs": [0.06449917404562244, 0.06655115528055378, 0.06040391942597412, 0.059070027262411506, 0.060839520480333587, 0.05531045006693036, 0.06457032414223352, 0.06662631940977137, 0.06047053154375248]}}
{"id": "e2a9caa9-9406-4b17-9ded-e810406fa63b", "fitness": 0.06491026574192522, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced exploration via dynamic adaptive step size and strategic reduction factor.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.3  # Changed from 0.5 to 0.3\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Changed from 1.2 to 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85  # Changed from 0.9 to 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06491 with standard deviation 0.00517.", "error": "", "parent_ids": ["e394df85-3b24-4132-92d9-c605ec86adc9"], "operator": null, "metadata": {"aucs": [0.07273322766791812, 0.06165312438873516, 0.06603351936379409, 0.06623646886850654, 0.05649473587528864, 0.0603934732864424, 0.07281962366496242, 0.06172054929476434, 0.06610766926691536]}}
{"id": "8f5577e6-435e-40b2-961a-3cae0fa7f08f", "fitness": 0.059146496854626994, "name": "EnhancedAdaptiveRandomSearch", "description": "Refined exploration and exploitation balance through dynamic step size and adaptive reduction factor.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 4  # Changed from 5 to 4\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25  # Changed from 0.3 to 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.35  # Changed from 1.3 to 1.35\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05915 with standard deviation 0.00560.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.06799526374137688, 0.0587592425147051, 0.055724918792660394, 0.06212663966244303, 0.05389208386128497, 0.051140581658157824, 0.06807264437176741, 0.05882264642770496, 0.05578445066154236]}}
{"id": "eebc3c0b-90b7-44e8-b39b-9f54bb9168ee", "fitness": -Infinity, "name": "EnhancedAdaptiveRandomSearchV2", "description": "Exploiting historical knowledge with dynamic step size adjustments and memory to enhance convergence. ", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearchV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        # Memory to store past successful moves\n        self.memory = []\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.3\n\n            while self.evaluations < self.budget:\n                # Utilize memory of successful steps for exploration\n                if len(self.memory) > 0 and np.random.rand() < 0.5:\n                    memory_vector = np.random.choice(self.memory)\n                    candidate = np.clip(current_best + memory_vector, lb, ub)\n                else:\n                    candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                \n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    step_vector = candidate - current_best\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                    # Store successful steps in memory\n                    self.memory.append(step_vector)\n                    if len(self.memory) > 10:  # Limit memory size\n                        self.memory.pop(0)\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 70, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {}}
{"id": "7c9c5e96-5dd9-485a-a99e-d93cfc2c06da", "fitness": 0.053562816711758, "name": "DynamicFeedbackAdaptiveRandomSearch", "description": "Incorporates dynamic environmental feedback to adaptively adjust exploration and exploitation balance within the search space.", "code": "import numpy as np\n\nclass DynamicFeedbackAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.4  # Modified reduction factor\n            exploration_factor = 1.1  # New factor for exploration adjustment\n            exploitation_factor = 0.8  # New factor for exploitation adjustment\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= exploration_factor  # Explore more when improving\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= exploitation_factor  # Exploit more when not improving\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.05\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 71, "feedback": "The algorithm DynamicFeedbackAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05356 with standard deviation 0.00341.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.05876377869053362, 0.05233673669100303, 0.05407209741046015, 0.05388856788356944, 0.04803105368538907, 0.04962352447947216, 0.05882730321608909, 0.05239249647025668, 0.05412979187904876]}}
{"id": "463b71a2-ee24-4c29-8d1e-fce37fcb9ba1", "fitness": 0.058431970514309865, "name": "EnhancedAdaptiveRandomSearch", "description": "Introduce adaptive learning rate and memory-based restart strategy for enhanced convergence and escape from local optima.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            # Memory of best solutions\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            memory = [current_best]\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.3\n            learning_rate = 0.05  # Introduced learning rate\n\n            while self.evaluations < self.budget:\n                perturbation = np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim)\n                candidate = np.clip(current_best + perturbation, lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= (1 + learning_rate)  # Dynamic learning rate\n                    memory.append(current_best)  # Memory update\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= (1 - learning_rate)\n\n                if no_improvement_count >= max_no_improvement:\n                    if memory:  # Restart strategy using memory of past best solutions\n                        current_best = memory[np.random.randint(len(memory))]\n                        current_best_value = func(current_best)\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n            learning_rate = 0.05  # Reset learning rate\n\n        return best_solution", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05843 with standard deviation 0.00348.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.05922400185661458, 0.06363786980769004, 0.057390687882137414, 0.05431779275913684, 0.05823752988506503, 0.05263055163728225, 0.05928793618279837, 0.06370871201928441, 0.05745265259877985]}}
{"id": "3b8a70b5-b2cf-470f-9235-fd3c3f769949", "fitness": 0.05579509178192973, "name": "EnhancedAdaptiveRandomSearch", "description": "Integrate stochastic restarts and dynamic neighborhood size adjustment for improved exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.3\n            \n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n                    # Stochastic restart: Choose a new random point\n                    current_best = np.random.uniform(lb, ub)\n                    current_best_value = func(current_best)\n                    self.evaluations += 1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 8  # Adjusted from /10 to /8\n\n        return best_solution", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05580 with standard deviation 0.00446.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.05185853829177134, 0.061022020681242406, 0.059193488214828616, 0.04759625885988983, 0.05593798402665162, 0.05428801237910463, 0.051913706833477846, 0.061088402082211224, 0.05925741466819001]}}
{"id": "66430b06-cfd1-4085-a0eb-ee58cd6f3829", "fitness": 0.05916655350328981, "name": "EnhancedAdaptiveRandomSearch", "description": "Improved exploration-exploitation balance through optimized step size handling and adaptive reduction factor.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 4  # Changed from 5 to 4\n            reduction_factor = 0.35  # Changed from 0.3 to 0.35\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.4  # Changed from 1.3 to 1.4\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05917 with standard deviation 0.00521.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.061574872879485376, 0.05468231563953496, 0.0662689126235192, 0.056423041158163545, 0.050163474824772125, 0.06066062881409351, 0.061642210070193326, 0.05474098570106889, 0.06634253981877736]}}
{"id": "4cf6781c-0e55-43d2-9ed9-23af93d32cc2", "fitness": 0.05908313008874614, "name": "AdaptiveDiverseRandomSearch", "description": "Introduce a self-adaptive learning rate and a diversity-boost mechanism to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDiverseRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.3\n            diversity_boost = 0.1\n\n            while self.evaluations < self.budget:\n                # Introduce a diversity-boost mechanism\n                if np.random.rand() < diversity_boost:\n                    candidate = np.random.uniform(lb, ub)\n                else:\n                    candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                \n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 75, "feedback": "The algorithm AdaptiveDiverseRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05908 with standard deviation 0.00373.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.0602774694540017, 0.05745521268932674, 0.0645570621229844, 0.05525928924869972, 0.05264606171023045, 0.05906302673146013, 0.060342957139211006, 0.057517895266341434, 0.0646291964364597]}}
{"id": "023e7c51-9f9a-43d5-b3f3-72c67d6f98f4", "fitness": 0.059501511353466534, "name": "EnhancedAdaptiveRandomSearch", "description": "Adaptive exploration and exploitation with dynamic reduction of search space and adjustment based on local improvement trends.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.3\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.25  # Increased slightly for faster expansion\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Reduced more for better contraction\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    reduction_factor *= 1.05  # Slightly more aggressive on reduction factor\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05950 with standard deviation 0.00485.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.06535909600131506, 0.05521847702899185, 0.06301136968257903, 0.05974089952161077, 0.05066249622694141, 0.05773013124669202, 0.06543302169865228, 0.05527764037062355, 0.06308047040379283]}}
{"id": "d5a33855-0c22-4d8e-b643-4d943e5b04c3", "fitness": 0.06491026574192522, "name": "EnhancedAdaptiveRandomSearchWithMemory", "description": "Enhanced Dynamic Exploration with Adaptive Memory Integration for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearchWithMemory:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.history = []\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        while self.evaluations < self.budget:\n            # Select initial solution using previous knowledge\n            if len(self.history) > 0:\n                current_best = self.history[np.random.choice(len(self.history))]\n            else:\n                current_best = np.random.uniform(lb, ub)\n                \n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.3\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            self.history.append(current_best)\n            if len(self.history) > 10:\n                self.history.pop(0)\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedAdaptiveRandomSearchWithMemory got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06491 with standard deviation 0.00517.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.07273322766791812, 0.06165312438873516, 0.06603351936379409, 0.06623646886850654, 0.05649473587528864, 0.0603934732864424, 0.07281962366496242, 0.06172054929476434, 0.06610766926691536]}}
{"id": "a18d0303-9fe1-413e-9d75-a8f35f11b440", "fitness": 0.06491026574192522, "name": "MemoryAdaptiveRandomSearch", "description": "Adaptive Random Search with Memory-based Exploration: Integrates memory of past best solutions to guide exploration and exploit adaptive step sizes dynamically.", "code": "import numpy as np\n\nclass MemoryAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        memory = []\n\n        while self.evaluations < self.budget:\n            if memory and np.random.rand() < 0.3:\n                current_best = memory[np.random.choice(len(memory))]\n            else:\n                current_best = np.random.uniform(lb, ub)\n\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.3\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n                memory.append(current_best)  # Store in memory\n\n            if len(memory) > 10:  # Limit memory size\n                memory.pop(0)\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 78, "feedback": "The algorithm MemoryAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06491 with standard deviation 0.00517.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.07273322766791812, 0.06165312438873516, 0.06603351936379409, 0.06623646886850654, 0.05649473587528864, 0.0603934732864424, 0.07281962366496242, 0.06172054929476434, 0.06610766926691536]}}
{"id": "f336f39d-25a1-4e76-b78f-6b4725435c5b", "fitness": 0.068573858677414, "name": "EnhancedAdaptiveRandomSearch", "description": "Improved balance of exploration and exploitation by refining adaptation factors.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25  # Changed from 0.3 to 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Changed from 1.3 to 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Changed from 0.85 to 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06857 with standard deviation 0.00931.", "error": "", "parent_ids": ["e2a9caa9-9406-4b17-9ded-e810406fa63b"], "operator": null, "metadata": {"aucs": [0.08288321202872861, 0.06818137838246752, 0.06083439111706779, 0.07514018679468659, 0.062238530766171696, 0.0557388677373174, 0.08298733724452023, 0.06825985288770686, 0.06090097113805937]}}
{"id": "affd07aa-868a-4a84-95ba-0d3770b97ab7", "fitness": 0.05782111784863697, "name": "EnhancedAdaptiveRandomSearch", "description": "Fine-tuning exploration-exploitation balance with improved adaptive step size controls.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.2  # Changed from 0.25 to 0.2\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.3  # Changed from 1.2 to 1.3\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.7  # Changed from 0.8 to 0.7\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05782 with standard deviation 0.00672.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.05995483429292814, 0.0512769839205266, 0.06713580768417193, 0.05498283291941375, 0.04705206693534081, 0.06142534235381514, 0.060019665330680816, 0.051331657987703116, 0.06721086921315245]}}
{"id": "c9afb584-bcae-492f-9aa6-8686fa1b2fee", "fitness": 0.06784077625746664, "name": "EnhancedAdaptiveRandomSearch", "description": "Further refined adaptation process by dynamically adjusting reduction factor based on improvement history.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25  # Changed from 0.3 to 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Changed from 1.3 to 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Changed from 0.85 to 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.05  # Changed from 1.1 to 1.05\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06784 with standard deviation 0.01031.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.08326391993717952, 0.06803858978329114, 0.05832819946000567, 0.07546509569351967, 0.06211397837721744, 0.05348017244746828, 0.0833688590099948, 0.06811680446244905, 0.05839136714607418]}}
{"id": "9ee4e34e-8599-443d-8b99-45734290e270", "fitness": 0.06151115955872782, "name": "EnhancedAdaptiveRandomSearch", "description": "Refine adaptation factors and introduce dynamic reduction strategy for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.2  # Changed from 0.25 to 0.2\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.15  # Changed from 1.2 to 1.15\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85  # Changed from 0.8 to 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06151 with standard deviation 0.00911.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.07212237851381931, 0.050905126139421775, 0.06686557057496056, 0.06575063342283793, 0.046703740087548073, 0.0611457971312267, 0.07220688505044404, 0.050959489376985245, 0.06694081573130672]}}
{"id": "adaf858b-d327-4c89-9313-0b9cd5187d82", "fitness": 0.060477363505391374, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhance exploitation by adjusting step size dynamics based on improvement count.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9  # Changed from 0.8 to 0.9\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.05  # Changed from 1.1 to 1.05\n\n                if no_improvement_count == 3:  # Added to adjust step size dynamically\n                    adaptive_step_size *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06048 with standard deviation 0.00296.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.06438837134758824, 0.06183438164376265, 0.06037250158961893, 0.058975855799819454, 0.056618991649609596, 0.05530553052616738, 0.06445928236819554, 0.06190264489986763, 0.060438711723892924]}}
{"id": "1b4d13a8-796e-41be-a48b-c0df7132cb3b", "fitness": 0.06115132044587469, "name": "EnhancedAdaptiveRandomSearch", "description": "Further refined adaptation of step sizes to enhance local search efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 4.5  # Changed from 5 to 4.5\n            reduction_factor = 0.2  # Changed from 0.25 to 0.2\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.15  # Changed from 1.2 to 1.15\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06115 with standard deviation 0.00360.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.06060646927684321, 0.0614325803404594, 0.06662765677573246, 0.055562223643744635, 0.05626381227974786, 0.06099504619509721, 0.0606723074904274, 0.06150018612907715, 0.0667016018817429]}}
{"id": "13f683df-0db9-4005-8fc9-c2df5b8f9d5e", "fitness": 0.0679649575330213, "name": "EnhancedDynamicAdaptiveSearch", "description": "Introducing a dynamic adjustment of reduction factors and adaptive step sizes based on performance metrics to enhance convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicAdaptiveSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= (1.05 + 0.05 * (self.evaluations / self.budget))\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n            \n        return best_solution", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedDynamicAdaptiveSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06796 with standard deviation 0.01019.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.08327389206998548, 0.06808115579511109, 0.05866133416278041, 0.07547276663079172, 0.06215112925607513, 0.05378108028252304, 0.08337886690271301, 0.06815944758501913, 0.05872494511219273]}}
{"id": "274789d2-a663-4d61-afb7-5d2810b09a56", "fitness": 0.03888936804725288, "name": "EnhancedVarianceAdaptiveSearch", "description": "Implement a dynamic adaptation mechanism with variance-based learning rates to enhance exploitation efficiency.", "code": "import numpy as np\n\nclass EnhancedVarianceAdaptiveSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25\n            variance_factor = np.var(current_best)\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.1 + 0.1 * variance_factor\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.9 - 0.1 * variance_factor\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.05\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedVarianceAdaptiveSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03889 with standard deviation 0.00197.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.04018402179912073, 0.03852456671399107, 0.04132340499692755, 0.0367661988571899, 0.0352119409325623, 0.03783100280603979, 0.040227883965137656, 0.03856703444973575, 0.041368257904571215]}}
{"id": "6b605ee0-406a-48ff-80fe-7caf4bd96a40", "fitness": 0.058662797382165205, "name": "EnhancedAdaptiveRandomSearch", "description": "Further improved balance of exploration and exploitation by modifying step size adaptation and reduction factors.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.2  # Changed from 0.25 to 0.2\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.1  # Changed from 1.2 to 1.1\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85  # Changed from 0.8 to 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05866 with standard deviation 0.00722.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.06948614957448451, 0.05239699308960266, 0.059140007766129044, 0.06344933517872064, 0.04807146769861381, 0.05419781341173158, 0.06956588214546555, 0.052453037154525095, 0.05920449042021392]}}
{"id": "295e4309-e6fc-44aa-b9b7-d9cc03496c0f", "fitness": 0.05081176665067851, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced adaptive approach with improved exploration via dynamic step size adjustments.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 4  # Changed from 5 to 4\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.15  # Changed from 1.2 to 1.15\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05081 with standard deviation 0.00427.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.052655976372583235, 0.04732182602489032, 0.056714251703221974, 0.04832609443014835, 0.04340679808847314, 0.05202127890758046, 0.05271206196283129, 0.04737232434371785, 0.056775288022659964]}}
{"id": "29039e2c-6b69-42b3-a004-d7a9454edda1", "fitness": 0.04127331132090761, "name": "GradientPerturbationSearch", "description": "Enhanced dynamic adaptation through gradient-based perturbations for improved convergence.", "code": "import numpy as np\n\nclass GradientPerturbationSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        def estimate_gradient(x):\n            epsilon = 1e-8\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                delta = np.zeros(self.dim)\n                delta[i] = epsilon\n                grad[i] = (func(np.clip(x + delta, lb, ub)) - func(np.clip(x - delta, lb, ub))) / (2 * epsilon)\n                self.evaluations += 2\n            return grad\n\n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25\n\n            while self.evaluations < self.budget:\n                grad = estimate_gradient(current_best)\n                candidate = np.clip(current_best - adaptive_step_size * grad, lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 89, "feedback": "The algorithm GradientPerturbationSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04127 with standard deviation 0.00351.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.04018402295679835, 0.04690771048152598, 0.04027459559189961, 0.03676619994006003, 0.04297983903385616, 0.03684247092393467, 0.04022788512381004, 0.04695842247917592, 0.04031865535710777]}}
{"id": "ee4801ac-e26f-4ea0-afeb-807c1822b76d", "fitness": 0.068573858677414, "name": "EnhancedAdaptiveRandomSearch", "description": "Further improved adaptation by introducing dynamic adjustment of the maximum no-improvement count.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25  # Changed from 0.3 to 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2  # Changed from 1.3 to 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8  # Changed from 0.85 to 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n                    max_no_improvement = int(max_no_improvement * 1.1)  # Changed to dynamic adjustment\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06857 with standard deviation 0.00931.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.08288321202872861, 0.06818137838246752, 0.06083439111706779, 0.07514018679468659, 0.062238530766171696, 0.0557388677373174, 0.08298733724452023, 0.06825985288770686, 0.06090097113805937]}}
{"id": "c518fea4-e74e-4d23-9e64-5be60c6d9f8d", "fitness": 0.053735998511190854, "name": "EnhancedAdaptiveRandomSearch", "description": "Fine-tuned adaptive step size and improvement check frequency with increased reduction factor for better convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 3  # Changed from 5 to 3\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.3  # Changed from 0.25 to 0.3\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05374 with standard deviation 0.00796.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.06640990941599045, 0.04993008298599122, 0.04943084076573423, 0.06073273124362455, 0.0458123250131266, 0.045356749090633586, 0.06648456165058858, 0.04998331052971661, 0.04948347590531188]}}
{"id": "f271cb3a-de31-4053-b213-2b9cc1678535", "fitness": -Infinity, "name": "EnhancedAdaptiveRandomSearchV2", "description": "Adaptive Exploration with Dynamic Step Reduction and Population-Based Refinement to Improve Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearchV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial configuration\n        population_size = 5\n        adaptive_step_size = (ub - lb) / 5\n        reduction_factor = 0.25\n        max_no_improvement = 5\n        exploration_factor = 0.1\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        population_values = np.array([func(ind) for ind in population])\n        self.evaluations += population_size\n        \n        while self.evaluations < self.budget:\n            # Sort population by their fitness\n            sorted_indices = np.argsort(population_values)\n            population = population[sorted_indices]\n            population_values = population_values[sorted_indices]\n            current_best = population[0]\n            current_best_value = population_values[0]\n            no_improvement_count = 0\n            \n            for i in range(population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                candidate = np.clip(population[i] + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < population_values[i]:\n                    population[i] = candidate\n                    population_values[i] = candidate_value\n                    if candidate_value < current_best_value:\n                        current_best = candidate\n                        current_best_value = candidate_value\n                        no_improvement_count = 0\n                        adaptive_step_size = min((ub - lb) * exploration_factor, adaptive_step_size * 1.2)\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85\n\n                # Dynamic step size reduction and exploration\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.05\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n        return best_solution", "configspace": "", "generation": 92, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {}}
{"id": "74a89846-b8db-4faa-8943-0a2303984842", "fitness": 0.06003686192377869, "name": "EnhancedAdaptiveRandomSearch", "description": "Enhanced exploration through dynamic step size adjustment and stochastic perturbations.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 7  # Changed from 5 to 7\n            adaptive_step_size = (ub - lb) / 6  # Changed from 5 to 6\n            reduction_factor = 0.2  # Changed from 0.25 to 0.2\n\n            while self.evaluations < self.budget:\n                stochastic_perturbation = np.random.normal(scale=adaptive_step_size / 10, size=self.dim)  # New line\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim) + stochastic_perturbation, lb, ub)  # Modified line\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.1  # Changed from 1.2 to 1.1\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.75  # Changed from 0.8 to 0.75\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.15  # Changed from 1.1 to 1.15\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 8  # Changed from 10 to 8\n\n        return best_solution", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06004 with standard deviation 0.00575.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.0676877287405836, 0.06283795869660036, 0.05468581518515625, 0.061929422601474604, 0.057587450705361976, 0.05018916302582466, 0.06776344101612097, 0.06290662008429393, 0.05474415725859183]}}
{"id": "7100483b-d9e8-41ae-a335-b86d013f37d6", "fitness": 0.06743838149020959, "name": "EnhancedAdaptiveRandomSearch", "description": "Fine-tune the reduction factor to improve convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.2  # Changed from 0.25 to 0.2\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06744 with standard deviation 0.01154.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.08472104632068689, 0.06707107668565504, 0.05660841775900938, 0.07670435515617258, 0.061266174691289144, 0.05192832318757912, 0.08482917169917026, 0.06714759322842168, 0.05666927468390226]}}
{"id": "2e084902-1d9d-4b2b-99b3-62bddaad7068", "fitness": 0.05968014618653214, "name": "EnhancedAdaptiveRandomSearch", "description": "Fine-tune exploration by adjusting step size dynamics and reduction factor.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.2  # Changed from 0.25 to 0.2\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.25  # Changed from 1.2 to 1.25\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.75  # Changed from 0.8 to 0.75\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05968 with standard deviation 0.00510.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.06468951436584303, 0.05485005195968695, 0.06458387029837143, 0.05923815897954787, 0.05032345519116266, 0.05911079373490313, 0.06476097343197795, 0.05490881683701032, 0.06465568088028595]}}
{"id": "9c38cba5-d27f-45bd-8a5e-0e6915ef6ad6", "fitness": 0.05921899002461038, "name": "EnhancedAdaptiveRandomSearch", "description": "Fine-tuned balance between exploration and exploitation using adaptive mechanisms for step size adjustments.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25  # Changed from 0.3 to 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.15  # Changed from 1.2 to 1.15\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.75  # Changed from 0.8 to 0.75\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.05  # Changed from 1.1 to 1.05\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05922 with standard deviation 0.00452.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.06020966207182643, 0.06603688206330216, 0.05644160292353595, 0.05520719889885517, 0.060407463602041855, 0.05178011734436483, 0.06027492036541293, 0.06611086133361044, 0.05650220161854369]}}
{"id": "0bc9e396-8a2c-4d4d-96cf-2259120f169e", "fitness": 0.0634300505384186, "name": "EnhancedAdaptiveRandomSearch", "description": "Further enhanced adaptive random search with refined step-size adaptation rules.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25  # Changed from 0.3 to 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.15  # Changed from 1.2 to 1.15\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85  # Changed from 0.8 to 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06343 with standard deviation 0.00590.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.06949146628619163, 0.05758151946538925, 0.06874497854873729, 0.06345743423175965, 0.05277525605526212, 0.06278078835906353, 0.06957115509884348, 0.05764415067768025, 0.06882370612284017]}}
{"id": "cab870a8-10c9-45b4-8189-124c4c82230d", "fitness": 0.06151115955872782, "name": "EnhancedAdaptiveRandomSearch", "description": "Improved exploration-exploitation balance by adjusting step size evolution and reduction factors.", "code": "import numpy as np\n\nclass EnhancedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.2  # Changed from 0.25 to 0.2\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.15  # Changed from 1.2 to 1.15\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.85  # Changed from 0.8 to 0.85\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    no_improvement_count = 0\n                    reduction_factor *= 1.1\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06151 with standard deviation 0.00911.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.07212237851381931, 0.050905126139421775, 0.06686557057496056, 0.06575063342283793, 0.046703740087548073, 0.0611457971312267, 0.07220688505044404, 0.050959489376985245, 0.06694081573130672]}}
{"id": "8019bfc2-0550-43ba-a009-0ea05c1ef73a", "fitness": 0.0672012295384627, "name": "RefinedAdaptiveRandomSearch", "description": "Introduce a dynamic reduction factor and adaptive step size reset mechanism for enhanced convergence in global optimization tasks.", "code": "import numpy as np\n\nclass RefinedAdaptiveRandomSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        while self.evaluations < self.budget:\n            current_best = np.random.uniform(lb, ub)\n            current_best_value = func(current_best)\n            self.evaluations += 1\n            no_improvement_count = 0\n            max_no_improvement = 5\n            adaptive_step_size = (ub - lb) / 5\n            reduction_factor = 0.25\n\n            while self.evaluations < self.budget:\n                candidate = np.clip(current_best + np.random.uniform(-adaptive_step_size, adaptive_step_size, self.dim), lb, ub)\n                candidate_value = func(candidate)\n                self.evaluations += 1\n                \n                if candidate_value < current_best_value:\n                    current_best = candidate\n                    current_best_value = candidate_value\n                    no_improvement_count = 0\n                    adaptive_step_size *= 1.2\n                else:\n                    no_improvement_count += 1\n                    adaptive_step_size *= 0.8\n\n                if no_improvement_count >= max_no_improvement:\n                    adaptive_step_size *= reduction_factor\n                    reduction_factor = 0.25 + 0.75 * np.random.rand()\n                    no_improvement_count = 0\n\n            if current_best_value < best_value:\n                best_solution = current_best\n                best_value = current_best_value\n\n            adaptive_step_size = (ub - lb) / 10\n\n        return best_solution", "configspace": "", "generation": 99, "feedback": "The algorithm RefinedAdaptiveRandomSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06720 with standard deviation 0.01112.", "error": "", "parent_ids": ["f336f39d-25a1-4e76-b78f-6b4725435c5b"], "operator": null, "metadata": {"aucs": [0.08252361917018125, 0.06988537029100772, 0.055264055805950285, 0.07480338177777923, 0.06370980561829143, 0.05070692922018738, 0.08262748184090929, 0.06996718976554417, 0.055323232356313645]}}
