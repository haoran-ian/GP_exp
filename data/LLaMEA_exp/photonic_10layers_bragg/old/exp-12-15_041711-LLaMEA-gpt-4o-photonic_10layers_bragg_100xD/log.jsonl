{"id": "03afffd9-1c32-4322-9f96-ad7c9eec8907", "fitness": 0.0698340253123206, "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining elements of particle swarm optimization and differential evolution for efficient exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.7\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Update velocities and positions using PSO\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply DE mutation and crossover\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant_vector = personal_best_positions[a] + differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 0, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06983 with standard deviation 0.00925.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.077303223277573, 0.07539686398573409, 0.05680198867365471]}}
{"id": "e6eb24cc-49ee-4413-bfc7-552c40c1de28", "fitness": 0.07630758114751957, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer integrating adaptive inertia and mutation strategies for improved convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Update velocities and positions using PSO with adaptive inertia\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply adaptive DE mutation and crossover\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07631 with standard deviation 0.00249.", "error": "", "parent_ids": ["03afffd9-1c32-4322-9f96-ad7c9eec8907"], "operator": null, "metadata": {"aucs": [0.0795759458706159, 0.0758092409392005, 0.0735375566327423]}}
{"id": "9088a637-ef61-4d1c-891a-8e6c00f8a9e3", "fitness": 0.07630758114751957, "name": "SynergisticAdaptiveSwarmHybridization", "description": "Synergistic Adaptive Swarm Hybridization (SASH) integrates adaptive PSO inertia, DE mutation, and learning phases with dynamic population adjustments for black box optimization.", "code": "import numpy as np\n\nclass SynergisticAdaptiveSwarmHybridization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Adaptive inertia adjustment\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            population_scaling_factor = 1 + (0.5 * eval_count / self.budget)\n            \n            # Update velocities and positions using PSO with adaptive inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply adaptive DE mutation and crossover\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 2, "feedback": "The algorithm SynergisticAdaptiveSwarmHybridization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07631 with standard deviation 0.00249.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.0795759458706159, 0.0758092409392005, 0.0735375566327423]}}
{"id": "b5737020-2814-441a-b39d-0109994af7f0", "fitness": 0.06915778163440744, "name": "AdaptiveQuantumInspiredHybridOptimizer", "description": "Adaptive Quantum-Inspired Hybrid Optimizer incorporating quantum-inspired search dynamics and adaptive parameters for enhanced exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass AdaptiveQuantumInspiredHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Update velocities and positions using PSO with adaptive inertia\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply adaptive DE mutation and crossover\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                \n                # Quantum-inspired position update\n                quantum_factor = np.random.uniform(-1, 1, self.dim)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c]) + quantum_factor * (particles[i] - global_best_position)\n                \n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveQuantumInspiredHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06916 with standard deviation 0.00597.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.07356840185364566, 0.07318990083549726, 0.06071504221407942]}}
{"id": "56740d66-3f8d-4532-b552-ac72722033ab", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Optimizer with dynamic population size and chaotic search to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Update velocities and positions using PSO with adaptive inertia\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply adaptive DE mutation and crossover\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n            # Dynamic population adjustment\n            if eval_count < self.budget * 0.5:\n                num_particles = int(30 + (20 * np.sin(0.1 * eval_count)))  # Change 1\n\n            # Introduce chaotic search perturbation\n            chaotic_factor = 0.0001 * eval_count / self.budget  # Change 2\n            particles += chaotic_factor * np.random.uniform(-1, 1, particles.shape)  # Change 3\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 4, "feedback": "An exception occurred: IndexError('index 30 is out of bounds for axis 0 with size 30').", "error": "IndexError('index 30 is out of bounds for axis 0 with size 30')", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {}}
{"id": "0fc9ab2f-38c5-4157-8268-95844186d0ba", "fitness": 0.07158148683637393, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with dynamic cognitive and social components for improved performance in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Update velocities and positions using PSO with adaptive inertia\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            # Dynamic adjustment of cognitive and social components\n            cognitive_component = 1.5 + (0.5 * eval_count / self.budget)\n            social_component = 1.5 - (0.5 * eval_count / self.budget)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply adaptive DE mutation and crossover\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07158 with standard deviation 0.00664.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.07909954576251066, 0.07268791010044673, 0.06295700464616438]}}
{"id": "a7f60908-c95b-4ca5-ad2e-61b3022510c7", "fitness": 0.05511623649395888, "name": "AdvancedHybridOptimizer", "description": "Advanced Hybrid Optimizer with Adaptive Learning Rates and Multi-phase Exploration for Enhanced Convergence.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n        adapt_rate = 0.05  # New adaptive learning rate\n\n        while eval_count < self.budget:\n            learning_factor = 0.1 + 0.9 * (eval_count / self.budget)  # New learning factor\n\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight *= 0.99  # Adjust inertia weight dynamically\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            velocities = np.clip(velocities, -1, 1)  # Limit velocity\n\n            particles += velocities * learning_factor  # Apply learning factor\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 6, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05512 with standard deviation 0.00390.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.04990454780996556, 0.05614760818472464, 0.05929655348718643]}}
{"id": "b09a5de9-fe75-42ad-ad0f-217237e158e1", "fitness": 0.07157098408539464, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Optimizer incorporating dynamic component adjustments and diversity preservation for enhanced global exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        global_best_position = None\n        global_best_score = float('inf')\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c, d = np.random.choice(indices, 4, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c] + personal_best_positions[d] - global_best_position)\n                if np.random.rand() < 0.5:\n                    cross_points = np.random.rand(self.dim) < crossover_rate\n                else:\n                    cross_points = np.random.rand(self.dim) > crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07157 with standard deviation 0.00942.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.07312477338825008, 0.08225378294984276, 0.05933439591809109]}}
{"id": "deccaf04-354c-4e8e-a939-856fd83cc080", "fitness": 0.07360021316627752, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with dynamic cognitive and social components that improve convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Update velocities and positions using PSO with adaptive inertia\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            cognitive_component = 1.5 + (0.5 * eval_count / self.budget)  # Change 1: Dynamic cognitive component\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply adaptive DE mutation and crossover\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07360 with standard deviation 0.00496.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.0803815222547899, 0.0717547197118511, 0.06866439753219156]}}
{"id": "c025f9a2-c6e4-4263-807e-33549288ad37", "fitness": 0.06953426338067248, "name": "AdaptiveMetaheuristicOptimizer", "description": "Adaptive Metaheuristic Optimizer with dynamic parameter control and elite archival strategy for improved exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight_initial = 0.9\n        inertia_weight_final = 0.4\n        cognitive_component_initial = 2.0\n        cognitive_component_final = 1.0\n        social_component_initial = 1.0\n        social_component_final = 2.0\n        differential_weight_initial = 0.9\n        differential_weight_final = 0.5\n        crossover_rate_initial = 0.9\n        crossover_rate_final = 0.6\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best and elite archive initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        elite_archive = []\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n                    if len(elite_archive) < num_particles:\n                        elite_archive.append((particles[i], score))\n                    else:\n                        worst_in_elite = max(elite_archive, key=lambda x: x[1])\n                        if score < worst_in_elite[1]:\n                            elite_archive.remove(worst_in_elite)\n                            elite_archive.append((particles[i], score))\n\n                if eval_count >= self.budget:\n                    break\n\n            # Dynamically adjust parameters\n            inertia_weight = inertia_weight_initial - (inertia_weight_initial - inertia_weight_final) * (eval_count / self.budget)\n            cognitive_component = cognitive_component_initial - (cognitive_component_initial - cognitive_component_final) * (eval_count / self.budget)\n            social_component = social_component_initial + (social_component_final - social_component_initial) * (eval_count / self.budget)\n            differential_weight = differential_weight_initial - (differential_weight_initial - differential_weight_final) * (eval_count / self.budget)\n            crossover_rate = crossover_rate_initial - (crossover_rate_initial - crossover_rate_final) * (eval_count / self.budget)\n\n            # Update velocities and positions using PSO with adaptive parameters\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply DE mutation and crossover with adaptive parameters\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant_vector = personal_best_positions[a] + differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n                    if len(elite_archive) < num_particles:\n                        elite_archive.append((trial_vector, trial_score))\n                    else:\n                        worst_in_elite = max(elite_archive, key=lambda x: x[1])\n                        if trial_score < worst_in_elite[1]:\n                            elite_archive.remove(worst_in_elite)\n                            elite_archive.append((trial_vector, trial_score))\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06953 with standard deviation 0.00208.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.0710784093387955, 0.07092812697939022, 0.06659625382383172]}}
{"id": "3877f324-ddc9-4d3a-930f-5dcfaae7ded5", "fitness": 0.06624809287889304, "name": "AdaptiveMultiSwarmHybridOptimizer", "description": "Adaptive Multi-Swarm Hybrid Optimizer combining parallel particle swarms with adaptive differential evolution to enhance diversity and convergence in black box optimization.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_swarms = 3\n        particles_per_swarm = 10\n        total_particles = num_swarms * particles_per_swarm\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        lb, ub = func.bounds.lb, func.bounds.ub\n\n        # Initialize all particles for each swarm\n        all_particles = np.random.uniform(lb, ub, (total_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (total_particles, self.dim))\n        personal_best_positions = np.copy(all_particles)\n        personal_best_scores = np.array([float('inf')] * total_particles)\n\n        # Global best initialization per swarm\n        global_best_positions = np.copy(personal_best_positions[:num_swarms])\n        global_best_scores = np.array([float('inf')] * num_swarms)\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for swarm_index in range(num_swarms):\n                start_index = swarm_index * particles_per_swarm\n                end_index = start_index + particles_per_swarm\n                swarm_particles = all_particles[start_index:end_index]\n                swarm_velocities = velocities[start_index:end_index]\n                swarm_personal_best_positions = personal_best_positions[start_index:end_index]\n                swarm_personal_best_scores = personal_best_scores[start_index:end_index]\n\n                # Evaluate all particles in the current swarm\n                for i in range(particles_per_swarm):\n                    score = func(swarm_particles[i])\n                    eval_count += 1\n\n                    if score < swarm_personal_best_scores[i]:\n                        swarm_personal_best_scores[i] = score\n                        swarm_personal_best_positions[i] = swarm_particles[i]\n\n                    if score < global_best_scores[swarm_index]:\n                        global_best_scores[swarm_index] = score\n                        global_best_positions[swarm_index] = swarm_particles[i]\n\n                    if eval_count >= self.budget:\n                        break\n\n                # Update velocities and positions using PSO with adaptive inertia\n                inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                swarm_velocities = (inertia_weight * swarm_velocities +\n                                    cognitive_component * r1 * (swarm_personal_best_positions - swarm_particles) +\n                                    social_component * r2 * (global_best_positions[swarm_index] - swarm_particles))\n                swarm_particles += swarm_velocities\n                swarm_particles = np.clip(swarm_particles, lb, ub)\n\n                # Apply adaptive DE mutation and crossover to current swarm\n                for i in range(particles_per_swarm):\n                    if eval_count >= self.budget:\n                        break\n                    indices = list(range(particles_per_swarm))\n                    indices.remove(i)\n                    a, b, c = np.random.choice(indices, 3, replace=False) + start_index\n                    adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                    mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                    cross_points = np.random.rand(self.dim) < crossover_rate\n                    trial_vector = np.where(cross_points, mutant_vector, swarm_particles[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_score = func(trial_vector)\n                    eval_count += 1\n\n                    if trial_score < swarm_personal_best_scores[i]:\n                        swarm_personal_best_scores[i] = trial_score\n                        swarm_personal_best_positions[i] = trial_vector\n\n                    if trial_score < global_best_scores[swarm_index]:\n                        global_best_scores[swarm_index] = trial_score\n                        global_best_positions[swarm_index] = trial_vector\n\n                # Update global best positions and scores\n                personal_best_positions[start_index:end_index] = swarm_personal_best_positions\n                personal_best_scores[start_index:end_index] = swarm_personal_best_scores\n                all_particles[start_index:end_index] = swarm_particles\n                velocities[start_index:end_index] = swarm_velocities\n\n        # Find the best global position and score across all swarms\n        best_global_index = np.argmin(global_best_scores)\n        best_global_position = global_best_positions[best_global_index]\n        best_global_score = global_best_scores[best_global_index]\n\n        return best_global_position, best_global_score", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveMultiSwarmHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06625 with standard deviation 0.00180.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.06595764849013752, 0.06858311868045042, 0.06420351146609116]}}
{"id": "aa600830-2efc-43a0-bd0f-e3c733980258", "fitness": 0.07363422154807982, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer with dynamic parameter tuning using a feedback mechanism for enhanced adaptive capabilities in black box optimization.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        max_inertia_weight = 0.9\n        min_inertia_weight = 0.4\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n        improvement_count = 0\n        \n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    improvement_count += 1\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Update velocities and positions using PSO with feedback-based dynamic inertia\n            inertia_weight = max_inertia_weight - (max_inertia_weight - min_inertia_weight) * (improvement_count / eval_count)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply adaptive DE mutation and crossover\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    improvement_count += 1\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 11, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07363 with standard deviation 0.00675.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.0806858266761642, 0.07568217043232028, 0.06453466753575499]}}
{"id": "bce7aec2-51c9-4728-95a4-d7b2a6277bd7", "fitness": 0.07980099158701821, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Adaptive Exploration-Exploitation Hybrid Optimizer with dynamic particle clustering and chaos-induced mutation for enhanced global search capabilities.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(eval_count * np.pi / self.budget)  # chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07980 with standard deviation 0.00278.", "error": "", "parent_ids": ["e6eb24cc-49ee-4413-bfc7-552c40c1de28"], "operator": null, "metadata": {"aucs": [0.08230143547769475, 0.08117628726116699, 0.07592525202219291]}}
{"id": "0a4190ff-6c86-40f6-b95b-66494d9d7d38", "fitness": 0.07780498888559717, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with adaptive inertia weight and local neighborhood search for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.4 + 0.5 * (self.budget - eval_count) / self.budget  # Adaptive inertia weight\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(eval_count * np.pi / self.budget)  # chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n                # Local neighborhood search\n                local_indices = np.random.choice(indices, 2, replace=False)\n                local_best_position = personal_best_positions[local_indices].mean(axis=0)\n                local_search_vector = particles[i] + 0.1 * (local_best_position - particles[i])\n                local_search_vector = np.clip(local_search_vector, lb, ub)\n\n                local_search_score = func(local_search_vector)\n                eval_count += 1\n\n                if local_search_score < personal_best_scores[i]:\n                    personal_best_scores[i] = local_search_score\n                    personal_best_positions[i] = local_search_vector\n\n                if local_search_score < global_best_score:\n                    global_best_score = local_search_score\n                    global_best_position = local_search_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07780 with standard deviation 0.00944.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.08810179214049929, 0.08002158260475889, 0.06529159191153333]}}
{"id": "b9b018c0-8761-4038-9f0e-77da5bbd41d2", "fitness": 0.07753197213630263, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "EnhancedHybridMetaheuristicOptimizer with adaptive learning rates and levy flight for improved exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                \n                # Apply Levy flight for exploration\n                levy_flight = 0.01 * levy_distribution(self.dim)\n                mutant_vector += levy_flight\n                \n                chaos = np.sin(eval_count * np.pi / self.budget)  # chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score\n\ndef levy_distribution(dim, beta=1.5):\n    sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n    u = np.random.normal(0, sigma, dim)\n    v = np.random.normal(0, 1, dim)\n    return u / np.abs(v)**(1 / beta)", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07753 with standard deviation 0.00436.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.08218607506191922, 0.07871668408228694, 0.07169315726470171]}}
{"id": "680cf450-f22b-4114-aae8-2927d94cc0bb", "fitness": 0.0784149914287912, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved Adaptive Exploration-Exploitation Hybrid Optimizer with enhanced velocity update and adaptive crossover for better convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.9 * inertia_weight * velocities +  # Changed line for enhanced velocity update\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(eval_count * np.pi / self.budget)  # chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                crossover_rate = 0.9 - 0.3 * eval_count / self.budget  # Changed line for adaptive crossover\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07841 with standard deviation 0.00424.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.08318161134044189, 0.07917442895512938, 0.07288893399080232]}}
{"id": "2823309a-845c-431a-88fb-2c01fab8f625", "fitness": 0.07630879879522963, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced particle swarm optimizer with chaos-enhanced mutation and adaptive crossover for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.9 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = 0.5 * np.sin(eval_count * np.pi / self.budget)  # enhanced chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                adaptive_crossover_rate = 0.8 + 0.2 * np.sin(np.pi * eval_count / self.budget)  # adaptive crossover\n                cross_points = np.random.rand(self.dim) < adaptive_crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07631 with standard deviation 0.00790.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.0856904148615687, 0.07686279175384869, 0.06637318977027151]}}
{"id": "237e1a51-ee1e-4c9b-ab16-ce6d9ffab35b", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced particle selection using Lvy flights to improve exploration and convergence in the optimization process.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n\n        def levy_flight(Lambda):\n            sigma1 = np.power((np.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2)) /\n                              (np.gamma((1 + Lambda) / 2) * Lambda * \n                               np.power(2, (Lambda - 1) / 2)), 1 / Lambda)\n            u = np.random.normal(0, sigma1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            step = u / np.power(np.abs(v), 1 / Lambda)\n            return step\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities + levy_flight(1.5)\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(eval_count * np.pi / self.budget)  # chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 17, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {}}
{"id": "e32abd58-145e-4790-9f65-67c21e735690", "fitness": 0.07856994502388688, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced a dynamic adjustment to the crossover rate based on evaluation progress for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(eval_count * np.pi / self.budget)  # chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < (crossover_rate * (1 - eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07857 with standard deviation 0.00644.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.08351716792230335, 0.08272191069109058, 0.06947075645826672]}}
{"id": "713a2913-11c0-41e7-92fe-3b19122685cd", "fitness": 0.05703932004669091, "name": "AdaptiveParticleCooperationOptimizer", "description": "Adaptive Particle-Cooperation Optimizer with inertia decay, cooperative fusion, and chaos-driven diversity preservation for superior global convergence.", "code": "import numpy as np\n\nclass AdaptiveParticleCooperationOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        cooperation_factor = 0.5\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cooperation_component = cooperation_factor * np.mean(personal_best_positions, axis=0)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles) +\n                          cooperation_component - particles)\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(eval_count * np.pi / self.budget)  # chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 19, "feedback": "The algorithm AdaptiveParticleCooperationOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05704 with standard deviation 0.00474.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.06294485289732676, 0.05684576820336751, 0.05132733903937847]}}
{"id": "1502b665-2292-4a3a-8857-cbcaacfebba6", "fitness": 0.07097958525693142, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Adaptive Particle-Swarm Differential Evolution with Dynamic Velocity Scaling and Chaotic Perturbation for Enhanced Convergence and Robust Search.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 40\n        inertia_weight_initial = 0.9\n        cognitive_component = 1.7\n        social_component = 1.7\n        differential_weight_initial = 0.9\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = inertia_weight_initial - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            velocities *= np.tanh(1.0 - eval_count / self.budget)  # dynamic velocity scaling\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight_initial * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(eval_count * np.pi / self.budget)  # chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07098 with standard deviation 0.00670.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.06174518629869441, 0.07744889736442218, 0.07374467210767766]}}
{"id": "ad18c560-029c-42c5-a7a9-e9965d37436b", "fitness": 0.07591953653839878, "name": "EnhancedDynamicParticleSwarmOptimizer", "description": "Enhanced Dynamic Particle Swarm Optimization with Adaptive Learning Rate and Stochastic Perturbation for Improved Global Search Efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicParticleSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Adaptive inertia weight\n            inertia_weight = max(0.4, 0.9 - (0.5 * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Stochastic perturbation to escape local optima\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                perturbation = np.random.normal(0, 0.1, self.dim) * (np.random.rand(self.dim) < 0.2)\n                mutant_vector += perturbation\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedDynamicParticleSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07592 with standard deviation 0.00141.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.07789435857181348, 0.0746953944734342, 0.07516885656994865]}}
{"id": "5488f781-c7a9-49e0-ae6e-db631b6530ad", "fitness": 0.0768693069647, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Particle Swarm Optimizer with Adaptive Velocity Control and Lvy Flight Mutation for improved exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Adaptive inertia weight\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = (personal_best_positions[a] +\n                                 adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c]))\n                \n                # Lvy flight mutation\n                beta = 1.5\n                sigma_u = ((np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)) /\n                           (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n                u = np.random.randn(self.dim) * sigma_u\n                v = np.random.randn(self.dim)\n                step = u / abs(v)**(1 / beta)\n                mutant_vector += step\n\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07687 with standard deviation 0.00149.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.07666409693797915, 0.07878389902965766, 0.07515992492646317]}}
{"id": "0fcae1b2-4023-4249-a491-fac2855715ee", "fitness": 0.0795088514210428, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved exploration via adaptive immune-inspired mutation and velocity scaling for enhanced convergence in high-dimensional space.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                immune_mutation_factor = np.random.rand(self.dim)  # immune-inspired mutation\n                mutant_vector += immune_mutation_factor * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07951 with standard deviation 0.00466.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.08520310351988447, 0.0795355565508361, 0.07378789419240783]}}
{"id": "5f2ed072-80d3-434e-bb93-3551baa2c9e7", "fitness": 0.07976624765180433, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Exploration-Exploitation Optimizer with improved chaotic mutation and adaptive inertia weight for better convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.7 * eval_count / self.budget)  # Modified adaptive inertia weight\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(eval_count * np.pi / self.budget)  # Improved chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07977 with standard deviation 0.00397.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.0834234877023452, 0.08161970747091873, 0.07425554778214905]}}
{"id": "bc2f75ba-0c89-4a63-8239-60c30b7b7fb9", "fitness": 0.06979294473256976, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic Optimizer with improved dynamic inertia and adaptive mutation for superior convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight_initial = 0.9\n        inertia_weight_final = 0.4\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight_min = 0.5\n        differential_weight_max = 0.9\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = inertia_weight_initial - ((inertia_weight_initial - inertia_weight_final) * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight_min + (differential_weight_max - differential_weight_min) * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(eval_count * np.pi / self.budget)  # chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06979 with standard deviation 0.00917.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.08032299523761877, 0.07107935431349943, 0.05797648464659111]}}
{"id": "3e822722-f23b-4e93-abac-44625bf95563", "fitness": 0.07821929669990464, "name": "QuantumInspiredMultiStrategyOptimizer", "description": "Dynamic Quantum-Inspired Multi-Strategy Optimizer integrating quantum superposition principles with adaptive mutation and crossover for superior balance between exploration and exploitation.", "code": "import numpy as np\n\nclass QuantumInspiredMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                quantum_factor = 0.1 * np.sin(eval_count * np.pi / self.budget)  # quantum-inspired factor\n                mutant_vector += quantum_factor * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 26, "feedback": "The algorithm QuantumInspiredMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07822 with standard deviation 0.00263.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.08186859970205718, 0.07579375670846578, 0.07699553368919099]}}
{"id": "f81de1c7-d61f-4a75-b270-7ea675d481a9", "fitness": 0.07766648851840156, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Enhanced Adaptive Hybrid Optimizer with dynamic swarm intelligence, chaotic local mutation, and adaptive inertia for superior convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(num_particles, float('inf'))\n\n        global_best_position = None\n        global_best_score = float('inf')\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                \n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(eval_count * np.pi / self.budget) * (np.random.rand(self.dim) - 0.5)\n                mutant_vector += chaos\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07767 with standard deviation 0.00290.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.08159589006074408, 0.07468069750227291, 0.07672287799218769]}}
{"id": "52dce5cc-4721-4fdd-8769-a3ca85b3f885", "fitness": 0.08078036142141165, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Global Search with Dynamic Inertia Reduction and Adaptive Mutation Strategy.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)  # Modified chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))  # Adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08078 with standard deviation 0.00794.", "error": "", "parent_ids": ["bce7aec2-51c9-4728-95a4-d7b2a6277bd7"], "operator": null, "metadata": {"aucs": [0.08681548420477025, 0.08596583423991899, 0.0695597658195457]}}
{"id": "3825301a-130d-4c5f-b9ac-dc57e283ea3c", "fitness": 0.0796333347064671, "name": "EnhancedAdaptiveMetaheuristicOptimizer", "description": "Adaptive Exploratory and Exploitative Search with Chaotic Dynamics and Progressive Parameter Tuning.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Dynamic inertia weight reduction with chaotic influence\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget) + 0.1 * np.sin(2 * np.pi * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = 0.5 * np.sin(np.pi * eval_count / self.budget)  # Modified chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))  # Adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07963 with standard deviation 0.00677.", "error": "", "parent_ids": ["52dce5cc-4721-4fdd-8769-a3ca85b3f885"], "operator": null, "metadata": {"aucs": [0.083127002470896, 0.0856030047554216, 0.07016999689308367]}}
{"id": "583deb44-023a-4329-b4b8-d51786f50c75", "fitness": 0.07943343654613184, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Particle Swarm Optimization with Adaptive Differential Evolution and Chaos-Driven Perturbations for Robust Search Capability.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)  # Modified chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count * np.pi / self.budget))  # Adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07943 with standard deviation 0.00929.", "error": "", "parent_ids": ["52dce5cc-4721-4fdd-8769-a3ca85b3f885"], "operator": null, "metadata": {"aucs": [0.08453952396755149, 0.08736803063068366, 0.0663927550401604]}}
{"id": "b9cd13fb-236b-4015-862e-fbd6f83158a8", "fitness": 0.07759360878517578, "name": "OptimizedHybridMetaheuristic", "description": "Optimized Hybrid Metaheuristic with Nonlinear Inertia Reduction, Adaptive Mutation, and Dynamic Swarm Intelligence.", "code": "import numpy as np\n\nclass OptimizedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Nonlinear inertia reduction\n            inertia_weight = 0.9 - (0.5 * (eval_count / self.budget)**2)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(np.pi * eval_count / self.budget)  # Enhanced chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * np.sin(np.pi * eval_count / (2 * self.budget)))  # Dynamic crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 31, "feedback": "The algorithm OptimizedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07759 with standard deviation 0.00525.", "error": "", "parent_ids": ["52dce5cc-4721-4fdd-8769-a3ca85b3f885"], "operator": null, "metadata": {"aucs": [0.08392959856587956, 0.07777362611484706, 0.0710776016748007]}}
{"id": "e2ee9bcc-6951-4384-b6e2-7581052779a9", "fitness": 0.08078945934133626, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved Adaptive Strategy with Dynamic Velocity Damping and Enhanced Mutation Scaling.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)  # Modified chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8  # Enhanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))  # Adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08079 with standard deviation 0.00796.", "error": "", "parent_ids": ["52dce5cc-4721-4fdd-8769-a3ca85b3f885"], "operator": null, "metadata": {"aucs": [0.08681518133410882, 0.08600702416908756, 0.06954617252081241]}}
{"id": "27d5e1f9-ae3c-4a38-9b5e-66c880d11eb2", "fitness": 0.07835285573168649, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Adaptive Velocity Control with Chaotic Perturbation and Dynamic Crossover to Improve Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)  # More dynamic inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          1.4 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.7 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.8 * eval_count * np.pi / self.budget)  # Modified chaotic sequence for better perturbation\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.9  # Enhanced mutation scaling\n                adaptive_crossover_rate = crossover_rate + 0.15 * np.cos(eval_count / self.budget)  # Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < adaptive_crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07835 with standard deviation 0.00196.", "error": "", "parent_ids": ["e2ee9bcc-6951-4384-b6e2-7581052779a9"], "operator": null, "metadata": {"aucs": [0.07783049949549303, 0.08096677398148211, 0.07626129371808432]}}
{"id": "5a58eb9d-eb6c-4bcb-afa8-69a7b3fe723a", "fitness": 0.07997783576645741, "name": "AdvancedAdaptiveOptimizer", "description": "Advanced Adaptive Strategy with Dynamic Inertia Reduction and Chaotic Mutation for Enhanced Global Convergence.", "code": "import numpy as np\n\nclass AdvancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight_max = 0.9\n        inertia_weight_min = 0.4\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Dynamic inertia weight reduction\n            inertia_weight = inertia_weight_max - ((inertia_weight_max - inertia_weight_min) * (eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.6 * eval_count * np.pi / self.budget)  # Enhanced chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8  # Enhanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.15 * chaos)  # Adaptive crossover rate with chaos influence\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 34, "feedback": "The algorithm AdvancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07998 with standard deviation 0.00478.", "error": "", "parent_ids": ["e2ee9bcc-6951-4384-b6e2-7581052779a9"], "operator": null, "metadata": {"aucs": [0.08580848649091954, 0.08002789144844447, 0.07409712936000823]}}
{"id": "fac5d4e4-dca0-47a4-aec1-883dcc1b5230", "fitness": 0.07171850473544295, "name": "QuantumEnhancedOptimizer", "description": "Adaptive Mechanism Enhanced with Quantum-inspired Rotation and Oscillating Crossover Adjustment for Swift Convergence.", "code": "import numpy as np\n\nclass QuantumEnhancedOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.4 + 0.5 * np.cos(np.pi * eval_count / self.budget)  # Oscillating inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                quantum_rotation = np.random.rand(self.dim) * np.sin(2 * np.pi * np.random.rand(self.dim))\n                mutant_vector += quantum_rotation  # Quantum-inspired mutation\n                cross_points = np.random.rand(self.dim) < (crossover_rate * np.cos(2 * np.pi * eval_count / self.budget))  # Oscillating crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 35, "feedback": "The algorithm QuantumEnhancedOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07172 with standard deviation 0.00433.", "error": "", "parent_ids": ["e2ee9bcc-6951-4384-b6e2-7581052779a9"], "operator": null, "metadata": {"aucs": [0.07730355068248584, 0.07109018922029064, 0.06676177430355235]}}
{"id": "1c4d4809-5f5f-40c4-8ad8-713958cb1467", "fitness": 0.0778728089776511, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer with Time-Varying Parameters and Noise-Resilient Stochastic Perturbations for Enhanced Convergence.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        initial_inertia_weight = 0.9\n        final_inertia_weight = 0.4\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Time-varying inertia weight\n            inertia_weight = initial_inertia_weight - (initial_inertia_weight - final_inertia_weight) * (eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 - eval_count / self.budget) ** 2\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                perturbation = 0.01 * np.random.normal(size=self.dim)  # Noise-resilient stochastic perturbation\n                mutant_vector += perturbation\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 36, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07787 with standard deviation 0.00489.", "error": "", "parent_ids": ["e2ee9bcc-6951-4384-b6e2-7581052779a9"], "operator": null, "metadata": {"aucs": [0.08365807975403072, 0.07825977579831012, 0.07170057138061248]}}
{"id": "24452cca-e09c-4332-802b-b3c72c3ffa44", "fitness": 0.06287484729651693, "name": "AdvancedCoevolutionaryMetaheuristicOptimizer", "description": "Advanced Coevolutionary Metaheuristic with Adaptive Chaos and Dynamic Learning, which optimizes parallel exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdvancedCoevolutionaryMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 2.0\n        social_component = 2.0\n        differential_weight = 0.9\n        crossover_rate = 0.7\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            dynamic_inertia = 0.5 + (0.5 * np.cos(np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (dynamic_inertia * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                coadaptive_diff_weight = differential_weight * (0.5 + 0.5 * np.sin(eval_count * np.pi / self.budget))\n                mutant_vector = personal_best_positions[a] + coadaptive_diff_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos_factor = np.cos(0.3 * eval_count * np.pi / self.budget)  # Improved chaotic sequence\n                mutant_vector += chaos_factor * (np.random.rand(self.dim) - 0.5) * 0.9  # Advanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.15 * np.cos(eval_count / self.budget))  # Dynamic crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 37, "feedback": "The algorithm AdvancedCoevolutionaryMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06287 with standard deviation 0.00564.", "error": "", "parent_ids": ["e2ee9bcc-6951-4384-b6e2-7581052779a9"], "operator": null, "metadata": {"aucs": [0.057641896293571526, 0.0706973418271607, 0.06028530376881858]}}
{"id": "507855bd-da9f-44ee-8aae-d95d39e4a2c7", "fitness": 0.08733876842312023, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy with chaos-based velocity and adaptive mutation scaling to improve convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +  # Changed scaling factor for inertia weight\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)  # Altered adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)  # Modified chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8  # Enhanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))  # Adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08734 with standard deviation 0.00243.", "error": "", "parent_ids": ["e2ee9bcc-6951-4384-b6e2-7581052779a9"], "operator": null, "metadata": {"aucs": [0.09066303427313638, 0.08490442943786392, 0.08644884155836041]}}
{"id": "4f9437d2-7fac-481f-a416-2020035fe99a", "fitness": 0.08428084448621147, "name": "EnhancedHybridMetaheuristicOptimizerV2", "description": "Dynamic adaptive mutation and crossover with chaotic exploration to enhance convergence efficiency and diversity in the search process.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (0.5 + 0.5 * np.sin(np.pi * eval_count / self.budget))  # Enhanced adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = 0.5 * np.sin(2 * np.pi * eval_count / self.budget)  # Refined chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8  # Enhanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * np.sin(2 * np.pi * eval_count / self.budget))  # Adaptive and dynamic crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08428 with standard deviation 0.00198.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08396080050061228, 0.08685049523540256, 0.08203123772261955]}}
{"id": "80317d6a-e572-48b1-9f0a-2513afebea0f", "fitness": 0.08250651600004644, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy incorporating Lvy flight for exploration and a dynamic trade-off between local and global search for improved convergence. ", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def levy_flight(self, L):\n        u = np.random.normal(0, 1, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / np.power(np.fabs(v), 1 / L)\n        return 0.01 * step\n\n    def __call__(self, func):\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        global_best_position = None\n        global_best_score = float('inf')\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                levy_part = self.levy_flight(1.5)  # Added Lvy flight component\n                trial_vector += levy_part\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08251 with standard deviation 0.00617.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08717142798787691, 0.08655889954410523, 0.07378922046815717]}}
{"id": "007c1181-3a3d-42ca-85d6-16ad5ad9e751", "fitness": 0.07826147121761858, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "A novel hybrid optimizer combining self-adaptive particle swarm optimization and dynamic differential evolution with chaotic mapping to enhance convergence and stability.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.5 + 0.4 * np.cos(eval_count * np.pi / (2 * self.budget))  # Cosine-based inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.abs(np.cos(eval_count * np.pi / self.budget))\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.tanh(0.9 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.6\n                cross_points = np.random.rand(self.dim) < (crossover_rate * (0.5 + 0.5 * np.sin(eval_count * np.pi / self.budget)))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 41, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07826 with standard deviation 0.00721.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08598254176525866, 0.08016375146472365, 0.06863812042287343]}}
{"id": "552f312d-6f50-4228-944c-a2e652c71190", "fitness": 0.07363592853465868, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined hybrid metaheuristic with enhanced inertia adaptation, chaos-based mutation, and dynamic population size to boost exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 50  # Increased particle count for better diversity\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.5 + 0.4 * (1 - eval_count / self.budget)  # Enhanced inertia adaptation\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.log1p(eval_count / self.budget)  # Enhanced adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(0.5 * eval_count * np.pi / self.budget)  # Enhanced chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07364 with standard deviation 0.00344.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.07850711030049462, 0.07124078048073756, 0.07115989482274387]}}
{"id": "5504b5ee-a2e3-49c1-87be-0550ae3fc24f", "fitness": 0.0815017138842948, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced hybrid optimizer integrating adaptive chaos-inducing factors and dynamic learning rates to enhance convergence and performance consistency in diverse optimization landscapes.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight_initial = 0.9\n        inertia_weight_final = 0.4\n        cognitive_component = 1.4\n        social_component = 1.4\n        differential_weight = 0.7\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(num_particles, float('inf'))\n\n        # Global best initialization\n        global_best_position = np.copy(particles[0])\n        global_best_score = float('inf')\n\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n                \n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = np.copy(particles[i])\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = np.copy(particles[i])\n\n                if eval_count >= self.budget:\n                    break\n\n            # Adaptive inertia weight based on the progress\n            inertia_weight = (inertia_weight_initial - inertia_weight_final) * (1 - eval_count / self.budget) + inertia_weight_final\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            \n            # Update velocities and positions\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = np.delete(np.arange(num_particles), i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                # Adaptive differential weight and chaos theory\n                adaptive_differential_weight = differential_weight * (0.5 + 0.5 * np.sin(np.pi * eval_count / self.budget))\n                chaos_factor = 0.8 * np.sin(0.6 * np.pi * eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c]) + chaos_factor * (np.random.rand(self.dim) - 0.5)\n                \n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.cos(eval_count * np.pi / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n                \n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = np.copy(trial_vector)\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = np.copy(trial_vector)\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 43, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08150 with standard deviation 0.00188.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.0832700120757659, 0.08233946271338943, 0.07889566686372906]}}
{"id": "f5303fe2-f8e6-43e3-af70-ba6b065e2671", "fitness": 0.08600512717955551, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined enhanced hybrid strategy by introducing adaptive learning coefficients and a sigmoid-based mutation factor for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_component = 1.2 + 0.3 * (1 - eval_count/self.budget)  # Adapted cognitive component\n            social_component = 1.2 + 0.3 * (eval_count/self.budget)  # Adapted social component\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                sigmoid_factor = 1 / (1 + np.exp(-5 * (eval_count/self.budget - 0.5)))  # Introduced sigmoid factor\n                mutant_vector += sigmoid_factor * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08601 with standard deviation 0.00307.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08991848053576534, 0.0824216964577531, 0.0856752045451481]}}
{"id": "422f7257-10fe-4b1e-a1d0-aa185312fde6", "fitness": 0.08523200818396692, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved hybrid strategy with dynamic neighborhood-based interactions and adaptive inertia reduction for enhanced convergence and exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.5 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +  # Reduced inertia scaling factor\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.9 * social_component * r2 * (global_best_position - particles))  # Enhanced social component scaling\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                neighbors = np.random.choice(indices, 3, replace=False)  # Neighborhood-based selection\n                a, b, c = neighbors\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.cos(eval_count / self.budget))  # Modified adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08523 with standard deviation 0.00492.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.09150362746424057, 0.08470465226372115, 0.07948774482393905]}}
{"id": "e0e9a176-61a6-4681-b6d3-526b226525e1", "fitness": 0.08279905080300927, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic with enhanced adaptive mutation and velocity scaling for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.6 - (0.3 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.9 * inertia_weight * velocities +  # Changed scaling factor for inertia weight\n                          1.3 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.9 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (0.5 + 0.5 * np.sin(eval_count / self.budget))  # Altered adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.7 * eval_count * np.pi / self.budget)  # Modified chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.7  # Enhanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.cos(eval_count / self.budget))  # Adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08280 with standard deviation 0.00242.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08520541622522293, 0.08369990861072629, 0.0794918275730786]}}
{"id": "5ced0be6-a7e8-4409-833c-4998ffeca4a2", "fitness": 0.06953335331388044, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined Enhanced Hybrid Strategy with adaptive learning rate and improved chaotic sequences for faster convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 2.0  # (1) Adjusted the social component for better exploration\n        differential_weight = 0.85  # (2) Slightly increased differential weight\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.3 * eval_count / self.budget)  # (3) Fine-tuned inertia adjustment\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities + \n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.cos(eval_count / self.budget)  # (4) Improved adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(0.5 * eval_count * np.pi / self.budget)  # (5) Modified chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.9  # (6) Adjusted mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.15 * np.cos(eval_count / self.budget))  # (7) Enhanced adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06953 with standard deviation 0.00641.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.06960625619259209, 0.07734921490648528, 0.06164458884256396]}}
{"id": "6bc935b0-f056-48b9-bcac-b05b73165323", "fitness": 0.08359393191563307, "name": "InnovativeSwarmDifferentialOptimizer", "description": "Innovative Swarm-Differential Strategy employing time-adaptive chaos-driven parameters and randomized mutation for enhanced convergence.", "code": "import numpy as np\n\nclass InnovativeSwarmDifferentialOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.9 * inertia_weight * velocities +\n                          1.3 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.7 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (0.5 + 0.5 * np.sin(eval_count * np.pi / self.budget))\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.6 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5)\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 48, "feedback": "The algorithm InnovativeSwarmDifferentialOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08359 with standard deviation 0.00273.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08058552121059193, 0.08720324080104946, 0.08299303373525779]}}
{"id": "11bebbfa-67b6-4f65-9e1b-c3e2732d0c71", "fitness": 0.08317973718316189, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced evolutionary algorithm integrating dynamic neighborhood topology and refined selection pressure for better convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.7  # Increased cognitive component\n        social_component = 1.3  # Decreased social component\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                neighbors = np.random.choice(indices, 5, replace=False)  # Dynamic neighborhood selection\n                a, b, c = np.random.choice(neighbors, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08318 with standard deviation 0.00532.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08752892185806871, 0.0863201851617198, 0.07569010452969716]}}
{"id": "1763ea76-b13e-49de-824d-859fd93fa6ec", "fitness": -Infinity, "name": "DynamicLevyHybridOptimizer", "description": "Integrates dynamic parameter control and Lvy-flight-based exploration for enhanced convergence and diverse solution space traversal.", "code": "import numpy as np\n\nclass DynamicLevyHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        def levy_flight(Lambda):\n            sigma = (np.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2) / \n                     (np.gamma((1 + Lambda) / 2) * Lambda * 2 ** ((Lambda - 1) / 2))) ** (1 / Lambda)\n            u = np.random.randn() * sigma\n            v = np.random.randn()\n            step = u / abs(v) ** (1 / Lambda)\n            return step\n        \n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.5 + 0.4 * np.cos(np.pi * eval_count / self.budget)  # Dynamic inertia weight\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities + \n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.abs(np.cos(eval_count / self.budget * np.pi))\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                \n                # Apply Lvy flight for enhanced exploration\n                if np.random.rand() < 0.3:\n                    mutant_vector += levy_flight(1.5) * (particles[i] - global_best_position)\n\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 50, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {}}
{"id": "7bf299ea-e855-48fb-9d6b-b6b38a1f8835", "fitness": 0.0831072023222107, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Dynamic adaptive strategy with multi-phase velocity control and enhanced mutation, improving exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.8 * inertia_weight * velocities +\n                          1.1 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.7 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.cos(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.9\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * np.cos(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08311 with standard deviation 0.00163.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08369924732972345, 0.084744381382027, 0.08087797825488163]}}
{"id": "4c51c674-db96-42fe-9f77-e73af073f145", "fitness": 0.0844017952563284, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined hybrid strategy with stochastic acceleration and enhanced adaptive mutation to boost exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 2.0  # Increased cognitive component\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.75 * inertia_weight * velocities +  # Changed scaling factor for inertia weight\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          1.0 * social_component * r2 * (global_best_position - particles))  # Adjusted social component factor\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(2 * eval_count / self.budget)  # Enhanced adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(1.0 * eval_count * np.pi / self.budget)  # Refined chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.7  # Modified mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.cos(eval_count / self.budget))  # Adaptive crossover rate with cosine\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08440 with standard deviation 0.00320.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08887964618264288, 0.08269699875231118, 0.08162874083403115]}}
{"id": "657e6a23-248f-4972-83a0-594f6972c50a", "fitness": 0.07828133228257095, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved adaptive parameters and refined chaotic sequence for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.8  # Modified\n        social_component = 1.8  # Modified\n        differential_weight = 0.85  # Modified\n        crossover_rate = 0.95  # Modified\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(np.pi * eval_count / self.budget)  # Modified\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(0.7 * eval_count * np.pi / self.budget)  # Modified\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07828 with standard deviation 0.00689.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08769969728526161, 0.07572150164485647, 0.07142279791759476]}}
{"id": "cdba2ff5-61cc-4550-92e9-b52c5ea3bd2c", "fitness": 0.08453855120153715, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved hybrid strategy with enhanced chaotic maps and adaptive learning factors to boost exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.3 * cognitive_component * r1 * (personal_best_positions - particles) +  # Updated factor\n                          0.9 * social_component * r2 * (global_best_position - particles))  # Updated factor\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = 0.6 * np.sin(0.5 * eval_count * np.pi / self.budget) + 0.4 * np.cos(0.5 * eval_count * np.pi / self.budget)  # Enhanced chaotic map\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08454 with standard deviation 0.00462.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.09091107880669391, 0.08010150130703986, 0.0826030734908777]}}
{"id": "b3f9960b-78a5-4b2d-84ad-3f2b8f781d41", "fitness": 0.07672641596412888, "name": "AdvancedHybridOptimizer", "description": "Advanced hybrid metaheuristic optimizer that integrates chaotic inertia reduction, adaptive cognitive and social components, and a novel randomness-influenced differential mutation strategy for accelerated convergence.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Dynamic inertia and component adjustments\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            cog_factor = cognitive_component * (1 + 0.2 * np.sin(eval_count / self.budget * np.pi))\n            soc_factor = social_component * (1 + 0.2 * np.cos(eval_count / self.budget * np.pi))\n            \n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities + \n                          cog_factor * r1 * (personal_best_positions - particles) +\n                          soc_factor * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget * np.pi) \n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * np.random.uniform(-0.5, 0.5, self.dim)\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget * np.pi))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 55, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07673 with standard deviation 0.00395.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08042452200737893, 0.07849740881622325, 0.07125731706878446]}}
{"id": "9928d52d-8f8c-429a-a3d1-fc62ebc62465", "fitness": 0.06806501138418664, "name": "ImprovedEnhancedHybridMetaheuristicOptimizer", "description": "Improved Enhanced Hybrid Metaheuristic Optimizer with dynamic learning rates and synergistic diversification to accelerate convergence and exploit search space efficiently.", "code": "import numpy as np\n\nclass ImprovedEnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Dynamic inertia reduction and learning rates\n            inertia_weight = 0.9 - (0.6 * eval_count / self.budget)\n            cognitive_component = 1.5 + 0.5 * np.sin(eval_count / self.budget * np.pi)\n            social_component = 1.5 + 0.5 * np.cos(eval_count / self.budget * np.pi)\n\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                # Enhanced adaptive differential weight\n                adaptive_differential_weight = differential_weight * (1 + np.cos(eval_count / self.budget * np.pi / 2))\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n\n                # Synergistic chaos mutation\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.5\n\n                # Adaptive crossover rate adjustment\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 56, "feedback": "The algorithm ImprovedEnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06807 with standard deviation 0.00232.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.07059215737869606, 0.06861204503768648, 0.06499083173617737]}}
{"id": "7a151bfa-8b45-4048-8a6c-2ca82c06f863", "fitness": 0.07367083338909308, "name": "AdvancedHybridOptimizer", "description": "Advanced hybrid optimizer integrating dynamic inertia adjustment and stochastic perturbation to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.4 + 0.5 * (1 - eval_count / self.budget)  # Dynamic inertia adjustment\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.cos(eval_count / self.budget)  # Enhanced adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.random.normal(0, 0.1, self.dim)  # Stochastic perturbation for diversity\n                mutant_vector += chaos\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 57, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07367 with standard deviation 0.00397.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.0792734554603306, 0.07108743242032778, 0.07065161228662087]}}
{"id": "a5c9bfe4-1955-481c-9acc-df58d2ae0466", "fitness": 0.07891906762592434, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined hybrid strategy with stochastic adaptive inertia and enhanced chaotic mutation for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget) + 0.1 * np.random.rand()  # Adjusted stochastic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)  # Maintaining existing chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 1.0  # Enhanced mutation scale\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07892 with standard deviation 0.00271.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08274630663721971, 0.07684574640878894, 0.07716514983176437]}}
{"id": "a34fefb4-287c-4847-9322-eb4c9bbf18bd", "fitness": 0.06732337563211206, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced hybrid optimizer utilizing adaptive chaotic exploration and dual mutation strategies for enhanced convergence accuracy and diversity.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 2.0\n        social_component = 2.0\n        differential_weight = 0.9\n        crossover_rate = 0.8\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.5 + 0.4 * np.cos(eval_count / self.budget * np.pi)  # Dynamic inertia adjustment\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                updated_differential_weight = differential_weight * (1 - eval_count / self.budget)  # Adaptively reduced differential weight\n                mutant_vector = personal_best_positions[a] + updated_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos_factor = np.sin(eval_count ** 1.01 * np.pi / self.budget)  # Refined chaotic sequence\n                dual_mutation = chaos_factor * np.random.rand(self.dim) * 1.2  # Amplified dual mutation\n                mutant_vector += dual_mutation\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 59, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06732 with standard deviation 0.01221.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.07587877247900188, 0.0760339822689361, 0.05005737214839823]}}
{"id": "939835de-ed27-4257-a63a-1ca65b0df727", "fitness": 0.07540189628088771, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Introduction of an adaptive particle diversity mechanism and nonlinear convergence acceleration to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(num_particles, float('inf'))\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Evaluate particle fitness and update personal and global bests\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Adaptive inertia weight with nonlinear convergence\n            inertia_weight = 0.4 + 0.5 * np.exp(-5 * (eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities + \n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Diversity measure and adaptive mutation scaling\n            particle_std = np.std(particles, axis=0)\n            diversity_factor = np.sum(particle_std) / num_particles\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (0.5 + 0.5 * np.sin(np.pi * eval_count / self.budget))\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * diversity_factor\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 60, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07540 with standard deviation 0.00878.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08592412417270501, 0.07584704122954034, 0.06443452344041778]}}
{"id": "c33a0cda-65a0-4a3d-add1-11916179370e", "fitness": 0.07304729683164124, "name": "AdvancedChaoticHybridOptimizer", "description": "An advanced chaotic hybrid optimization incorporating Levy flights and adaptive velocity control for superior convergence and precision.", "code": "import numpy as np\n\nclass AdvancedChaoticHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        def levy_flight(Lambda):\n            sigma1 = np.power((np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2)) / \n                              (np.math.gamma((1 + Lambda) / 2) * Lambda * np.power(2, ((Lambda - 1) / 2))), 1 / Lambda)\n            sigma2 = 1\n            u = np.random.normal(0, sigma1, self.dim)\n            v = np.random.normal(0, sigma2, self.dim)\n            step = u / np.power(np.abs(v), 1 / Lambda)\n            return step\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities + \n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.cos(eval_count / self.budget * np.pi)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * levy_flight(1.5)\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.cos(eval_count / self.budget * np.pi))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 61, "feedback": "The algorithm AdvancedChaoticHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07305 with standard deviation 0.00801.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.0782066983196078, 0.07919822529005272, 0.0617369668852632]}}
{"id": "e6e07eb5-5770-435f-987b-4377e06b87e7", "fitness": 0.07694576644634517, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy with adaptive diversification using Lvy flight and chaotic perturbation to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities + \n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                \n                levy_flight = np.random.normal(0, 1) * (np.random.rand(self.dim) ** -1.5)  # Added Lvy flight component\n                mutant_vector += levy_flight * (ub - lb) * chaos  # Combined with chaotic perturbation\n               \n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07695 with standard deviation 0.00611.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08148122891171206, 0.08104662224334602, 0.06830944818397744]}}
{"id": "6910d59f-31b1-4cfc-82a3-c0714b68b9dc", "fitness": 0.08283338708374129, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Adaptive swarm optimizer integrating random walk and entropy-based mutation for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                entropy_factor = 1 / (1 + np.exp(-0.1 * (eval_count - (self.budget / 2))))  # Entropy-based mutation\n                mutant_vector += entropy_factor * np.random.normal(0, 0.1, self.dim)  # Random walk integration\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08283 with standard deviation 0.00949.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08941484179589376, 0.0896738803611834, 0.06941143909414671]}}
{"id": "be837894-9a5b-4858-8825-1a386b7137d6", "fitness": 0.08028228387471932, "name": "RefinedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimizer with adaptive neighborhood exploration and dynamic strategy blending for improved convergence and diversity maintenance.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.9 * inertia_weight * velocities +\n                          1.3 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.9 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n                # Adaptive neighborhood exploration\n                neighbor_indices = np.random.choice(num_particles, 5, replace=False)\n                neighbor_positions = particles[neighbor_indices]\n                exploration_vector = np.mean(neighbor_positions, axis=0) + 0.1 * np.random.randn(self.dim)\n                exploration_vector = np.clip(exploration_vector, lb, ub)\n\n                exploration_score = func(exploration_vector)\n                eval_count += 1\n\n                if exploration_score < personal_best_scores[i]:\n                    personal_best_scores[i] = exploration_score\n                    personal_best_positions[i] = exploration_vector\n\n                if exploration_score < global_best_score:\n                    global_best_score = exploration_score\n                    global_best_position = exploration_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 64, "feedback": "The algorithm RefinedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08028 with standard deviation 0.00862.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.09208488187749808, 0.0770068804061601, 0.07175508934049979]}}
{"id": "c1b7ed7a-6445-4329-a601-227df25fcf14", "fitness": -Infinity, "name": "AdvancedAdaptiveMetaheuristicOptimizer", "description": "Advanced convergence optimization with dynamic adaptive chaos and multi-population strategy to enhance global search efficiency.", "code": "import numpy as np\n\nclass AdvancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        # Multi-population strategy\n        subpop_size = num_particles // 3\n        subpops = [particles[i:i+subpop_size] for i in range(0, num_particles, subpop_size)]\n        \n        while eval_count < self.budget:\n            for subpop in subpops:\n                for i in range(len(subpop)):\n                    score = func(subpop[i])\n                    eval_count += 1\n\n                    if score < personal_best_scores[i]:\n                        personal_best_scores[i] = score\n                        personal_best_positions[i] = subpop[i]\n\n                    if score < global_best_score:\n                        global_best_score = score\n                        global_best_position = subpop[i]\n\n                    if eval_count >= self.budget:\n                        break\n\n                inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                velocities = (0.85 * inertia_weight * velocities +\n                              1.2 * cognitive_component * r1 * (personal_best_positions - subpop) +\n                              0.8 * social_component * r2 * (global_best_position - subpop))\n                subpop += velocities\n                subpop = np.clip(subpop, lb, ub)\n\n                for i in range(len(subpop)):\n                    if eval_count >= self.budget:\n                        break\n                    indices = list(range(len(subpop)))\n                    indices.remove(i)\n                    a, b, c = np.random.choice(indices, 3, replace=False)\n                    adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                    mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                    chaos = np.tanh(0.5 * eval_count * np.pi / self.budget)  # Further modification to chaotic sequence\n                    mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                    cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                    trial_vector = np.where(cross_points, mutant_vector, subpop[i])\n                    trial_vector = np.clip(trial_vector, lb, ub)\n\n                    trial_score = func(trial_vector)\n                    eval_count += 1\n\n                    if trial_score < personal_best_scores[i]:\n                        personal_best_scores[i] = trial_score\n                        personal_best_positions[i] = trial_vector\n\n                    if trial_score < global_best_score:\n                        global_best_score = trial_score\n                        global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 65, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (30,10) (10,10) ').", "error": "ValueError('operands could not be broadcast together with shapes (30,10) (10,10) ')", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {}}
{"id": "39cb2066-4f23-48c1-85d7-66788ebd0e08", "fitness": 0.08362119326294815, "name": "RefinedHybridMetaheuristicOptimizer", "description": "Refined hybrid algorithm incorporating velocity perturbation and multi-swarm interaction for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Multi-swarm approach: Split particles into smaller groups\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Adjust inertia and introduce velocity perturbation\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            perturbation = np.random.uniform(-0.1, 0.1, (num_particles, self.dim))\n            velocities = (inertia_weight * velocities + \n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles) + perturbation)\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 66, "feedback": "The algorithm RefinedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08362 with standard deviation 0.00377.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08746894503443281, 0.08489918415736442, 0.07849545059704721]}}
{"id": "6b2d1731-faa4-4ebd-bc25-6da16b986242", "fitness": 0.08733739769227337, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy with adaptive chaotic sequence modification to improve convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +  # Changed scaling factor for inertia weight\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)  # Altered adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.45 * eval_count * np.pi / self.budget)  # Modified chaotic sequence with a slight change in frequency\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8  # Enhanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))  # Adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08734 with standard deviation 0.00243.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.0906577984530278, 0.08490722672179518, 0.08644716790199714]}}
{"id": "0055d479-afce-40f7-9e6e-9e55bf0707e0", "fitness": 0.08065417372014327, "name": "QuantumAdaptiveMetaheuristicOptimizer", "description": "Integrating adaptive learning with quantum-inspired search to enhance exploration and exploitation balance in optimization.", "code": "import numpy as np\n\nclass QuantumAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia weight\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.9 * inertia_weight * velocities +\n                          1.3 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.9 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget) \n                quantum_perturbation = 0.1 * np.random.randn(self.dim)  # Quantum-inspired perturbation\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c]) + quantum_perturbation\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)  \n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8  \n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))  \n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 68, "feedback": "The algorithm QuantumAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08065 with standard deviation 0.00667.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08550057134815814, 0.08523811288350669, 0.071223836928765]}}
{"id": "accbc2e3-fe86-4c42-8440-e94801bab7f7", "fitness": 0.08504005523735259, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy with chaos-based velocity and adaptive mutation scaling, refined with dynamic social component adjustment for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)  # Adjusted inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            social_component = 1.5 + 0.5 * np.cos(eval_count / self.budget * np.pi)  # Dynamic social component adjustment\n            velocities = (0.85 * inertia_weight * velocities +  # Changed scaling factor for inertia weight\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)  # Altered adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)  # Modified chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8  # Enhanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))  # Adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08504 with standard deviation 0.00323.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08943026462075732, 0.0839165349249067, 0.08177336616639375]}}
{"id": "2e0c18a1-3bba-4e0b-b925-ee068a113a31", "fitness": 0.08108156492116698, "name": "RefinedHybridMetaheuristicOptimizer", "description": "Integrates a chaotic Levy flight mechanism and dynamic parameter adjustment to refine exploration and exploitation phases for enhanced convergence in a diverse range of optimization tasks.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def levy_flight(self, L):\n        # Levy flight mechanism\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=L.shape)\n        v = np.random.normal(0, 1, size=L.shape)\n        step = u / np.abs(v)**(1/beta)\n        return step\n\n    def __call__(self, func):\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                levy_steps = self.levy_flight(mutant_vector) * chaos\n                mutant_vector += levy_steps\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 70, "feedback": "The algorithm RefinedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08108 with standard deviation 0.00774.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08795594103328719, 0.08502048477005986, 0.07026826896015392]}}
{"id": "6f7155c2-272f-4612-90bb-a03a214e18f9", "fitness": 0.06675586729439158, "name": "ImprovedAdaptiveSwarmOptimizer", "description": "Improved adaptive particle swarm optimization with dynamic chaos control and adaptive parameter tuning for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass ImprovedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 2.0\n        social_component = 2.0\n        differential_weight = 0.9\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n            \n            inertia_weight = 0.9 - (0.5 * eval_count / self.budget)  # More dynamic inertia reduction\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (0.5 + 0.5 * np.sin(eval_count * np.pi / self.budget))\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.7 * eval_count * np.pi / self.budget)  # Adjusted chaotic sequence for better exploration\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.7\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * np.sin(eval_count * 2 * np.pi / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 71, "feedback": "The algorithm ImprovedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06676 with standard deviation 0.00266.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.0638027358661245, 0.07024786527522553, 0.06621700074182468]}}
{"id": "35812263-9d34-426d-a6e8-7053f8237821", "fitness": 0.08162314968920277, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy with chaotic inertia modulation and dynamic cooperative foraging to enhance global exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Dynamic chaotic inertia weight modulation\n            chaos_component = 0.5 * (np.sin(0.5 * np.pi * eval_count / self.budget) + 1)\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget) * chaos_component\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 + chaos_component) * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                dynamic_chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += dynamic_chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08162 with standard deviation 0.00313.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08604648666951653, 0.07945274994708984, 0.07937021245100195]}}
{"id": "cae08381-90f6-45da-a7bc-013dcfb5b716", "fitness": 0.08558890260122125, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined hybrid metaheuristic using a dynamic chaotic sequence and enhanced velocity adjustment for improved global exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.3 * cognitive_component * r1 * (personal_best_positions - particles) +  # Changed cognitive factor\n                          0.75 * social_component * r2 * (global_best_position - particles))  # Changed social factor\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.6 * eval_count * np.pi / self.budget)  # Adjusted chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.9  # Slightly adjusted mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.15 * np.sin(eval_count / self.budget))  # Adjusted crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08559 with standard deviation 0.00305.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08988474998936657, 0.08304771968838254, 0.08383423812591462]}}
{"id": "4ba173b7-8f5e-461d-a581-411cb2d405d7", "fitness": 0.08683383779921099, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy with adaptive chaotic sequences and refined particle interactions for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget) \n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.6 * eval_count * np.pi / self.budget)  # Modified chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.7  # Enhanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.15 * np.sin(eval_count / self.budget))  # Refined crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08683 with standard deviation 0.00284.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.09074780116369074, 0.08409468749340787, 0.08565902474053433]}}
{"id": "04082798-e719-492a-a736-064e3bc66b22", "fitness": 0.08263164268824386, "name": "EnhancedMultiPhaseMetaheuristicOptimizer", "description": "Incorporate a multi-phase dynamic adaptation strategy to balance exploration and exploitation, enhancing convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedMultiPhaseMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.4\n        social_component = 1.4\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Update phase according to the budget usage\n            phase = (eval_count / self.budget)\n            if phase < 0.3:\n                # Exploration Phase\n                inertia_weight = 0.9\n                cognitive_component = 1.5\n                social_component = 1.0\n            elif phase < 0.6:\n                # Transition Phase\n                inertia_weight = 0.7\n                cognitive_component = 1.5\n                social_component = 1.5\n            else:\n                # Exploitation Phase\n                inertia_weight = 0.5\n                cognitive_component = 1.0\n                social_component = 1.8\n\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.85 * inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n                \n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedMultiPhaseMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08263 with standard deviation 0.00200.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08223096225848425, 0.08040627400361344, 0.08525769180263387]}}
{"id": "52043ddb-cd26-4756-88f5-ebc67255fe96", "fitness": 0.08587134314582612, "name": "RefinedHybridMetaheuristicOptimizer", "description": "Hybrid optimizer with fluctuating chaos and diversity reinforcement to enhance exploration and convergence balance.", "code": "import numpy as np\n\nclass RefinedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        diversity_rate = 0.1\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Evaluate particle positions\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Update velocities and positions with fluctuating chaos\n            inertia_weight = 0.7 - (0.4 * eval_count / self.budget)\n            chaos_factor = np.cos(0.5 * eval_count * np.pi / self.budget)  # Fluctuating chaotic sequence\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          chaos_factor * cognitive_component * r1 * (personal_best_positions - particles) +\n                          chaos_factor * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            # Apply differential evolution with diversity reinforcement\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = (personal_best_positions[a] +\n                                 adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c]))\n                mutant_vector += diversity_rate * (np.random.rand(self.dim) - 0.5)  # Diversity reinforcement\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * np.sin(eval_count / self.budget))\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 76, "feedback": "The algorithm RefinedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08587 with standard deviation 0.00294.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.08993437323360742, 0.08306555423373796, 0.08461410197013297]}}
{"id": "cd2cf190-a070-46fa-9b0d-5076c284f5bb", "fitness": 0.08065666161392748, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved hybrid optimizer with self-adaptive inertia and dynamic particle count adjustment for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = 0.7 - (0.3 * (eval_count / self.budget)**2)  # Self-adaptive inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (0.9 * inertia_weight * velocities +  # Adjusted scaling factor\n                          1.1 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.9 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.cos(eval_count / self.budget)  # Altered adaptive differential weight\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(0.5 * eval_count * np.pi / self.budget)  # Using cosine for chaotic sequence\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.7  # Enhanced mutation scaling\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * np.cos(eval_count / self.budget))  # Adaptive crossover rate\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08066 with standard deviation 0.00148.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.0825637232227977, 0.08044402505033033, 0.07896223656865442]}}
{"id": "7fd4dc32-7728-417b-9f0f-a6a7d91ce858", "fitness": 0.08827674046457883, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy with adaptive chaotic inertia, improved crossover dynamics, and smarter mutation for better convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))  # Adaptive chaotic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)  # Improved crossover dynamics\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08828 with standard deviation 0.00190.", "error": "", "parent_ids": ["507855bd-da9f-44ee-8aae-d95d39e4a2c7"], "operator": null, "metadata": {"aucs": [0.09092683358206577, 0.08660175138687687, 0.08730163642479383]}}
{"id": "5f0fbe1c-c800-48d6-aebc-d4ccc74cdc99", "fitness": 0.0822122942724847, "name": "AdvancedHybridOptimizer", "description": "Advanced hybrid optimizer integrating Levy flights for exploration, adaptive chaotic inertia, and random dynamic crowding for improved convergence and diversity.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        levy_factor = 1.5\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        def levy_flight(Lambda):\n            sigma1 = np.power((np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2)) /\n                              (np.math.gamma((1 + Lambda) / 2) * Lambda * np.power(2, (Lambda - 1) / 2)), 1 / Lambda)\n            u = np.random.normal(0, sigma1, size=self.dim)\n            v = np.random.normal(0, 1, size=self.dim)\n            step = u / np.power(np.abs(v), 1 / Lambda)\n            return 0.01 * step\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8 + levy_flight(levy_factor)\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 79, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08221 with standard deviation 0.01022.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.08817035169716447, 0.0906389364527882, 0.06782759466750143]}}
{"id": "bb35a1e1-87db-4759-992b-57690d3fe101", "fitness": 0.08582863638242084, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined hybrid approach with chaotic inertia, improved crossover dynamics, and enhanced mutation for superior exploration.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.cos(2 * np.pi * eval_count / self.budget)  # Adaptive chaotic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.3 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)  # Improved crossover dynamics\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08583 with standard deviation 0.00201.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.08776582586174042, 0.08306101596911453, 0.0866590673164076]}}
{"id": "f68bd2fc-8f2e-41c7-98d3-53da3fed2c55", "fitness": 0.08409799411665908, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy with chaotic perturbations, adaptive exploration, and selective mutation for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            velocities += 0.01 * np.random.normal(size=velocities.shape)  # Chaotic perturbation\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.cos(eval_count / self.budget)  # Adaptive exploration\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(0.5 * eval_count * np.pi / self.budget)  # Adjusted chaos\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08410 with standard deviation 0.00737.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.08824838804847734, 0.0903045189584839, 0.073741075343016]}}
{"id": "4ca236ce-b6b4-41d6-9ad5-d91db2df0dd3", "fitness": 0.0865029761894565, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced chaotic dynamics and adaptive mutation for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.4 * np.abs(np.sin(3 * np.pi * eval_count / self.budget))  # Enhanced chaotic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.cos(eval_count * np.pi / (2 * self.budget))  # Enhanced adaptive mutation\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(0.5 * eval_count * np.pi / self.budget)  # Enhanced chaos factor\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)  # Improved crossover dynamics\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08650 with standard deviation 0.00318.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.0898537672985269, 0.08742085197779803, 0.08223430929204456]}}
{"id": "d574149c-d3ee-4753-82f7-561ce4dbce28", "fitness": 0.08827674046457883, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with chaotic inertia, dynamic parameter adaptation, and crossover enhancements for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))  # Adaptive chaotic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)  # Improved crossover dynamics\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08828 with standard deviation 0.00190.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.09092683358206577, 0.08660175138687687, 0.08730163642479383]}}
{"id": "1cd55e5a-d51a-4e22-a7f7-1e37a3aea210", "fitness": 0.08616499132490563, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced chaotic dynamics with rhythmic inertia, boosted crossover, and dynamic mutation for accelerated convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.5 * np.abs(np.sin(4 * np.pi * eval_count / self.budget))  # Rhythmic chaotic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.3 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.9 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.9  # Enhanced chaos influence\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.3 * chaos)  # Boosted crossover dynamics\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08616 with standard deviation 0.00435.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.09204820782095668, 0.08166700221126588, 0.08477976394249431]}}
{"id": "10849864-cc41-480d-a603-8b8bdbab0880", "fitness": 0.08210536923226841, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "A refined hybrid optimizer incorporating dynamic cognitive scaling and non-linear adaptive crossover for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))  # Adaptive chaotic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            cognitive_scale = 1.5 + 0.5 * np.sin(np.pi * eval_count / self.budget)  # Dynamic cognitive scaling\n            velocities = (chaotic_inertia * velocities +\n                          cognitive_scale * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate * np.abs(chaos))  # Non-linear adaptive crossover\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08211 with standard deviation 0.00755.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.08803452007809609, 0.08682785340485755, 0.0714537342138516]}}
{"id": "78827715-ed05-453d-9757-58bad470628c", "fitness": 0.0644248543160384, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid approach with adaptive chaotic inertia, novel cooperative learning strategy, and dynamic differential weight to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))  # Adaptive chaotic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n\n            # Cooperative Learning Strategy\n            cooperative_component = 2.0\n            best_neighbors = np.mean(personal_best_positions, axis=0)\n            r3 = np.random.rand(self.dim)\n            velocities += cooperative_component * r3 * (best_neighbors - particles)\n\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                dynamic_differential_weight = differential_weight + 0.5 * np.cos(eval_count / self.budget * np.pi)\n                mutant_vector = personal_best_positions[a] + dynamic_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06442 with standard deviation 0.00516.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.05728589752381086, 0.06667431521987666, 0.06931435020442767]}}
{"id": "ace47baa-ba7c-40b9-8263-7c5ef207baaf", "fitness": 0.06995406128884223, "name": "EnhancedSynergisticMetaheuristicOptimizer", "description": "Enhanced synergy of chaotic dynamics and self-adaptive differential evolution with hybridized crossover for superior convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedSynergisticMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        initial_inertia = 0.9\n        final_inertia = 0.4\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            inertia_weight = initial_inertia - (initial_inertia - final_inertia) * (eval_count / self.budget)\n            chaotic_factor = np.sin(2 * np.pi * eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * (1 + chaotic_factor)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + chaos)  # Hybridized crossover with chaos\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedSynergisticMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06995 with standard deviation 0.00730.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.07772070838889933, 0.07195413113729376, 0.06018734434033357]}}
{"id": "8ec79391-7337-465c-9bc9-fe421dcbcc16", "fitness": 0.08389094644763088, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid strategy with adaptive chaotic inertia, improved crossover dynamics, smarter mutation, and dynamic population sizing for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            num_particles = max(10, int(30 * (1 - eval_count / self.budget)))  # Dynamic population size\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08389 with standard deviation 0.00602.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.09031804682636635, 0.08551446066330093, 0.07584033185322536]}}
{"id": "1512308c-b462-4fab-8232-cbaaaca9c6cf", "fitness": 0.08614078987975675, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined enhanced hybrid strategy with non-linear cognitive component scaling for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))  # Adaptive chaotic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.5 * np.sin(np.pi * eval_count / (2*self.budget)) * r1 * (personal_best_positions - particles) +  # Non-linear cognitive component scaling\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)  # Improved crossover dynamics\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08614 with standard deviation 0.00465.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.09222891032550462, 0.0852597847264055, 0.08093367458736012]}}
{"id": "9db12565-0ef5-4a0c-b55c-92d41e642a23", "fitness": 0.0775755030909528, "name": "AdvancedQuantumSwarmOptimizer", "description": "Advanced hybrid optimizer integrating quantum-behavioral dynamics, adaptive chaotic inertia, and parallel swarm intelligence for enhanced convergence precision.", "code": "import numpy as np\n\nclass AdvancedQuantumSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n        \n        # Quantum parameters\n        alpha = 0.5\n        \n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = inertia_weight * (0.5 + 0.5 * np.sin(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n                # Quantum behavior: perturbation from global best\n                perturbation = alpha * np.random.uniform(-1, 1, self.dim) * (global_best_position - particles[i])\n                particles[i] += perturbation\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 90, "feedback": "The algorithm AdvancedQuantumSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07758 with standard deviation 0.00607.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.08402086253432806, 0.07925655647515706, 0.06944909026337331]}}
{"id": "69ec2baf-e85a-4d9d-ac84-ca929ff5c2c5", "fitness": 0.08849978646552674, "name": "QuantumEnhancedMetaheuristicOptimizer", "description": "Introduce a novel quantum-inspired superposition operator with adaptive learning rates and elite-guided search to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass QuantumEnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n            \n            # Quantum-inspired superposition operator with adaptive learning rate\n            quantum_superposition = 0.5 * (personal_best_positions + global_best_position)\n            adaptive_learning_rate = 0.1 + 0.9 * eval_count / self.budget\n            superposed_particles = particles + adaptive_learning_rate * (quantum_superposition - particles)\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, superposed_particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 91, "feedback": "The algorithm QuantumEnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08850 with standard deviation 0.00162.", "error": "", "parent_ids": ["7fd4dc32-7728-417b-9f0f-a6a7d91ce858"], "operator": null, "metadata": {"aucs": [0.09072170270595836, 0.08688410528532409, 0.08789355140529775]}}
{"id": "31b054b2-da8e-4fc4-81c3-1106ca6b288f", "fitness": 0.08235679082165832, "name": "QuantumEnhancedMetaheuristicOptimizer", "description": "Introduce a dynamic adaptive mechanism for inertia weight and enhanced crossover operator to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass QuantumEnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            # Dynamic adaptive inertia weight \n            inertia_weight = 0.9 - 0.5 * (eval_count / self.budget)\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (inertia_weight * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n            \n            # Quantum-inspired superposition operator with adaptive learning rate\n            quantum_superposition = 0.5 * (personal_best_positions + global_best_position)\n            adaptive_learning_rate = 0.1 + 0.9 * eval_count / self.budget\n            superposed_particles = particles + adaptive_learning_rate * (quantum_superposition - particles)\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                # Enhanced crossover operator\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.3 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, superposed_particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 92, "feedback": "The algorithm QuantumEnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08236 with standard deviation 0.00342.", "error": "", "parent_ids": ["69ec2baf-e85a-4d9d-ac84-ca929ff5c2c5"], "operator": null, "metadata": {"aucs": [0.08716011146322067, 0.07942204377009854, 0.08048821723165578]}}
{"id": "81c178f9-34ef-4915-bf21-24756a5e103c", "fitness": 0.08409625096412514, "name": "EnhancedQuantumLevyMetaheuristicOptimizer", "description": "Implement a multi-phase quantum-inspired algorithm with hybrid chaotic and Levy flight mechanisms to boost convergence and solution quality.", "code": "import numpy as np\n\nclass EnhancedQuantumLevyMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.9 - 0.5 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))  # Modified chaotic inertia\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n            \n            # Quantum-inspired superposition operator with Levy flight\n            quantum_superposition = 0.5 * (personal_best_positions + global_best_position)\n            adaptive_learning_rate = 0.1 + 0.9 * eval_count / self.budget\n            levy_step = np.random.normal(size=(num_particles, self.dim)) * (1.5 / ((1.0 + np.abs(np.random.normal()))**(1 / 1.5)))\n            superposed_particles = particles + adaptive_learning_rate * (quantum_superposition - particles) + levy_step\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, superposed_particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedQuantumLevyMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08410 with standard deviation 0.00789.", "error": "", "parent_ids": ["69ec2baf-e85a-4d9d-ac84-ca929ff5c2c5"], "operator": null, "metadata": {"aucs": [0.08941536660609961, 0.08993420514934891, 0.07293918113692688]}}
{"id": "74f1536c-5905-4549-8480-f808e491318e", "fitness": 0.07164088906203685, "name": "AdvancedQuantumMetaheuristicOptimizer", "description": "Enhance quantum superposition with dynamic swarm adjustment, chaotic convergence strategy, and a multi-population framework for balanced exploration-exploitation.", "code": "import numpy as np\n\nclass AdvancedQuantumMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.full(num_particles, float('inf'))\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.5 + 0.4 * np.abs(np.cos(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n            \n            # Enhanced quantum-inspired superposition operator\n            quantum_superposition = np.mean(np.vstack((personal_best_positions, global_best_position)), axis=0)\n            adaptive_learning_rate = 0.1 + 0.9 * (1 - eval_count / self.budget)\n            superposed_particles = particles + adaptive_learning_rate * (quantum_superposition - particles)\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.cos(eval_count * np.pi / (2 * self.budget))\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.1 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, superposed_particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 94, "feedback": "The algorithm AdvancedQuantumMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07164 with standard deviation 0.00949.", "error": "", "parent_ids": ["69ec2baf-e85a-4d9d-ac84-ca929ff5c2c5"], "operator": null, "metadata": {"aucs": [0.08276846434673579, 0.07257100241479564, 0.0595832004245791]}}
{"id": "e01124e4-67c2-4084-8545-a81b0777b8f5", "fitness": 0.0842233192460284, "name": "QuantumEnhancedMetaheuristicOptimizer", "description": "Integrate a stochastic tunneling component for enhanced escape from local optima, while maintaining adaptive search dynamics.", "code": "import numpy as np\n\nclass QuantumEnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n            \n            # Quantum-inspired superposition operator with adaptive learning rate\n            quantum_superposition = 0.5 * (personal_best_positions + global_best_position)\n            adaptive_learning_rate = 0.1 + 0.9 * eval_count / self.budget\n            superposed_particles = particles + adaptive_learning_rate * (quantum_superposition - particles)\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                \n                # Stochastic tunneling component\n                tunneling_intensity = 0.05 * np.random.randn(self.dim)\n                mutant_vector += tunneling_intensity\n                \n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, superposed_particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 95, "feedback": "The algorithm QuantumEnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08422 with standard deviation 0.00396.", "error": "", "parent_ids": ["69ec2baf-e85a-4d9d-ac84-ca929ff5c2c5"], "operator": null, "metadata": {"aucs": [0.08884121776738363, 0.08465212673560374, 0.07917661323509784]}}
{"id": "0d68ffc2-3fc1-45c7-8d43-19bfcfdf3250", "fitness": 0.08138853562874308, "name": "EnhancedQuantumMetaheuristicOptimizer", "description": "Introduce chaotic perturbations and multi-elite guidance in the quantum-inspired superposition to enhance exploration and convergence speed.", "code": "import numpy as np\n\nclass EnhancedQuantumMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.cos(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n            \n            # Multi-elite guidance and quantum-inspired superposition with chaotic perturbation\n            elites = np.argsort(personal_best_scores)[:3]  # Select top 3 elite particles\n            elite_positions = personal_best_positions[elites]\n            quantum_superposition = np.mean(elite_positions, axis=0)\n            adaptive_learning_rate = 0.2 + 0.8 * (eval_count / self.budget)\n            chaos = 0.1 * np.sin(np.pi * eval_count / self.budget)\n            superposed_particles = quantum_superposition + adaptive_learning_rate * (particles - quantum_superposition) + chaos * (np.random.rand(self.dim) - 0.5)\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.cos(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.cos(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, superposed_particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedQuantumMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08139 with standard deviation 0.00606.", "error": "", "parent_ids": ["69ec2baf-e85a-4d9d-ac84-ca929ff5c2c5"], "operator": null, "metadata": {"aucs": [0.08917051014185651, 0.08059946635233939, 0.07439563039203334]}}
{"id": "3b423c84-4ed2-4d8e-8dd7-cd8f71aa35ca", "fitness": 0.08849978646552674, "name": "QuantumEnhancedMetaheuristicOptimizer", "description": "Refine the particle velocity update by introducing a dynamic inertia weight adjustment based on chaotic behavior for enhanced convergence.", "code": "import numpy as np\n\nclass QuantumEnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          1.2 * cognitive_component * r1 * (personal_best_positions - particles) +\n                          0.8 * social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n            \n            # Quantum-inspired superposition operator with adaptive learning rate\n            quantum_superposition = 0.5 * (personal_best_positions + global_best_position)\n            adaptive_learning_rate = 0.1 + 0.9 * eval_count / self.budget\n            superposed_particles = particles + adaptive_learning_rate * (quantum_superposition - particles)\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, superposed_particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 97, "feedback": "The algorithm QuantumEnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08850 with standard deviation 0.00162.", "error": "", "parent_ids": ["69ec2baf-e85a-4d9d-ac84-ca929ff5c2c5"], "operator": null, "metadata": {"aucs": [0.09072170270595836, 0.08688410528532409, 0.08789355140529775]}}
{"id": "d7ce6543-cd4f-4874-9eee-0a50edf21e13", "fitness": 0.0847632338602079, "name": "AdvancedQuantumAdaptiveOptimizer", "description": "Introduce a multi-phase adaptive quantum-inspired optimization strategy with chaotic and Lvy flight dynamics for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdvancedQuantumAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        num_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (num_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (num_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * num_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = 0.7 - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n            \n            # Quantum-inspired superposition operator with adaptive learning rate\n            quantum_superposition = 0.5 * (personal_best_positions + global_best_position)\n            adaptive_learning_rate = 0.1 + 0.9 * eval_count / self.budget\n            superposed_particles = particles + adaptive_learning_rate * (quantum_superposition - particles)\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n            \n            # Introduce Lvy flight for enhanced exploration\n            levy_flight = np.random.standard_cauchy(self.dim) * 0.01\n            superposed_particles += levy_flight\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, superposed_particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 98, "feedback": "The algorithm AdvancedQuantumAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08476 with standard deviation 0.00405.", "error": "", "parent_ids": ["69ec2baf-e85a-4d9d-ac84-ca929ff5c2c5"], "operator": null, "metadata": {"aucs": [0.09047941148651895, 0.0817125466083628, 0.08209774348574195]}}
{"id": "29d2f795-6d59-4add-aa7b-9ce9977454b7", "fitness": 0.08353585611622849, "name": "EnhancedQuantumMetaheuristicOptimizer", "description": "Enhance the quantum-inspired metaheuristic with dynamic population resizing and self-adaptive chaos control for improved convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedQuantumMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialize parameters\n        initial_particles = 30\n        inertia_weight = 0.9\n        cognitive_component = 1.5\n        social_component = 1.5\n        differential_weight = 0.8\n        crossover_rate = 0.9\n\n        # Initialize particles\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = np.random.uniform(lb, ub, (initial_particles, self.dim))\n        velocities = np.random.uniform(-1, 1, (initial_particles, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_scores = np.array([float('inf')] * initial_particles)\n        \n        # Global best initialization\n        global_best_position = None\n        global_best_score = float('inf')\n        \n        eval_count = 0\n\n        while eval_count < self.budget:\n            num_particles = max(10, initial_particles - eval_count // (self.budget // 3))  # Dynamic population\n            for i in range(num_particles):\n                score = func(particles[i])\n                eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = particles[i]\n\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = particles[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            chaotic_inertia = inertia_weight - 0.6 * np.abs(np.sin(2 * np.pi * eval_count / self.budget))\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities = (chaotic_inertia * velocities +\n                          cognitive_component * r1 * (personal_best_positions - particles) +\n                          social_component * r2 * (global_best_position - particles))\n            particles += velocities\n            particles = np.clip(particles, lb, ub)\n            \n            # Quantum-inspired superposition operator with adaptive learning rate\n            quantum_superposition = 0.5 * (personal_best_positions + global_best_position)\n            adaptive_learning_rate = 0.1 + 0.9 * eval_count / self.budget\n            superposed_particles = particles + adaptive_learning_rate * (quantum_superposition - particles)\n            superposed_particles = np.clip(superposed_particles, lb, ub)\n\n            for i in range(num_particles):\n                if eval_count >= self.budget:\n                    break\n                indices = list(range(num_particles))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adaptive_differential_weight = differential_weight * np.sin(eval_count / self.budget)\n                chaos = np.sin(0.5 * eval_count * np.pi / self.budget)\n                mutant_vector = personal_best_positions[a] + adaptive_differential_weight * (personal_best_positions[b] - personal_best_positions[c])\n                mutant_vector += chaos * (np.random.rand(self.dim) - 0.5) * 0.8\n                cross_points = np.random.rand(self.dim) < (crossover_rate + 0.2 * chaos)\n                trial_vector = np.where(cross_points, mutant_vector, superposed_particles[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                trial_score = func(trial_vector)\n                eval_count += 1\n\n                if trial_score < personal_best_scores[i]:\n                    personal_best_scores[i] = trial_score\n                    personal_best_positions[i] = trial_vector\n\n                if trial_score < global_best_score:\n                    global_best_score = trial_score\n                    global_best_position = trial_vector\n\n        return global_best_position, global_best_score", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedQuantumMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08354 with standard deviation 0.00530.", "error": "", "parent_ids": ["69ec2baf-e85a-4d9d-ac84-ca929ff5c2c5"], "operator": null, "metadata": {"aucs": [0.09071949832753334, 0.07809906294359314, 0.08178900707755898]}}
