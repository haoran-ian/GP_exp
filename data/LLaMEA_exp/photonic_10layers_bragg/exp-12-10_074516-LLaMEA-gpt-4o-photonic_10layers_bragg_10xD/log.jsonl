{"id": "b479984b-59ae-41ac-8538-efcc080ba6b3", "fitness": 0.06522791106694763, "name": "AdaptiveMultiSwarmOptimizer", "description": "A novel \"Adaptive Multi-Swarm Particle Optimization\" that dynamically adjusts swarm sizes and interactions based on exploration-exploitation balance to efficiently solve black-box optimization problems.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06523 with standard deviation 0.00336.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06784419253276208, 0.06735562520036564, 0.060483915467715144]}}
{"id": "dd07a463-b790-46d6-aee1-bc15fa8fd657", "fitness": 0.06297357256704916, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "An enhanced \"Adaptive Multi-Swarm Particle Optimization\" algorithm that utilizes a dynamic inertia weight and velocity constriction to improve convergence speed and solution accuracy in black-box optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w_max = 0.9  # Maximum inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n        self.vel_constriction_factor = 0.5\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub, global_best_value)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub, global_best_value):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        current_inertia = ((self.w_max - self.w_min) * (self.budget - self.func_evals) / self.budget) + self.w_min\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (current_inertia * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            velocities[i] *= self.vel_constriction_factor  # Apply velocity constriction\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06297 with standard deviation 0.00107.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06156815158335893, 0.06416338302807456, 0.063189183089714]}}
{"id": "664f0439-9858-4fc5-a920-76206cdded63", "fitness": 0.06522774829241189, "name": "ProgressiveMultiSwarmOptimizer", "description": "An enhanced \"Progressive Multi-Swarm Particle Optimization\" utilizing progressive fitness-based swarm migration and dynamic parameter tuning to effectively navigate complex black-box landscapes.", "code": "import numpy as np\n\nclass ProgressiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n            # Progressive migration strategy\n            self._progressive_migration(swarms, global_best, lb, ub)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n    def _progressive_migration(self, swarms, global_best, lb, ub):\n        # Introduce progressive migration where particles from the worst-performing swarm migrate to the best one\n        fitness_values = [np.min(swarm[\"personal_best_values\"]) for swarm in swarms]\n        worst_swarm_idx = np.argmax(fitness_values)\n        best_swarm_idx = np.argmin(fitness_values)\n\n        if worst_swarm_idx != best_swarm_idx:\n            worst_swarm = swarms[worst_swarm_idx]\n            best_swarm = swarms[best_swarm_idx]\n\n            num_migrants = self.particles_per_swarm // 2\n            migrants = worst_swarm[\"positions\"][:num_migrants]\n\n            # Reassign migrants to the best swarm\n            best_swarm[\"positions\"][-num_migrants:] = migrants\n\n            # Reset personal bests of migrants\n            best_swarm[\"personal_bests\"][-num_migrants:] = migrants\n            best_swarm[\"personal_best_values\"][-num_migrants:] = np.inf", "configspace": "", "generation": 2, "feedback": "The algorithm ProgressiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06523 with standard deviation 0.00336.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06784419253276208, 0.06735562520036564, 0.060483427144107926]}}
{"id": "a6c27e47-1fa7-42e8-84f1-181589faf3db", "fitness": 0.059413768053445594, "name": "AdaptiveMultiSwarmOptimizer", "description": "A refined \"Adaptive Multi-Swarm Particle Optimization\" with dynamic inertia weights and adaptive learning rates to enhance convergence speed and accuracy in solving black-box optimization problems.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w_min = 0.3  # Minimum inertia weight\n        self.w_max = 0.9  # Maximum inertia weight\n        self.c1_min = 1.0  # Minimum cognitive component\n        self.c1_max = 2.0  # Maximum cognitive component\n        self.c2_min = 1.0  # Minimum social component\n        self.c2_max = 2.0  # Maximum social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            w = self._dynamic_inertia_weight()\n            c1 = self._adaptive_learning_rate(global_best, positions[i], self.c1_min, self.c1_max)\n            c2 = self._adaptive_learning_rate(global_best, positions[i], self.c2_min, self.c2_max)\n\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_bests[i] - positions[i]) +\n                             c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _dynamic_inertia_weight(self):\n        \"\"\" Dynamically adjust inertia weight over time to balance exploration and exploitation. \"\"\"\n        return self.w_max - (self.w_max - self.w_min) * (self.func_evals / self.budget)\n\n    def _adaptive_learning_rate(self, global_best, position, min_rate, max_rate):\n        \"\"\" Adaptive learning rate based on distance to the global best. \"\"\"\n        distance = np.linalg.norm(global_best - position)\n        norm_distance = distance / np.sqrt(self.dim)\n        return min_rate + (max_rate - min_rate) * (1 - norm_distance)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05941 with standard deviation 0.00254.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06039503433698301, 0.06191124917132251, 0.055935020652031264]}}
{"id": "69379422-5c6f-45bd-822d-11f0916319f4", "fitness": 0.06522791106694763, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "An enhanced \"Adaptive Multi-Swarm Particle Optimization\" leveraging dynamic parameter adjustment and spatial diversity to improve convergence and efficiency for complex black-box optimization problems.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n        self.exploration_factor = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        swarm_improvements = [self._swarm_improvement(swarm) for swarm in swarms]\n\n        for i, improvement in enumerate(swarm_improvements):\n            if improvement < 1e-6:  # Threshold for stagnation\n                self._introduce_diversity(swarms[i])\n\n        if global_best_value < 1e-6:  # Global threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n    def _swarm_improvement(self, swarm):\n        return np.min(swarm[\"personal_best_values\"])\n\n    def _introduce_diversity(self, swarm):\n        lb, ub = np.min(swarm[\"positions\"], axis=0), np.max(swarm[\"positions\"], axis=0)\n        new_positions = np.random.uniform(lb - self.exploration_factor * (ub - lb),\n                                          ub + self.exploration_factor * (ub - lb),\n                                          (self.particles_per_swarm, self.dim))\n        swarm[\"positions\"] = np.clip(new_positions, lb, ub)", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06523 with standard deviation 0.00336.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06784419253276208, 0.06735562520036564, 0.060483915467715144]}}
{"id": "9e47afc4-7474-4d44-855d-71dab51a3ec3", "fitness": 0.06165275619534435, "name": "AdaptiveMultiSwarmOptimizer", "description": "A refined \"Adaptive Multi-Swarm Particle Optimization\" that subtly enhances particle velocity update by incorporating an additional random perturbation to improve exploration.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             np.random.normal(0, 0.1, self.dim))  # Added random perturbation\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06165 with standard deviation 0.00102.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.061973591611074186, 0.062712220276394, 0.06027245669856485]}}
{"id": "3b3865f7-6961-419b-b4a8-02059b596cde", "fitness": 0.06522791106694763, "name": "HierarchicalAdaptiveMultiSwarmOptimizer", "description": "An enhanced \"Hierarchical Adaptive Multi-Swarm Particle Optimization\" that incorporates hierarchical swarm interactions and adaptive learning rates to improve convergence speed and solution accuracy in black-box optimization problems.", "code": "import numpy as np\n\nclass HierarchicalAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n        self.learning_rate = 0.1  # Adaptive learning rate for dynamic adjustments\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters using hierarchical interaction\n            self._hierarchical_adaptation(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _hierarchical_adaptation(self, swarms, global_best_value):\n        improvement_threshold = 1e-6  # Set a threshold for improvement\n        if global_best_value < improvement_threshold:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n            self.w *= (1 + self.learning_rate)  # Increase inertia\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n            self.w *= (1 - self.learning_rate)  # Decrease inertia for more exploration\n\n        # Further enhance learning rates adaptively\n        self.c1 = 1.5 + self.learning_rate\n        self.c2 = 1.5 - self.learning_rate", "configspace": "", "generation": 6, "feedback": "The algorithm HierarchicalAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06523 with standard deviation 0.00336.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06784419253276208, 0.06735562520036564, 0.060483915467715144]}}
{"id": "2b9ba134-cc2b-43af-9a3c-49592b5f22aa", "fitness": 0.06522791106694763, "name": "AdaptiveMultiSwarmOptimizer", "description": "Enhanced inertia weight adaptation for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n        self.w = 0.4 + 0.5 * (self.func_evals / self.budget)  # New line: Adapt inertia weight", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06523 with standard deviation 0.00336.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06784419253276208, 0.06735562520036564, 0.060483915467715144]}}
{"id": "18c540af-d57d-471a-b68d-fe9d5ee06d46", "fitness": 0.059935011982067334, "name": "AdaptiveMultiSwarmOptimizer", "description": "Improved the \"Adaptive Multi-Swarm Particle Optimization\" by adding randomness to global best influence, enhancing exploration without exceeding line change limits.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)  # Added r3 for randomness in global best influence\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best * r3 - positions[i]))  # Modified global_best influence\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05994 with standard deviation 0.00198.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06066598284016189, 0.06191124917132251, 0.0572278039347176]}}
{"id": "6d5585cc-9859-47dc-89a3-3e2e9c9d62f2", "fitness": 0.06321945610556971, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Enhance the Adaptive Multi-Swarm Particle Optimization by incorporating a nonlinear inertia weight adjustment and adaptive learning factors based on convergence progress for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w_init = 0.9  # Initial inertia weight\n        self.w_final = 0.4  # Final inertia weight\n        self.c1_init = 2.5  # Initial cognitive component\n        self.c2_init = 0.5  # Initial social component\n        self.c1_final = 0.5  # Final cognitive component\n        self.c2_final = 2.5  # Final social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            progress_ratio = self.func_evals / self.budget\n            w = self._calculate_inertia_weight(progress_ratio)\n            c1, c2 = self._calculate_learning_factors(progress_ratio)\n\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub, w, c1, c2)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub, w, c1, c2):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_bests[i] - positions[i]) +\n                             c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n    def _calculate_inertia_weight(self, progress_ratio):\n        return self.w_init - (self.w_init - self.w_final) * progress_ratio\n\n    def _calculate_learning_factors(self, progress_ratio):\n        c1 = self.c1_init - (self.c1_init - self.c1_final) * progress_ratio\n        c2 = self.c2_init + (self.c2_final - self.c2_init) * progress_ratio\n        return c1, c2", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06322 with standard deviation 0.00213.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06443522151167225, 0.06499541526970265, 0.06022773153533423]}}
{"id": "4e714f69-8b3e-458d-a777-342a50fcb69c", "fitness": 0.06517299416969187, "name": "AdaptiveMultiSwarmOptimizer", "description": "Enhanced \"Adaptive Multi-Swarm Particle Optimization\" with improved inertia weight adaptation to balance exploration-exploitation more effectively.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = ((0.9 - 0.4 * (self.func_evals / self.budget)) * velocities[i] +  # Adjusted inertia weight\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06517 with standard deviation 0.00340.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06782600308657782, 0.06732199085302526, 0.06037098856947254]}}
{"id": "8bb6cb06-5aa9-46f2-a0fa-a12dd5925d82", "fitness": 0.06522791106694763, "name": "AdaptiveMultiSwarmOptimizer", "description": "A refined \"Adaptive Multi-Swarm Particle Optimization\" that enhances swarm adaptability using slight inertia weight decay to improve optimization performance.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n            self.w *= 0.99  # Decay inertia weight slightly by 1%\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06523 with standard deviation 0.00336.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06784419253276208, 0.06735562520036564, 0.060483915467715144]}}
{"id": "47fa9399-4dfb-4d20-bd72-6e4da7fe635e", "fitness": 0.06515840461154221, "name": "AdaptiveMultiSwarmOptimizer", "description": "A refined \"Adaptive Multi-Swarm Particle Optimization\" utilizes adaptive inertia weight adjustment for enhanced convergence efficiency in black-box optimization tasks.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            # Change: Updated inertia weight to adaptively adjust based on velocity magnitude\n            self.w = 0.9 - (0.5 * np.linalg.norm(velocities[i]) / np.linalg.norm(lb - ub))\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06516 with standard deviation 0.00341.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.0678181077216441, 0.06731428361180736, 0.060342822501175175]}}
{"id": "0bade488-aee4-484b-857b-79e5763e01d5", "fitness": 0.06517678239248148, "name": "DynamicConvergenceSwarmOptimizer", "description": "Introducing \"Dynamic Convergence Swarm Optimization\" (DCSO), which enhances convergence by dynamically adjusting particle velocities and swarm sizes through self-adaptive inertia weights and informed communication among particles.", "code": "import numpy as np\n\nclass DynamicConvergenceSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w_init = 0.9  # Initial inertia weight\n        self.w_final = 0.4  # Final inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n        self.global_best_value = np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < self.global_best_value:\n                    self.global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub, func)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub, func):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        w = self._adaptive_inertia_weight()\n        \n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_inertia_weight(self):\n        return self.w_final + (self.w_init - self.w_final) * (1 - self.func_evals / self.budget)\n\n    def _adaptive_adjustment(self, swarms, global_best):\n        # Example simple adjustment strategy based on global best improvement\n        if self.global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 13, "feedback": "The algorithm DynamicConvergenceSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06518 with standard deviation 0.00340.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06782798069284557, 0.06732422460002296, 0.06037814188457591]}}
{"id": "c136b67f-d592-45fe-8e50-298120aef5c0", "fitness": -Infinity, "name": "HierarchicalAdaptiveMultiSwarmOptimizer", "description": "An enhanced \"Hierarchical Adaptive Multi-Swarm Optimization\" that introduces hierarchical swarm interactions and dynamic parameter adjustment for improved convergence in black-box optimization tasks.", "code": "import numpy as np\n\nclass HierarchicalAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Introduce hierarchical adaptive adjustment\n            self._hierarchical_adjustment(swarms, global_best_value, lb, ub)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _hierarchical_adjustment(self, swarms, global_best_value, lb, ub):\n        # Hierarchical adjustment strategy based on swarm performance\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Intra-swarm learning enhancement\n        for swarm in swarms:\n            if self.func_evals >= self.budget:\n                break\n            intra_positions = swarm[\"positions\"]\n            for i, pos in enumerate(intra_positions):\n                neighborhood_best_value = func(pos)\n                if neighborhood_best_value < swarm[\"personal_best_values\"][i]:\n                    swarm[\"personal_best_values\"][i] = neighborhood_best_value\n                    swarm[\"personal_bests\"][i] = pos", "configspace": "", "generation": 14, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {}}
{"id": "c127276b-539c-4662-a615-e0f47fe5d27e", "fitness": 0.06515814989989284, "name": "AdaptiveMultiSwarmOptimizer", "description": "Improved inertia weight adjustment in \"Adaptive Multi-Swarm Particle Optimization\" for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.9  # Updated Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Example simple adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)", "configspace": "", "generation": 15, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06516 with standard deviation 0.00341.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06781802379170654, 0.06731416144029423, 0.060342264467677764]}}
{"id": "2e0fd0c3-7c9a-4b68-8454-690bba462e5e", "fitness": 0.06522791106694763, "name": "AdaptiveMultiSwarmOptimizer", "description": "An enhanced \"Adaptive Multi-Swarm Particle Optimization\" with refined swarm parameter adjustment for improved convergence efficiency.", "code": "import numpy as np\n\nclass AdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptively adjust swarm parameters\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Modified adjustment strategy based on global best improvement\n        if global_best_value < 1e-6:  # Threshold example\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n            self.w = max(0.4, self.w - 0.05)  # Slightly decrease inertia weight", "configspace": "", "generation": 16, "feedback": "The algorithm AdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06523 with standard deviation 0.00336.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06784419253276208, 0.06735562520036564, 0.060483915467715144]}}
{"id": "19e04d49-80ae-4f8c-bfe5-75d52c84f389", "fitness": 0.06522888604993211, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Enhanced \"Adaptive Multi-Swarm Particle Optimization\" using dynamic parameter tuning and swarm synergy to balance exploration and exploitation, boosting convergence rates for black-box optimization problems.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06523 with standard deviation 0.00336.", "error": "", "parent_ids": ["b479984b-59ae-41ac-8538-efcc080ba6b3"], "operator": null, "metadata": {"aucs": [0.06784482784330437, 0.06735616758544294, 0.06048566272104905]}}
{"id": "d3f2cd9e-03c5-43c4-a32c-a13d12e78f5f", "fitness": 0.06533267841955626, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Enhanced adaptive parameter tuning through synergy to improve convergence in black-box optimization using a multi-swarm particle approach.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06533 with standard deviation 0.00309.", "error": "", "parent_ids": ["19e04d49-80ae-4f8c-bfe5-75d52c84f389"], "operator": null, "metadata": {"aucs": [0.06805753391993408, 0.06693591935625065, 0.06100458198248404]}}
{"id": "b5f91d38-09b7-482d-949f-d3e26bb21665", "fitness": 0.06527162449251878, "name": "DynamicHierarchicalSwarmOptimizer", "description": "Introducing Dynamic Hierarchical Swarm Optimization with a Fuzzy Logic Controller for adaptive exploration-exploitation balance to enhance convergence in black-box problems.", "code": "import numpy as np\n\nclass DynamicHierarchicalSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.func_evals = 0\n        self.w_decay = 0.98\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w = self.max_inertia\n        self.c1 = 1.5\n        self.c2 = 1.6\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize swarms\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            self._fuzzy_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _fuzzy_adjustment(self, swarms, global_best_value):\n        # Fuzzy Logic Controller for adaptive parameter tuning\n        improvement = (global_best_value < 1e-6)\n        if improvement:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Adjust cognitive and social components using fuzzy logic\n        if self.num_swarms > 5:\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n        else:\n            self.c1 = min(2.0, self.c1 * 1.05)\n            self.c2 = max(1.0, self.c2 * 0.95)", "configspace": "", "generation": 19, "feedback": "The algorithm DynamicHierarchicalSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06527 with standard deviation 0.00314.", "error": "", "parent_ids": ["d3f2cd9e-03c5-43c4-a32c-a13d12e78f5f"], "operator": null, "metadata": {"aucs": [0.06804033500137374, 0.06689633558626695, 0.060878202889915634]}}
{"id": "3bc25515-cc91-4757-b787-687ce0838bcf", "fitness": 0.06416192109846353, "name": "HierarchicalSynergySwarmOptimizer", "description": "Introduce a hierarchical swarm structure with local and global synergies for enhanced exploration-exploitation balance in multi-swarm optimization.", "code": "import numpy as np\n\nclass HierarchicalSynergySwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.global_synergy_factor = 0.5\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Hierarchical synergy adjustment\n            self._hierarchical_adjustment(swarms, global_best_value, global_best)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (swarm_best - positions[i]) +\n                             self.global_synergy_factor * r3 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _hierarchical_adjustment(self, swarms, global_best_value, global_best):\n        # Dynamic adjustment based on global synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component\n        self.global_synergy_factor = min(1.0, self.global_synergy_factor * 1.01)  # Strengthen global influence", "configspace": "", "generation": 20, "feedback": "The algorithm HierarchicalSynergySwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06416 with standard deviation 0.00191.", "error": "", "parent_ids": ["d3f2cd9e-03c5-43c4-a32c-a13d12e78f5f"], "operator": null, "metadata": {"aucs": [0.06548621405742494, 0.06553861083080204, 0.0614609384071636]}}
{"id": "8ba004ff-a083-4086-a134-9b8fd36cccef", "fitness": 0.06533223630818719, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Refined inertia weight decay to enhance adaptability and convergence in multi-swarm particle optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.995  # Changed from 0.99 to 0.995\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06533 with standard deviation 0.00309.", "error": "", "parent_ids": ["d3f2cd9e-03c5-43c4-a32c-a13d12e78f5f"], "operator": null, "metadata": {"aucs": [0.06805731336084775, 0.0669356523289184, 0.061003743234795405]}}
{"id": "1313b999-4139-4a82-bb42-e77059c2109f", "fitness": 0.06533267841955626, "name": "EnhancedAdaptiveMultiSwarmOptimizerV2", "description": "Introduction of a diversity-enhanced multi-swarm strategy with dynamic neighborhood topology to improve exploration-exploitation balance and convergence speed in black-box optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.diversity_threshold = 0.01  # Diversity threshold for dynamic topology\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component\n\n        # Diversity enhancement through dynamic neighborhood topology\n        for swarm in swarms:\n            diversity = np.mean(np.std(swarm[\"positions\"], axis=0))\n            if diversity < self.diversity_threshold:\n                self._increase_diversity(swarm)\n\n    def _increase_diversity(self, swarm):\n        # Introduce random changes to increase diversity\n        random_indices = np.random.choice(self.particles_per_swarm, size=2, replace=False)\n        swarm[\"positions\"][random_indices[0]] += np.random.uniform(-0.1, 0.1, self.dim)\n        swarm[\"positions\"][random_indices[1]] -= np.random.uniform(-0.1, 0.1, self.dim)\n        swarm[\"positions\"] = np.clip(swarm[\"positions\"], func.bounds.lb, func.bounds.ub)", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06533 with standard deviation 0.00309.", "error": "", "parent_ids": ["d3f2cd9e-03c5-43c4-a32c-a13d12e78f5f"], "operator": null, "metadata": {"aucs": [0.06805753391993408, 0.06693591935625065, 0.06100458198248404]}}
{"id": "6fbf1ea0-7e52-45ea-9581-a69197c90268", "fitness": 0.06284866938231785, "name": "AdvancedAdaptiveMultiSwarmOptimizer", "description": "Integrate stochastic perturbations with adaptive learning to enhance convergence and diversity control in multi-swarm particle optimization.", "code": "import numpy as np\n\nclass AdvancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Start with higher inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.stochastic_perturbation_prob = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._apply_stochastic_perturbation(swarm, lb, ub)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _apply_stochastic_perturbation(self, swarm, lb, ub):\n        for i in range(self.particles_per_swarm):\n            if np.random.rand() < self.stochastic_perturbation_prob:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                swarm[\"positions\"][i] = np.clip(swarm[\"positions\"][i] + perturbation, lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)", "configspace": "", "generation": 23, "feedback": "The algorithm AdvancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06285 with standard deviation 0.00274.", "error": "", "parent_ids": ["d3f2cd9e-03c5-43c4-a32c-a13d12e78f5f"], "operator": null, "metadata": {"aucs": [0.06361019432872728, 0.06575423896891275, 0.05918157484931352]}}
{"id": "562ad5be-dd3e-4e81-88b0-7448f00b3e98", "fitness": 0.06517247535081098, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Improved convergence by tuning inertia and enhancing swarm communication dynamics with a focus on more efficient exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Changed: Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.7  # Changed: Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.995  # Changed: Inertia decay\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06517 with standard deviation 0.00290.", "error": "", "parent_ids": ["d3f2cd9e-03c5-43c4-a32c-a13d12e78f5f"], "operator": null, "metadata": {"aucs": [0.06752479051128235, 0.06690482080514004, 0.06108781473601055]}}
{"id": "db16521c-f922-47bd-b1b8-d60098626da2", "fitness": 0.06530172694424163, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Enhanced inertia weight strategy for improved convergence in black-box optimization using a multi-swarm particle approach.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06530 with standard deviation 0.00312.", "error": "", "parent_ids": ["d3f2cd9e-03c5-43c4-a32c-a13d12e78f5f"], "operator": null, "metadata": {"aucs": [0.0680486692836676, 0.06691603366764054, 0.060940477881416766]}}
{"id": "b4cefebe-7e81-4a40-80f0-6bfc1dab57f7", "fitness": 0.06654623949208431, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Introduced a dynamic velocity clamping mechanism to enhance global exploration and prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = (ub - lb) * 0.2  # Dynamic velocity clamping\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06655 with standard deviation 0.00389.", "error": "", "parent_ids": ["d3f2cd9e-03c5-43c4-a32c-a13d12e78f5f"], "operator": null, "metadata": {"aucs": [0.07200069189680436, 0.06444996927489433, 0.06318805730455423]}}
{"id": "117d6b7a-91c1-42e6-b86b-7e002ec5c59c", "fitness": 0.06435174595365607, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Improved swarm resilience by introducing a mutation step to occasionally perturb particle positions.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = (ub - lb) * 0.2  # Dynamic velocity clamping\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n            \n            # Mutation step\n            if np.random.rand() < 0.05:  # 5% chance to mutate\n                positions[i] += np.random.uniform(lb, ub, self.dim) * 0.1\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06435 with standard deviation 0.00149.", "error": "", "parent_ids": ["b4cefebe-7e81-4a40-80f0-6bfc1dab57f7"], "operator": null, "metadata": {"aucs": [0.06644885566347081, 0.06341832489294319, 0.06318805730455423]}}
{"id": "6156e482-51b2-4a86-baa1-be84b21ebf1a", "fitness": 0.06654608153709081, "name": "HierarchicalSwarmOptimizer", "description": "Introduce a hierarchical learning mechanism to reinforce swarm collaboration and improve convergence speed.", "code": "import numpy as np\n\nclass HierarchicalSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.98\n        self.lb = None\n        self.ub = None\n        self.v_max_factor = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.lb, self.ub = bounds.lb, bounds.ub\n        self.v_max = (self.ub - self.lb) * self.v_max_factor\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm() for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best)\n\n            # Hierarchical adjustment based on synergy and results\n            self._hierarchical_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self):\n        positions = np.random.uniform(self.lb, self.ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], self.lb, self.ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _hierarchical_adjustment(self, swarms, global_best_value):\n        # Hierarchical adjustment considering overall performance and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(30, self.particles_per_swarm + 2)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Adaptive synergy-based parameter tuning\n        self.c1 = max(1.0, self.c1 * 0.9)  # Dynamic cognitive component\n        self.c2 = min(2.5, self.c2 * 1.1)  # Dynamic social component", "configspace": "", "generation": 28, "feedback": "The algorithm HierarchicalSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06655 with standard deviation 0.00389.", "error": "", "parent_ids": ["b4cefebe-7e81-4a40-80f0-6bfc1dab57f7"], "operator": null, "metadata": {"aucs": [0.07200069189680436, 0.06444949540991385, 0.06318805730455423]}}
{"id": "f423aa27-ba91-4bb2-bebd-1c4ca8241c67", "fitness": -Infinity, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Introduced adaptive learning rates for swarm particles to improve convergence speed and precision.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping, initialized later\n        self.lr_min = 0.01\n        self.lr_max = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.2\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values,\n                \"learning_rates\": np.full((self.particles_per_swarm, self.dim), self.lr_max)}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n        learning_rates = swarm[\"learning_rates\"]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2 = np.random.rand(2)\n            velocities[i] = (self.w * velocities[i] +\n                             learning_rates[i] * (self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                                                  self.c2 * r2 * (global_best - positions[i])))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n            # Dynamic adjustment of learning rates\n            if np.linalg.norm(velocities[i]) > np.linalg.norm(global_best - positions[i]):\n                learning_rates[i] = max(self.lr_min, learning_rates[i] * 0.9)\n            else:\n                learning_rates[i] = min(self.lr_max, learning_rates[i] * 1.1)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 29, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["b4cefebe-7e81-4a40-80f0-6bfc1dab57f7"], "operator": null, "metadata": {}}
{"id": "635459f9-e5ae-401d-b81b-edc1bdc6fecc", "fitness": 0.06720490627021743, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Introduced swarm communication enhancement by integrating a local neighborhood best to refine exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.2  # Update dynamic velocity clamping here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06720 with standard deviation 0.00195.", "error": "", "parent_ids": ["b4cefebe-7e81-4a40-80f0-6bfc1dab57f7"], "operator": null, "metadata": {"aucs": [0.06967561215688467, 0.06492075217590487, 0.06701835447786275]}}
{"id": "83b569af-1a96-467a-80d2-b02ac7bfe4c9", "fitness": 0.06720490627021743, "name": "EnhancedAdaptiveMultiSwarmOptimizerV2", "description": "Enhance swarm dynamics by introducing particle diversity control and adaptive velocity scaling for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n        self.diversity_threshold = 0.1  # Control swarm diversity\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.2  # Update dynamic velocity clamping here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n        \n        # Control particle diversity\n        particle_diversity = np.std(positions, axis=0).mean()\n        if particle_diversity < self.diversity_threshold:\n            # Introduce random perturbation to increase diversity\n            perturbation = np.random.uniform(-0.1, 0.1, positions.shape)\n            positions += perturbation\n            positions = np.clip(positions, lb, ub)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06720 with standard deviation 0.00195.", "error": "", "parent_ids": ["635459f9-e5ae-401d-b81b-edc1bdc6fecc"], "operator": null, "metadata": {"aucs": [0.06967561215688467, 0.06492075217590487, 0.06701835447786275]}}
{"id": "039a5568-092c-44a8-b00c-d16da0597763", "fitness": 0.06720490627021743, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "Enhanced swarm synergy by refining parameter adaptation thresholds and velocity update strategy.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.2  # Update dynamic velocity clamping here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             1.2 * self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Modified local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        self.w = max(self.min_inertia, self.w * self.w_decay)\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-7:  # Modified threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06720 with standard deviation 0.00195.", "error": "", "parent_ids": ["635459f9-e5ae-401d-b81b-edc1bdc6fecc"], "operator": null, "metadata": {"aucs": [0.06967561215688467, 0.06492075217590487, 0.06701835447786275]}}
{"id": "efaf8572-ed50-4052-b52a-72dd415794c0", "fitness": 0.06719601787117639, "name": "EnhancedAdaptiveMultiSwarmOptimizer", "description": "EnhancedAdaptiveMultiSwarmOptimizer with Self-Adaptive Velocity and Dynamic Swarm Reconfiguration for improved balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.2  # Update dynamic velocity clamping here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Self-adaptive inertia\n        swarm_best_distance = np.linalg.norm(global_best - swarm_best)\n        self.w = max(self.min_inertia, min(self.max_inertia, self.w * (1 + swarm_best_distance / (ub - lb).mean())))\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06720 with standard deviation 0.00196.", "error": "", "parent_ids": ["635459f9-e5ae-401d-b81b-edc1bdc6fecc"], "operator": null, "metadata": {"aucs": [0.06967561215688467, 0.06489408697878174, 0.06701835447786275]}}
{"id": "6eef6bd8-757c-4045-9a82-5ca9c33ac37d", "fitness": 0.06720852453918306, "name": "EnhancedAdaptiveMultiSwarmOptimizerV2", "description": "Introduce a multi-velocity adaptation mechanism and swarm synergy-based dynamic parameter adjustment to refine exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1  # New parameter for velocity adaptation\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.2  # Update dynamic velocity clamping here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Modify inertia weight dynamically based on velocity adaptation strategy\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedAdaptiveMultiSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06721 with standard deviation 0.00194.", "error": "", "parent_ids": ["635459f9-e5ae-401d-b81b-edc1bdc6fecc"], "operator": null, "metadata": {"aucs": [0.06967561215688467, 0.06493160698280176, 0.06701835447786275]}}
{"id": "989587fd-48f5-4969-aa59-55a0aaca5648", "fitness": 0.06960856878339357, "name": "AdvancedDecentralizedSwarmOptimizer", "description": "Introduce a decentralized swarm synergy mechanism with adaptive multi-velocity and hierarchical swarm interactions to optimize exploration-exploitation balance further.", "code": "import numpy as np\n\nclass AdvancedDecentralizedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1  # New parameter for velocity adaptation\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.2  # Update dynamic velocity clamping here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Modify inertia weight dynamically based on velocity adaptation strategy\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index):\n        if swarm_index == 0:\n            return  # Skip interaction for the first swarm\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 35, "feedback": "The algorithm AdvancedDecentralizedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06961 with standard deviation 0.00190.", "error": "", "parent_ids": ["6eef6bd8-757c-4045-9a82-5ca9c33ac37d"], "operator": null, "metadata": {"aucs": [0.0721678776830248, 0.06762665279263547, 0.06903117587452046]}}
{"id": "ae881340-65ba-426e-a8c4-e05c2d253381", "fitness": 0.06981750811721847, "name": "AdvancedDecentralizedSwarmOptimizer", "description": "Introduce dynamic velocity scaling to enhance global exploration capabilities.", "code": "import numpy as np\n\nclass AdvancedDecentralizedSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1  # New parameter for velocity adaptation\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3  # Update dynamic velocity scaling here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Modify inertia weight dynamically based on velocity adaptation strategy\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index):\n        if swarm_index == 0:\n            return  # Skip interaction for the first swarm\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component", "configspace": "", "generation": 36, "feedback": "The algorithm AdvancedDecentralizedSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06982 with standard deviation 0.00141.", "error": "", "parent_ids": ["989587fd-48f5-4969-aa59-55a0aaca5648"], "operator": null, "metadata": {"aucs": [0.07114024791271423, 0.06787169243918745, 0.07044058399975373]}}
{"id": "46604766-2d20-4ee5-836b-92710e1bd054", "fitness": 0.07066619172514425, "name": "EnhancedMultiSwarmCommunicator", "description": "Enhance swarm coordination by introducing adaptive multi-swarm communication mechanisms for improved convergence.", "code": "import numpy as np\n\nclass EnhancedMultiSwarmCommunicator:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.3  # Inter-swarm communication component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1  # New parameter for velocity adaptation\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3  # Update dynamic velocity scaling here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index, global_best)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Modify inertia weight dynamically based on velocity adaptation strategy\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return  # Skip interaction for the first swarm\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component\n        self.c4 = min(0.5, self.c4 * 1.05)  # Dynamic inter-swarm communication component", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedMultiSwarmCommunicator got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07067 with standard deviation 0.00145.", "error": "", "parent_ids": ["ae881340-65ba-426e-a8c4-e05c2d253381"], "operator": null, "metadata": {"aucs": [0.07263604357088804, 0.07017564319444647, 0.06918688841009824]}}
{"id": "53c2ed6f-741c-49b3-8e4c-3b414ac25af4", "fitness": 0.07066619172514425, "name": "EnhancedMultiSwarmCommunicator", "description": "Enhance global convergence by refining velocity adaptation with dynamic learning rates.", "code": "import numpy as np\n\nclass EnhancedMultiSwarmCommunicator:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.3  # Inter-swarm communication component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1  # New parameter for velocity adaptation\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3  # Update dynamic velocity scaling here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index, global_best)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Modify inertia weight dynamically based on velocity adaptation strategy\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return  # Skip interaction for the first swarm\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component\n        self.c4 = min(0.5, self.c4 * 1.05)  # Dynamic inter-swarm communication component", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedMultiSwarmCommunicator got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07067 with standard deviation 0.00145.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.07263604357088804, 0.07017564319444647, 0.06918688841009824]}}
{"id": "1bbcfde7-235b-4bc1-af21-0dc2c5462527", "fitness": 0.06924254348538565, "name": "EnhancedMultiSwarmCommunicator", "description": "Introduce velocity perturbation for enhanced exploration while maintaining the overall swarm behavior.", "code": "import numpy as np\n\nclass EnhancedMultiSwarmCommunicator:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.3  # Inter-swarm communication component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1  # New parameter for velocity adaptation\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3  # Update dynamic velocity scaling here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index, global_best)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3, r5 = np.random.rand(4)  # Added r5 for perturbation\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]) + \n                             0.1 * r5 * (ub - lb))  # Added velocity perturbation\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Modify inertia weight dynamically based on velocity adaptation strategy\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return  # Skip interaction for the first swarm\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component\n        self.c4 = min(0.5, self.c4 * 1.05)  # Dynamic inter-swarm communication component", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedMultiSwarmCommunicator got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06924 with standard deviation 0.00214.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.0718730512240181, 0.06922730098102448, 0.06662727825111436]}}
{"id": "4bbfcd7a-9cb6-4a33-901c-bcd1f6bcdbb3", "fitness": 0.06990582165120547, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Introduce a multi-phase adaptive strategy combining swarm size variation and enhanced velocity adaptation for improved global exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 3\n        self.particles_per_swarm = 15\n        self.w = 0.6\n        self.c1 = 1.5\n        self.c2 = 1.8\n        self.c3 = 0.4\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.8\n        self.w_decay = 0.98\n        self.v_max = None\n        self.velocity_adaptation_threshold = 0.05\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.2\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index, global_best)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_threshold:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(25, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.97)\n        self.c2 = min(2.1, self.c2 * 1.03)\n        self.c4 = min(0.6, self.c4 * 1.02)", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06991 with standard deviation 0.00501.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.07400238165024986, 0.06285354869508186, 0.0728615346082847]}}
{"id": "31560dc5-178a-441d-a708-704b4e1c57aa", "fitness": 0.0697309635710041, "name": "EnhancedMultiSwarmCommunicator", "description": "Introduce swarm diversity enhancement and memory-based velocity adaptation for improved convergence and exploration.", "code": "import numpy as np\n\nclass EnhancedMultiSwarmCommunicator:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.5\n        self.c4 = 0.3\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None\n        self.velocity_adaptation_rate = 0.1\n        self.diversity_threshold = 1.0  # New diversity threshold parameter\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index, global_best)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3, r5 = np.random.rand(4)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i])) \n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            if np.std(positions, axis=0).mean() < self.diversity_threshold:\n                velocities[i] += r5 * (np.mean(positions, axis=0) - positions[i])  # Diversity enhancement\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedMultiSwarmCommunicator got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06973 with standard deviation 0.00131.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.07158830488815626, 0.06877860098980637, 0.06882598483504965]}}
{"id": "64a94afc-34e8-4849-aec8-f221e35fb687", "fitness": 0.0681861140378126, "name": "EnhancedMultiSwarmCommunicator", "description": "Introduce adaptive velocity scaling and region-based selective communication to enhance convergence speed.", "code": "import numpy as np\n\nclass EnhancedMultiSwarmCommunicator:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.3  # Inter-swarm communication component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1  # New parameter for velocity adaptation\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3  # Update dynamic velocity scaling here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index, global_best)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Modify inertia weight dynamically based on velocity adaptation strategy\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return  # Skip interaction for the first swarm\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            if np.all(np.abs(current_swarm['positions'][i] - global_best) < 0.1):  # Selective interaction condition\n                r4 = np.random.rand()\n                interaction_factor = np.random.rand()\n                current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                                 + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component\n        self.c4 = min(0.5, self.c4 * 1.05)  # Dynamic inter-swarm communication component", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedMultiSwarmCommunicator got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06819 with standard deviation 0.00073.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.06746375989443376, 0.06790769380890582, 0.06918688841009824]}}
{"id": "4780a56a-8f99-4fde-bb72-00078198c657", "fitness": 0.07066619172514425, "name": "EnhancedMultiSwarmWithPheromone", "description": "Integrate adaptive pheromone-based communication and strategic swarm contraction to improve convergence and exploration balance in multi-swarm optimization.", "code": "import numpy as np\n\nclass EnhancedMultiSwarmWithPheromone:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.3  # Inter-swarm communication component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None\n        self.velocity_adaptation_rate = 0.1\n        self.pheromone_trail = np.zeros((self.num_swarms, self.dim))\n        self.pheromone_decay = 0.95\n        self.contraction_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._update_pheromone_trail(swarm_index, swarm_best)\n\n                self._inter_swarm_interaction(swarms, swarm_index, global_best)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _update_pheromone_trail(self, swarm_index, swarm_best):\n        self.pheromone_trail[swarm_index] *= self.pheromone_decay\n        self.pheromone_trail[swarm_index] += swarm_best\n\n    def _inter_swarm_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n        if np.linalg.norm(current_swarm['positions'] - previous_swarm['positions']) < self.contraction_threshold:\n            self._contract_swarm(current_swarm, global_best)\n\n    def _contract_swarm(self, swarm, global_best):\n        positions = swarm[\"positions\"]\n        for i in range(self.particles_per_swarm):\n            contraction_vector = global_best - positions[i]\n            positions[i] += contraction_vector * 0.5  # Contract towards the global best\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedMultiSwarmWithPheromone got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07067 with standard deviation 0.00145.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.07263604357088804, 0.07017564319444647, 0.06918688841009824]}}
{"id": "7e295d98-d14a-4354-86b8-fc82575bfcd2", "fitness": 0.0594082051173466, "name": "HierarchicalLeaderSwarmOptimizer", "description": "Introduce hierarchical swarm adaptation with leader-based dynamic strategy to enhance convergence precision.", "code": "import numpy as np\n\nclass HierarchicalLeaderSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.5\n        self.c4 = 0.3\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.995\n        self.v_max = None\n        self.leader_decay = 0.9\n        self.velocity_adaptation_rate = 0.1\n    \n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n        \n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            leaders = []\n            for swarm in swarms:\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                leaders.append((swarm_best, swarm_best_value))\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n\n            leaders.sort(key=lambda x: x[1])\n            self._hierarchical_leader_interaction(swarms, leaders, global_best)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n        \n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _hierarchical_leader_interaction(self, swarms, leaders, global_best):\n        leader_positions = [leader[0] for leader in leaders]\n        for swarm in swarms:\n            for i in range(self.particles_per_swarm):\n                r5 = np.random.rand()\n                leader_influence = sum(((l_pos - swarm['positions'][i]) for l_pos in leader_positions), np.zeros_like(swarm['positions'][i]))\n                swarm['positions'][i] += self.c4 * r5 * leader_influence * self.leader_decay\n                swarm['positions'][i] = np.clip(swarm['positions'][i], leaders[0][0], global_best)\n    \n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 44, "feedback": "The algorithm HierarchicalLeaderSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05941 with standard deviation 0.00254.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.06039503433698301, 0.06191124917132251, 0.05591833184373429]}}
{"id": "1ebeb740-48f2-4054-b86a-22030f60087d", "fitness": 0.07041980007163569, "name": "EnhancedMultiSwarmCommunicator", "description": "Introduce chaos-driven swarm reshuffling and neighborhood exploration to enhance convergence and diversity control.", "code": "import numpy as np\n\nclass EnhancedMultiSwarmCommunicator:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.3  # Inter-swarm communication component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1  # New parameter for velocity adaptation\n        self.chaos_factor = 0.5  # Chaos factor for reshuffling\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3  # Update dynamic velocity scaling here\n\n        # Initialize positions and velocities\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index, global_best)\n\n            # Adaptive parameter adjustment based on synergy and results\n            self._adaptive_adjustment(swarms, global_best_value)\n\n            # Chaos-driven reshuffling\n            if np.random.rand() < self.chaos_factor:\n                self._chaos_reshuffling(swarms, lb, ub)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)  # Local neighborhood best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))  # Added local component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Modify inertia weight dynamically based on velocity adaptation strategy\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return  # Skip interaction for the first swarm\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        # Dynamic adjustment based on swarm synergy and convergence\n        if global_best_value < 1e-6:  # Threshold for significant improvement\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        # Further synergy-based adaptation\n        self.c1 = max(1.0, self.c1 * 0.95)  # Dynamic cognitive component\n        self.c2 = min(2.0, self.c2 * 1.05)  # Dynamic social component\n        self.c4 = min(0.5, self.c4 * 1.05)  # Dynamic inter-swarm communication component\n\n    def _chaos_reshuffling(self, swarms, lb, ub):\n        for swarm in swarms:\n            reshuffle_indices = np.random.permutation(self.particles_per_swarm)\n            swarm[\"positions\"] = swarm[\"positions\"][reshuffle_indices]\n            swarm[\"velocities\"] = swarm[\"velocities\"][reshuffle_indices]\n            swarm[\"personal_bests\"] = swarm[\"personal_bests\"][reshuffle_indices]\n            swarm[\"personal_best_values\"] = swarm[\"personal_best_values\"][reshuffle_indices]", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedMultiSwarmCommunicator got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07042 with standard deviation 0.00138.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.07235151261991579, 0.06966512008364734, 0.06924276751134395]}}
{"id": "ff122e03-c352-4655-b057-57d2612c4d24", "fitness": 0.0594082051173466, "name": "HierarchicalMultiSwarmOptimizer", "description": "Introduce hierarchical swarm communication with adaptive learning rates to further enhance convergence and exploration.", "code": "import numpy as np\n\nclass HierarchicalMultiSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.3  # Inter-swarm communication component\n        self.c5 = 0.2  # Hierarchical communication component\n        self.func_evals = 0\n        self.min_inertia = 0.3\n        self.max_inertia = 0.9\n        self.w_decay = 0.99\n        self.v_max = None\n        self.velocity_adaptation_rate = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            hierarchy_best, hierarchy_best_value = self._evaluate_hierarchy(swarms, func)\n            if hierarchy_best_value < global_best_value:\n                global_best_value = hierarchy_best_value\n                global_best = hierarchy_best\n\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._inter_swarm_interaction(swarms, swarm_index, global_best)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _evaluate_hierarchy(self, swarms, func):\n        hierarchy_best = None\n        hierarchy_best_value = np.inf\n        for swarm in swarms:\n            for position in swarm[\"positions\"]:\n                if self.func_evals >= self.budget:\n                    break\n                value = func(position)\n                self.func_evals += 1\n                if value < hierarchy_best_value:\n                    hierarchy_best_value = value\n                    hierarchy_best = position\n        return hierarchy_best, hierarchy_best_value\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3, r5 = np.random.rand(4)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]) +\n                             self.c5 * r5 * (swarm_best - positions[i]))  # Hierarchical component\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w = min(self.max_inertia, self.w * (1 + self.w_decay))\n        else:\n            self.w = max(self.min_inertia, self.w * (1 - self.w_decay))\n\n    def _inter_swarm_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 46, "feedback": "The algorithm HierarchicalMultiSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05941 with standard deviation 0.00254.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.06039503433698301, 0.06191124917132251, 0.05591833184373429]}}
{"id": "4d611376-87e7-4e0e-905b-a0188456655e", "fitness": 0.0706674835093813, "name": "HierarchicalSwarmLearner", "description": "Introduce hierarchical swarm learning with dynamic neighborhood adaptation for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass HierarchicalSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.3  # Inter-swarm communication component\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 47, "feedback": "The algorithm HierarchicalSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07067 with standard deviation 0.00145.", "error": "", "parent_ids": ["46604766-2d20-4ee5-836b-92710e1bd054"], "operator": null, "metadata": {"aucs": [0.072640137529841, 0.0701754245882047, 0.06918688841009824]}}
{"id": "8ec5db7b-0db0-4462-9987-c4e975b8e2fb", "fitness": 0.07084148091400426, "name": "HierarchicalSwarmLearner", "description": "Enhance global exploration by adjusting the inter-swarm communication component.", "code": "import numpy as np\n\nclass HierarchicalSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.35  # Inter-swarm communication component (adjusted)\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + self.c4 * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 48, "feedback": "The algorithm HierarchicalSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07084 with standard deviation 0.00171.", "error": "", "parent_ids": ["4d611376-87e7-4e0e-905b-a0188456655e"], "operator": null, "metadata": {"aucs": [0.07319411994871583, 0.0701434343831987, 0.06918688841009824]}}
{"id": "bfe0c026-0d28-43df-ace8-c0099eec2153", "fitness": 0.07087582966946067, "name": "EnhancedHierarchicalSwarmLearner", "description": "Refine inter-swarm communication by dynamically adjusting the influence factors based on swarm performance diversity.", "code": "import numpy as np\n\nclass EnhancedHierarchicalSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.35  # Inter-swarm communication component\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedHierarchicalSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07088 with standard deviation 0.00177.", "error": "", "parent_ids": ["8ec5db7b-0db0-4462-9987-c4e975b8e2fb"], "operator": null, "metadata": {"aucs": [0.07332288707836743, 0.07011771351991636, 0.06918688841009824]}}
{"id": "557964f1-5102-41e6-a3ea-a4cc258ae08c", "fitness": 0.0700461112204122, "name": "EnhancedHierarchicalSwarmLearner", "description": "Introduce adaptive mutation and elite preservation mechanisms into the hierarchical swarm framework to enhance exploration and exploitative balance.", "code": "import numpy as np\n\nclass EnhancedHierarchicalSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.35  # Inter-swarm communication component\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1\n        self.mutation_rate = 0.02  # Adaptive mutation rate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        elite_position = None  # Track the best ever position for elite preservation\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    elite_position = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n        # Preserve the elite position found\n        if elite_position is not None:\n            return elite_position\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            # Apply adaptive mutation\n            if np.random.rand() < self.mutation_rate:\n                mutation = np.random.normal(0, self.v_max * 0.1, self.dim)\n                positions[i] += mutation\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedHierarchicalSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07005 with standard deviation 0.00184.", "error": "", "parent_ids": ["bfe0c026-0d28-43df-ace8-c0099eec2153"], "operator": null, "metadata": {"aucs": [0.07264210767921198, 0.06867024114697495, 0.06882598483504965]}}
{"id": "be540ee4-a1a0-4291-a4ae-36ca1015d999", "fitness": -Infinity, "name": "AdaptiveHybridSwarm", "description": "Introduce adaptive inertia weight and hybridized learning strategies with elite preservation to enhance convergence and solution diversity. ", "code": "import numpy as np\n\nclass AdaptiveHybridSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.9  # Initial inertia weight\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.35  # Inter-swarm communication component\n        self.elite_fraction = 0.2  # Fraction of elite particles\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            for swarm_index, swarm in enumerate(swarms):\n                self._evaluate_swarm(swarm, func)\n                swarm_best, swarm_best_value = self._get_swarm_best(swarm)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best)\n\n            self._adaptive_weight_adjustment()\n            self._preserve_elites(swarms)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n    def _get_swarm_best(self, swarm):\n        best_idx = np.argmin(swarm[\"personal_best_values\"])\n        return swarm[\"personal_bests\"][best_idx], swarm[\"personal_best_values\"][best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        interaction_strength = self.c4\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_weight_adjustment(self):\n        self.w = max(self.w_min, self.w * 0.99)\n\n    def _preserve_elites(self, swarms):\n        num_elites = max(1, int(self.elite_fraction * self.particles_per_swarm))\n        for swarm in swarms:\n            sorted_indices = np.argsort(swarm[\"personal_best_values\"])\n            elites = [swarm[\"personal_bests\"][i] for i in sorted_indices[:num_elites]]\n            for i in range(num_elites, self.particles_per_swarm):\n                if np.any(swarm[\"positions\"][i] in elites):\n                    continue\n                random_elite = elites[np.random.randint(0, len(elites))]\n                swarm[\"positions\"][i] = random_elite", "configspace": "", "generation": 51, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["bfe0c026-0d28-43df-ace8-c0099eec2153"], "operator": null, "metadata": {}}
{"id": "0e807809-0cb7-4b8f-9002-a2e2b1d17b86", "fitness": 0.07016130486241197, "name": "AdvancedAdaptiveSwarmLearner", "description": "Introduce adaptive inter-swarm learning rates and leverage a performance-based restart mechanism to enhance exploration-exploitation trade-offs.", "code": "import numpy as np\n\nclass AdvancedAdaptiveSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.35  # Inter-swarm communication component\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1\n        self.restart_threshold = 0.01\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._adaptive_hierarchical_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n            if global_best_value < self.restart_threshold:\n                swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n\n    def _adaptive_hierarchical_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        dynamic_learning_rate = 1.0 / (1.0 + np.exp(-performance_diversity))\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * dynamic_learning_rate\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 52, "feedback": "The algorithm AdvancedAdaptiveSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07016 with standard deviation 0.00185.", "error": "", "parent_ids": ["bfe0c026-0d28-43df-ace8-c0099eec2153"], "operator": null, "metadata": {"aucs": [0.07274985031423509, 0.0685471758629026, 0.06918688841009824]}}
{"id": "7571de68-ca26-4c26-9bee-88c49debd89b", "fitness": 0.04993904800908363, "name": "EnhancedHierarchicalSwarmLearner", "description": "Introduce non-linear velocity scaling to dynamically adjust particle velocities for convergence stability.", "code": "import numpy as np\n\nclass EnhancedHierarchicalSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.35  # Inter-swarm communication component\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i]**1.5, -self.v_max, self.v_max)  # Non-linear scaling\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedHierarchicalSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04994 with standard deviation 0.00257.", "error": "", "parent_ids": ["bfe0c026-0d28-43df-ace8-c0099eec2153"], "operator": null, "metadata": {"aucs": [0.0509523612616859, 0.05246010625805464, 0.046404676507510345]}}
{"id": "34ca4cb5-4101-4b1f-ac5b-aca5cc64dccf", "fitness": 0.07070384238297493, "name": "AdaptiveDynamicSwarmOptimizer", "description": "Improve swarm convergence speed by introducing adaptive inertia weight and enhanced swarm diversity management.", "code": "import numpy as np\n\nclass AdaptiveDynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w_max = 0.9  # Max inertia weight\n        self.w_min = 0.4  # Min inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.5  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.4  # Inter-swarm communication component\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n        w = self.w_max - (self.w_max - self.w_min) * (self.func_evals / self.budget)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(3, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n        \n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 54, "feedback": "The algorithm AdaptiveDynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07070 with standard deviation 0.00232.", "error": "", "parent_ids": ["bfe0c026-0d28-43df-ace8-c0099eec2153"], "operator": null, "metadata": {"aucs": [0.07397820003311883, 0.06928830558413324, 0.06884502153167271]}}
{"id": "523a3d47-c31c-402f-bf8e-bb7a438067dc", "fitness": 0.07087582966946067, "name": "AdaptiveCooperativeSwarmLearner", "description": "Introduce adaptive learning rates and cooperative learning among swarms using a reward-based mechanism to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass AdaptiveCooperativeSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.35  # Inter-swarm communication component\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1\n        self.reward_threshold = 1e-3\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._cooperative_learning(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adapt_learning_rates(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n\n    def _cooperative_learning(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand()\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adapt_learning_rates(self, swarms, global_best_value):\n        for swarm in swarms:\n            improvement = np.mean(swarm['personal_best_values']) - global_best_value\n            if improvement > self.reward_threshold:\n                self.c1 += self.learning_rate * improvement\n                self.c2 += self.learning_rate * improvement\n            else:\n                self.c1 = max(1.0, self.c1 * 0.95)\n                self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 55, "feedback": "The algorithm AdaptiveCooperativeSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07088 with standard deviation 0.00177.", "error": "", "parent_ids": ["bfe0c026-0d28-43df-ace8-c0099eec2153"], "operator": null, "metadata": {"aucs": [0.07332288707836743, 0.07011771351991636, 0.06918688841009824]}}
{"id": "17ac53db-96e9-4950-86ef-6d3a3d2f3fdf", "fitness": 0.0724461320218095, "name": "EnhancedHierarchicalSwarmLearner", "description": "Integrate adaptive neighborhood influence and energy conservation strategies to enhance convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedHierarchicalSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.6  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.35  # Inter-swarm communication component\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n        self.velocity_adaptation_rate = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity) # Changed\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01  # Modified\n        self.w = (self.w * 0.99) + (0.01 * np.random.rand())  # Modified\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedHierarchicalSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07245 with standard deviation 0.00237.", "error": "", "parent_ids": ["bfe0c026-0d28-43df-ace8-c0099eec2153"], "operator": null, "metadata": {"aucs": [0.07340534151791256, 0.07474616613741769, 0.06918688841009824]}}
{"id": "9c893b80-2653-43c8-b212-1ffd9ece2cec", "fitness": 0.060976281227879715, "name": "RefinedHierarchicalSwarmLearner", "description": "Introduce multi-level learning rates and adaptive mutation to boost exploration and exploitation balance in hierarchical swarm optimization.", "code": "import numpy as np\n\nclass RefinedHierarchicalSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 15\n        self.w = 0.6  # Inertia weight\n        self.c1 = 1.5  # Cognitive component\n        self.c2 = 1.7  # Social component\n        self.c3 = 0.5  # Local neighborhood component\n        self.c4 = 0.3  # Inter-swarm communication component\n        self.mutation_rate = 0.05\n        self.func_evals = 0\n        self.v_max = None  # Dynamic velocity clamping\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._adaptive_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n            # Adaptive mutation for exploration\n            if np.random.rand() < self.mutation_rate:\n                positions[i] += np.random.normal(0, 0.1, size=self.dim)\n\n    def _adaptive_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            alpha = self.c4 * (1 + np.tanh(performance_diversity))\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + alpha * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.2, self.c1 * 0.9)\n        self.c2 = min(2.2, self.c2 * 1.1)\n        self.c3 *= 1.01\n        self.w = (self.w * 0.98) + (0.02 * np.random.rand())\n        self.c4 = min(0.6, self.c4 * 1.05)", "configspace": "", "generation": 57, "feedback": "The algorithm RefinedHierarchicalSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06098 with standard deviation 0.00318.", "error": "", "parent_ids": ["17ac53db-96e9-4950-86ef-6d3a3d2f3fdf"], "operator": null, "metadata": {"aucs": [0.060252124725500344, 0.05749875949696337, 0.06517795946117544]}}
{"id": "cbc4322b-9a16-4085-928f-534bf11788a4", "fitness": 0.07328489668621396, "name": "EnhancedHierarchicalSwarmLearner", "description": "Introduce a multi-layer feedback mechanism and dynamic neighborhood adjustments to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHierarchicalSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6  # Adjusted to enhance local search\n        self.c4 = 0.4  # Adjusted for stronger inter-swarm influence\n        self.func_evals = 0\n        self.v_max = None\n        self.velocity_adaptation_rate = 0.1\n        self.feedback_factor = 0.2  # New: Feedback mechanism\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n        \n        # New: feedback adjustment\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.w = (self.w * 0.99) + (0.01 * np.random.rand())\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedHierarchicalSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07328 with standard deviation 0.00295.", "error": "", "parent_ids": ["17ac53db-96e9-4950-86ef-6d3a3d2f3fdf"], "operator": null, "metadata": {"aucs": [0.07204124194772321, 0.07734996201161337, 0.0704634860993053]}}
{"id": "6c53e2b7-56f5-409c-a09b-a4a0d44385f4", "fitness": 0.07328489668621396, "name": "AdaptiveFeedbackSwarmOptimizer", "description": "Introduce adaptive feedback-based influence and dynamic inertia weight adjustment to enhance convergence speed and accuracy.", "code": "import numpy as np\n\nclass AdaptiveFeedbackSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.velocity_adaptation_rate = 0.1\n        self.feedback_factor = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.velocity_adaptation_rate:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n        \n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.w = (self.w * 0.99) + (0.01 * np.random.rand())\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 59, "feedback": "The algorithm AdaptiveFeedbackSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07328 with standard deviation 0.00295.", "error": "", "parent_ids": ["cbc4322b-9a16-4085-928f-534bf11788a4"], "operator": null, "metadata": {"aucs": [0.07204124194772321, 0.07734996201161337, 0.0704634860993053]}}
{"id": "d8085601-6dd8-4e62-85d9-d912cda96ca5", "fitness": 0.07328489668621396, "name": "AdaptiveDualSwarmLearner", "description": "Introduce adaptive learning rates and dual-agent dynamics to optimize exploration and exploitation in the swarm.", "code": "import numpy as np\n\nclass AdaptiveDualSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.agents_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.learning_rate_adaptation = 0.1\n        self.feedback_factor = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._dual_agent_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.agents_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.agents_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.agents_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.agents_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        if np.mean(np.abs(velocities)) < self.learning_rate_adaptation:\n            self.w *= 1.05\n        else:\n            self.w *= 0.95\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _dual_agent_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.agents_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.agents_per_swarm = min(20, self.agents_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.agents_per_swarm = max(5, self.agents_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.w = (self.w * 0.99) + (0.01 * np.random.rand())\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 60, "feedback": "The algorithm AdaptiveDualSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07328 with standard deviation 0.00295.", "error": "", "parent_ids": ["cbc4322b-9a16-4085-928f-534bf11788a4"], "operator": null, "metadata": {"aucs": [0.07204124194772321, 0.07734996201161337, 0.0704634860993053]}}
{"id": "31f562f5-f006-476e-a117-4a3ce7269110", "fitness": 0.07063385189265625, "name": "EnhancedHierarchicalSwarmLearner", "description": "Introduce swarm diversity through random restarts and fitness-based adaptive inertia to bolster exploration and exploitation adaptability.", "code": "import numpy as np\n\nclass EnhancedHierarchicalSwarmLearner:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.velocity_adaptation_rate = 0.1\n        self.feedback_factor = 0.2\n        self.inertia_dynamics = (0.9, 0.4)  # Adaptive inertia range\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._hierarchical_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._dynamic_adjustment(swarms, global_best_value)\n\n            # Introduce random restart for diversity\n            if self.func_evals % (self.budget // 10) == 0:\n                random_swarm = np.random.choice(swarms)\n                random_swarm.update(self._initialize_swarm(lb, ub))\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        # Adaptive inertia based on performance\n        norm_velocity = np.linalg.norm(velocities)\n        if norm_velocity < self.velocity_adaptation_rate:\n            self.w = np.clip(self.w * 1.05, *self.inertia_dynamics)\n        else:\n            self.w = np.clip(self.w * 0.95, *self.inertia_dynamics)\n\n        # Feedback adjustment\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _hierarchical_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _dynamic_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.w = (self.w * 0.99) + (0.01 * np.random.rand())\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedHierarchicalSwarmLearner got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07063 with standard deviation 0.00115.", "error": "", "parent_ids": ["cbc4322b-9a16-4085-928f-534bf11788a4"], "operator": null, "metadata": {"aucs": [0.07211621175272132, 0.06932185782594213, 0.0704634860993053]}}
{"id": "a7223a0b-04d3-4ce1-bb31-ec94267d73fd", "fitness": 0.07328495909528539, "name": "AdaptiveSwarmOptimizer", "description": "Incorporate adaptive swarm intelligence with variance-based learning rates and multi-swarm collaboration to enhance optimization efficiency.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 62, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07328 with standard deviation 0.00295.", "error": "", "parent_ids": ["cbc4322b-9a16-4085-928f-534bf11788a4"], "operator": null, "metadata": {"aucs": [0.07204060227011544, 0.07735078891643543, 0.0704634860993053]}}
{"id": "64202377-5b9e-4474-a447-6da7e38908de", "fitness": 0.07169940479713229, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Enhance swarm optimization by integrating dynamic topology adaptation and neighborhood-based learning to improve convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        # New attributes for dynamic topology\n        self.topology = 'global'\n        self.topology_change_threshold = 0.15\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, swarm_performance)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        if self.topology == 'global':\n            global_influence = global_best\n        else:\n            # Use neighborhood best for local topology\n            local_best = np.mean(personal_bests, axis=0)\n            global_influence = local_best\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_influence - positions[i]) +\n                             self.c3 * r3 * (swarm_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, swarm_performance):\n        # Introduce dynamic topology change based on performance\n        if np.std(swarm_performance) < self.topology_change_threshold:\n            self.topology = 'local'\n        else:\n            self.topology = 'global'\n\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07170 with standard deviation 0.00341.", "error": "", "parent_ids": ["a7223a0b-04d3-4ce1-bb31-ec94267d73fd"], "operator": null, "metadata": {"aucs": [0.07109656350008564, 0.07614024193874347, 0.06786140895256776]}}
{"id": "ba81f0c7-b33a-4d6c-8a96-2ce76ce84825", "fitness": 0.06969393874499723, "name": "AdaptiveSwarmOptimizer", "description": "Refined Adaptive Swarm Optimizer with stochastic rank-based velocity adjustment and dynamic swarm size scaling for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.rank_scale = 1.2  # Added line for stochastic rank-based velocity scaling\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3, r5 = np.random.rand(4)  # Added r5 for rank-based adjustment\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n            \n            rank_influence = 1 + (np.log1p(i) * self.rank_scale * r5)  # Added line for velocity rank scaling\n            velocities[i] *= rank_influence  # Integrate rank_influence into velocity\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 64, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06969 with standard deviation 0.00235.", "error": "", "parent_ids": ["a7223a0b-04d3-4ce1-bb31-ec94267d73fd"], "operator": null, "metadata": {"aucs": [0.07052118057821699, 0.06649090572758964, 0.07206972992918503]}}
{"id": "a60649cd-0e84-4dcc-b9b5-733111712c67", "fitness": 0.071952371875685, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Enhance adaptive swarm optimization by integrating dynamic learning factors and cooperative inter-swarm leadership to improve convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.leader_influence = 0.3\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 += feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            leader_effect = self.leader_influence * np.random.rand() * (global_best - current_swarm['positions'][i])\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * leader_effect\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07195 with standard deviation 0.00299.", "error": "", "parent_ids": ["a7223a0b-04d3-4ce1-bb31-ec94267d73fd"], "operator": null, "metadata": {"aucs": [0.06926112265018791, 0.07612091254077424, 0.07047508043609285]}}
{"id": "ca7b9665-5552-403e-9880-df7840bbbc2a", "fitness": -Infinity, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Introduce dynamic feedback mechanisms and self-adaptive swarm sizing to enhance the convergence and exploration balance in the optimization process.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.adaptive_factor = 0.05\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n        self.c3 += self.adaptive_factor * (global_best_value < 1e-6)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)\n        self.adaptive_factor *= 1.02", "configspace": "", "generation": 66, "feedback": "An exception occurred: NameError(\"name 'global_best_value' is not defined\").", "error": "NameError(\"name 'global_best_value' is not defined\")", "parent_ids": ["a7223a0b-04d3-4ce1-bb31-ec94267d73fd"], "operator": null, "metadata": {}}
{"id": "3901a382-b558-4c76-8ed3-85c5904ce459", "fitness": -Infinity, "name": "HierarchicalSwarmOptimizer", "description": "Introduce a hierarchical topology with dynamic role assignments and exploratory-exploitative learning to enhance swarm adaptability and convergence speed.", "code": "import numpy as np\n\nclass HierarchicalSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.exploration_rate = 0.7\n        self.exploitation_rate = 0.3\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)\n\n        # Dynamic role assignment and exploration-exploitation balance\n        for swarm in swarms:\n            leader_role = np.random.choice([True, False], self.particles_per_swarm, p=[self.exploration_rate, self.exploitation_rate])\n            followers = np.invert(leader_role)\n            leader_positions = swarm['positions'][leader_role]\n            follower_positions = swarm['positions'][followers]\n\n            # Adjust exploration and exploitation rates\n            self.exploration_rate = 0.7 if global_best_value > 0.1 else 0.5\n            self.exploitation_rate = 0.3 if global_best_value > 0.1 else 0.5\n\n            if len(leader_positions) > 0:\n                leader_global = np.mean(leader_positions, axis=0)\n                swarm['positions'][leader_role] += np.random.normal(0, 0.1, leader_positions.shape) * (leader_global - leader_positions)\n\n            if len(follower_positions) > 0:\n                follower_swarm_best = np.mean(swarm['personal_bests'][followers], axis=0)\n                swarm['positions'][followers] += np.random.normal(0, 0.1, follower_positions.shape) * (follower_swarm_best - follower_positions)", "configspace": "", "generation": 67, "feedback": "An exception occurred: IndexError('boolean index did not match indexed array along dimension 0; dimension is 10 but corresponding boolean dimension is 9').", "error": "IndexError('boolean index did not match indexed array along dimension 0; dimension is 10 but corresponding boolean dimension is 9')", "parent_ids": ["a7223a0b-04d3-4ce1-bb31-ec94267d73fd"], "operator": null, "metadata": {}}
{"id": "9760f055-9863-40c7-b2b5-cc4e0d6641ea", "fitness": 0.07328495909528539, "name": "AdaptiveSwarmOptimizer", "description": "Introduce velocity control with periodic reinitialization and adaptive inertia to improve convergence in complex landscapes.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n            if self.func_evals % (self.budget // 10) == 0:\n                self._reinitialize_velocities(swarms)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)\n\n    def _reinitialize_velocities(self, swarms):\n        for swarm in swarms:\n            swarm['velocities'] = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))", "configspace": "", "generation": 68, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07328 with standard deviation 0.00295.", "error": "", "parent_ids": ["a7223a0b-04d3-4ce1-bb31-ec94267d73fd"], "operator": null, "metadata": {"aucs": [0.07204060227011544, 0.07735078891643543, 0.0704634860993053]}}
{"id": "caf33ac0-2b75-4d42-b488-08a39a26dbd6", "fitness": 0.07093596751344984, "name": "AdaptiveSwarmOptimizer", "description": "Enhance the swarm performance by improving velocity update equation with new random factor and global best influence.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3, r5 = np.random.rand(4)  # Added r5\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]) +\n                             self.c4 * r5 * (global_best - local_best))  # Changed line to add new influence\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 69, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07094 with standard deviation 0.00191.", "error": "", "parent_ids": ["a7223a0b-04d3-4ce1-bb31-ec94267d73fd"], "operator": null, "metadata": {"aucs": [0.07096966051538023, 0.07325935807315598, 0.06857888395181333]}}
{"id": "28fbe1bf-d43a-49f2-a953-83c55cd7ff2b", "fitness": 0.07328495909528539, "name": "AdaptiveSwarmOptimizer", "description": "Enhance swarm adaptability with dynamic leadership and stochastic feedback mechanisms.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + performance_diversity)\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.uniform(0, 1) * np.exp(-performance_diversity)  # Changed: np.random.rand() to np.random.uniform(0, 1)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)\n        self.c4 = max(0.2, self.c4 * np.random.uniform(0.9, 1.1))  # Changed: added stochastic adjustment to c4", "configspace": "", "generation": 70, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07328 with standard deviation 0.00295.", "error": "", "parent_ids": ["a7223a0b-04d3-4ce1-bb31-ec94267d73fd"], "operator": null, "metadata": {"aucs": [0.07204060227011544, 0.07735078891643543, 0.0704634860993053]}}
{"id": "4c88ef6b-fddd-4127-968a-92b86e45da30", "fitness": 0.07328499723647948, "name": "AdaptiveSwarmOptimizer", "description": "Enhance adaptive swarm optimization by tuning collaboration strength dynamically based on performance diversity.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity))  # Changed line\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 71, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07328 with standard deviation 0.00295.", "error": "", "parent_ids": ["a7223a0b-04d3-4ce1-bb31-ec94267d73fd"], "operator": null, "metadata": {"aucs": [0.07204042472190364, 0.0773510808882295, 0.0704634860993053]}}
{"id": "17d8170e-959d-4bae-b108-8af89a2f06bd", "fitness": 0.07328499723647948, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Improve swarm optimization by integrating dynamic feedback through inter-swarm learning and adaptive velocity scaling based on historical performance trends.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.historical_performance = []\n        self.alpha = 0.1  # Learning rate for historical performance impact\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n            self.historical_performance.append(global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = self._dynamic_feedback()\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _dynamic_feedback(self):\n        if len(self.historical_performance) < 2:\n            return np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        trend = np.mean(np.diff(self.historical_performance[-5:]))\n        feedback = self.alpha * trend\n        return np.clip(feedback, -self.feedback_factor, self.feedback_factor)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity))\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07328 with standard deviation 0.00295.", "error": "", "parent_ids": ["4c88ef6b-fddd-4127-968a-92b86e45da30"], "operator": null, "metadata": {"aucs": [0.07204042472190364, 0.0773510808882295, 0.0704634860993053]}}
{"id": "30b0e599-1809-4748-86f5-a8c1c0ed0373", "fitness": 0.06930100227519453, "name": "AdaptiveSwarmOptimizer", "description": "Enhance adaptive swarm optimization by integrating noise-resilient updates and improved local-global coalescence dynamics.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.noise_factor = 0.05  # Newly added line\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            noise = np.random.uniform(-self.noise_factor, self.noise_factor, self.dim)  # Newly added line\n            value = func(position + noise)  # Modified line\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity))\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.uniform(0.5, 1.5) * np.exp(-performance_diversity)  # Modified line\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 73, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06930 with standard deviation 0.00198.", "error": "", "parent_ids": ["4c88ef6b-fddd-4127-968a-92b86e45da30"], "operator": null, "metadata": {"aucs": [0.07113524535366222, 0.07022314898988047, 0.06654461248204091]}}
{"id": "957eb7c4-3476-4a8f-9275-f5f28a2abbf2", "fitness": 0.06678182461600239, "name": "AdaptiveSwarmOptimizer", "description": "Refine adaptive swarm optimization by dynamically scaling interaction strength based on velocity variance.", "code": "import numpy as np\n\nclass AdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n        velocity_variance = np.var(velocities)\n        self.w = 0.4 + 0.6 * np.exp(-velocity_variance)\n\n        feedback = np.random.uniform(-self.feedback_factor, self.feedback_factor)\n        self.c1 += feedback * 0.1\n        self.c2 -= feedback * 0.1\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * (1 + np.sqrt(np.var(current_swarm['velocities']))) # Changed line\n        \n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        self.c1 = max(1.0, self.c1 * 0.95)\n        self.c2 = min(2.0, self.c2 * 1.05)\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 74, "feedback": "The algorithm AdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06678 with standard deviation 0.00261.", "error": "", "parent_ids": ["4c88ef6b-fddd-4127-968a-92b86e45da30"], "operator": null, "metadata": {"aucs": [0.06467355239440953, 0.06520843535429233, 0.0704634860993053]}}
{"id": "1af6f017-ae4b-46b8-ab18-3b6cbc7cbc98", "fitness": 0.07338179575289087, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Introduce a diversity-enhancing mechanism by adapting swarm inertia and social coefficients based on performance stagnation and diversity metrics.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity))\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07338 with standard deviation 0.00217.", "error": "", "parent_ids": ["4c88ef6b-fddd-4127-968a-92b86e45da30"], "operator": null, "metadata": {"aucs": [0.07402086249936968, 0.07566103865999763, 0.0704634860993053]}}
{"id": "9bd59a20-5d34-4d12-a0bc-4625b6ba6452", "fitness": -Infinity, "name": "RefinedAdaptiveSwarmOptimizer", "description": "Enhance swarm dynamics by introducing multi-level feedback mechanisms and adaptive parameter scaling based on a dynamic performance landscape to improve convergence.", "code": "import numpy as np\n\nclass RefinedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 30\n        self.diversity_threshold = 0.1\n        self.performance_memory = []\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value)\n            self.performance_memory.append(global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity))\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value):\n        recent_performance = self.performance_memory[-min(10, len(self.performance_memory)):]\n        performance_trend = np.polyfit(range(len(recent_performance)), recent_performance, 1)[0]\n\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if performance_trend > 0 or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 76, "feedback": "An exception occurred: TypeError('expected non-empty vector for x').", "error": "TypeError('expected non-empty vector for x')", "parent_ids": ["1af6f017-ae4b-46b8-ab18-3b6cbc7cbc98"], "operator": null, "metadata": {}}
{"id": "09a7a5fc-d5b6-4928-8c77-ec0f779227e0", "fitness": 0.07330489143710002, "name": "EnhancedAdaptiveSwarmOptimizerV2", "description": "Implement a multi-layer learning mechanism by introducing an inter-swarm knowledge exchange layer and adaptive feedback loop for dynamically tuning inertia and social coefficients.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n            self._inter_swarm_knowledge_exchange(swarms, global_best)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity))\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)\n\n    def _inter_swarm_knowledge_exchange(self, swarms, global_best):\n        for swarm in swarms:\n            positions = swarm[\"positions\"]\n            for i in range(self.particles_per_swarm):\n                if np.random.rand() < self.feedback_factor:\n                    external_influence = np.random.rand() * (global_best - positions[i])\n                    positions[i] += external_influence", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07330 with standard deviation 0.00211.", "error": "", "parent_ids": ["1af6f017-ae4b-46b8-ab18-3b6cbc7cbc98"], "operator": null, "metadata": {"aucs": [0.07393011265543936, 0.0755210755565554, 0.0704634860993053]}}
{"id": "469b65c6-952e-4042-9d47-5fc0ab975c24", "fitness": 0.06933689077941059, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Introduce stochastic velocity control and dynamic swarm interactions to enhance exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            r5 = np.random.rand()  # New random factor for stochastic control\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]) +\n                             r5 * np.random.uniform(-1, 1, self.dim))  # Stochastic control\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity))\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n            if np.random.rand() < 0.1:  # Dynamic swarm interaction\n                neighbor_index = np.random.randint(0, self.particles_per_swarm)\n                current_swarm['positions'][i] = np.copy(previous_swarm['positions'][neighbor_index])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06934 with standard deviation 0.00337.", "error": "", "parent_ids": ["1af6f017-ae4b-46b8-ab18-3b6cbc7cbc98"], "operator": null, "metadata": {"aucs": [0.06802351922976801, 0.07396202061911838, 0.06602513248934538]}}
{"id": "c2034ee8-5875-4b1c-abe5-1fb8bd6533f2", "fitness": 0.07351666382252857, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Enhance global exploration by adjusting swarm interaction more dynamically based on diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1  # Slight change for line modification\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07352 with standard deviation 0.00222.", "error": "", "parent_ids": ["1af6f017-ae4b-46b8-ab18-3b6cbc7cbc98"], "operator": null, "metadata": {"aucs": [0.07440718354707376, 0.07567932182120662, 0.0704634860993053]}}
{"id": "3552d300-4942-4aa0-999f-430216863091", "fitness": 0.07351666382252857, "name": "DynamicFeedbackSwarmOptimizer", "description": "Introduce a dynamic feedback loop to adaptively tune swarm interaction and inertia coefficients based on performance trends and diversity measures.", "code": "import numpy as np\n\nclass DynamicFeedbackSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n        performance_trend = []\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            performance_trend.append(global_best_value)\n            trend_change = np.mean(performance_trend[-5:]) - np.mean(performance_trend[-10:-5])\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter, trend_change)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter, trend_change):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)\n\n        if trend_change < 0:\n            self.feedback_factor *= 1.1\n        else:\n            self.feedback_factor = max(0.1, self.feedback_factor * 0.9)", "configspace": "", "generation": 80, "feedback": "The algorithm DynamicFeedbackSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07352 with standard deviation 0.00222.", "error": "", "parent_ids": ["c2034ee8-5875-4b1c-abe5-1fb8bd6533f2"], "operator": null, "metadata": {"aucs": [0.07440718354707376, 0.07567932182120662, 0.0704634860993053]}}
{"id": "475f8ba4-b471-4438-9c8c-518630be7228", "fitness": 0.07351666382252857, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Introduce dynamic inertia weights and adaptive swarm interactions to enhance exploration and convergence balance based on real-time performance feedback.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07352 with standard deviation 0.00222.", "error": "", "parent_ids": ["c2034ee8-5875-4b1c-abe5-1fb8bd6533f2"], "operator": null, "metadata": {"aucs": [0.07440718354707376, 0.07567932182120662, 0.0704634860993053]}}
{"id": "e79f0151-00f8-47c1-8485-0a4bd84f45ce", "fitness": 0.07351666382252857, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Enhance swarm exploration by dynamically adjusting collaboration strength based on performance variance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1  # Slight change for line modification\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07352 with standard deviation 0.00222.", "error": "", "parent_ids": ["c2034ee8-5875-4b1c-abe5-1fb8bd6533f2"], "operator": null, "metadata": {"aucs": [0.07440718354707376, 0.07567932182120662, 0.0704634860993053]}}
{"id": "fc0445f9-0750-42fb-be1c-15d0258edd0f", "fitness": 0.07351666382252857, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Introduce adaptive inertia and local learning strategies to boost convergence speed and robustness in swarm optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n            # Add local learning rate adjustment\n            self.c3 = self.c3 * (1 + 0.05 * np.tanh(diversity))\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07352 with standard deviation 0.00222.", "error": "", "parent_ids": ["c2034ee8-5875-4b1c-abe5-1fb8bd6533f2"], "operator": null, "metadata": {"aucs": [0.07440718354707376, 0.07567932182120662, 0.0704634860993053]}}
{"id": "8052aef7-f672-486b-b079-3abd25d22bb5", "fitness": -Infinity, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Enhance swarm convergence by dynamically adjusting the interaction factor based on fitness improvement.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.5\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * (1.1 + np.tanh(global_best_value - previous_global_best_value))  # Line changed\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.1)\n            self.c2 = min(2.0, self.c2 * 0.9)\n        else:\n            self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.95)\n            self.c2 = min(2.0, self.c2 * 1.05)\n\n        self.c3 *= 1.01\n        self.c4 = min(0.5, self.c4 * 1.05)", "configspace": "", "generation": 84, "feedback": "An exception occurred: NameError(\"name 'global_best_value' is not defined\").", "error": "NameError(\"name 'global_best_value' is not defined\")", "parent_ids": ["c2034ee8-5875-4b1c-abe5-1fb8bd6533f2"], "operator": null, "metadata": {}}
{"id": "7040dbf6-eaa0-4a16-af6e-e70620bfac7e", "fitness": 0.0735300074934803, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Further enhance exploration and convergence by incorporating a dynamic inertia weight and adaptive learning rate based on performance feedback.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Initial inertia weight\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.2, 0.4, 0.9)  # More aggressive adjustment\n            self.c1 = max(1.0, self.c1 * 1.2)  # Increase personal influence\n            self.c2 = min(2.0, self.c2 * 0.8)  # Decrease global influence\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)  # Slightly increase the interaction strength", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07353 with standard deviation 0.00219.", "error": "", "parent_ids": ["c2034ee8-5875-4b1c-abe5-1fb8bd6533f2"], "operator": null, "metadata": {"aucs": [0.07440436583351884, 0.07566933339242832, 0.07051632325449375]}}
{"id": "dd2965ef-59c5-4a44-9ae7-13050e7b8a46", "fitness": 0.0735300074934803, "name": "EnhancedMultiLevelSwarmOptimizer", "description": "Introduce multi-level exploration by adapting swarm sizes and incorporating historical performance-based learning adjustments for enhanced convergence.", "code": "import numpy as np\n\nclass EnhancedMultiLevelSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Initial inertia weight\n        self.c1 = 1.5\n        self.c2 = 1.6\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        historical_bests = []\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance, historical_bests)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n            historical_bests.append(global_best_value)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance, historical_bests):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        historical_factor = np.std(historical_bests) / (np.mean(historical_bests) + 1e-9) if historical_bests else 1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity) * historical_factor\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.2, 0.4, 0.9)  # More aggressive adjustment\n            self.c1 = max(1.0, self.c1 * 1.2)  # Increase personal influence\n            self.c2 = min(2.0, self.c2 * 0.8)  # Decrease global influence\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)  # Slightly increase the interaction strength", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedMultiLevelSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07353 with standard deviation 0.00219.", "error": "", "parent_ids": ["7040dbf6-eaa0-4a16-af6e-e70620bfac7e"], "operator": null, "metadata": {"aucs": [0.07440436583351884, 0.07566933339242832, 0.07051632325449375]}}
{"id": "b45a05bb-0adb-4592-a767-e6e8576524b9", "fitness": 0.07361640461677366, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Slightly enhance the global influence and personal influence to potentially improve convergence rates.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Initial inertia weight\n        self.c1 = 1.5\n        self.c2 = 1.7  # Increased from 1.6 to enhance global influence\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.2, 0.4, 0.9)  # More aggressive adjustment\n            self.c1 = max(1.0, self.c1 * 1.2)  # Increase personal influence\n            self.c2 = min(2.0, self.c2 * 0.8)  # Decrease global influence\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)  # Slightly increase the interaction strength", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07362 with standard deviation 0.00235.", "error": "", "parent_ids": ["7040dbf6-eaa0-4a16-af6e-e70620bfac7e"], "operator": null, "metadata": {"aucs": [0.07436879514760508, 0.07604254613620942, 0.07043787256650647]}}
{"id": "132703ac-dd83-4168-80d5-fcbe9200dd98", "fitness": 0.07361640461677366, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Introduce a minor tweak in swarm updating to slightly enhance convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Initial inertia weight\n        self.c1 = 1.5\n        self.c2 = 1.7  # Increased from 1.6 to enhance global influence\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub + self.feedback_factor)  # Adjusted line\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.2, 0.4, 0.9)  # More aggressive adjustment\n            self.c1 = max(1.0, self.c1 * 1.2)  # Increase personal influence\n            self.c2 = min(2.0, self.c2 * 0.8)  # Decrease global influence\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)  # Slightly increase the interaction strength", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07362 with standard deviation 0.00235.", "error": "", "parent_ids": ["b45a05bb-0adb-4592-a767-e6e8576524b9"], "operator": null, "metadata": {"aucs": [0.07436879514760508, 0.07604254613620942, 0.07043787256650647]}}
{"id": "4cc5b329-8886-4fe5-b1cf-b30ce5d1631f", "fitness": 0.07360102220766385, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Fine-tune the global influence parameter for better balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Initial inertia weight\n        self.c1 = 1.5\n        self.c2 = 1.68  # Adjusted from 1.7 to improve overall balance\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.2, 0.4, 0.9)  # More aggressive adjustment\n            self.c1 = max(1.0, self.c1 * 1.2)  # Increase personal influence\n            self.c2 = min(2.0, self.c2 * 0.8)  # Decrease global influence\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)  # Slightly increase the interaction strength", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07360 with standard deviation 0.00232.", "error": "", "parent_ids": ["b45a05bb-0adb-4592-a767-e6e8576524b9"], "operator": null, "metadata": {"aucs": [0.07437661998332334, 0.0759715123632182, 0.07045493427645]}}
{"id": "cdef886c-384b-4c70-acbc-995a97741b68", "fitness": 0.07361640461677366, "name": "AdvancedDynamicSwarmOptimizer", "description": "Introduce a dynamic strategy for swarm strategy adaptation, incorporating historical performance trends to better balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdvancedDynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Initial inertia weight\n        self.c1 = 1.5\n        self.c2 = 1.7\n        self.c3 = 0.6\n        self.c4 = 0.4\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.1\n        self.performance_history = []\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self.performance_history.append(global_best_value)\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if len(self.performance_history) > 5:\n            trend = np.polyfit(range(len(self.performance_history)), self.performance_history, 1)[0]\n            self.w = np.clip(self.w * (1 + trend), 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * (1 + trend))\n            self.c2 = min(2.0, self.c2 * (1 - trend))\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)", "configspace": "", "generation": 90, "feedback": "The algorithm AdvancedDynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07362 with standard deviation 0.00235.", "error": "", "parent_ids": ["b45a05bb-0adb-4592-a767-e6e8576524b9"], "operator": null, "metadata": {"aucs": [0.07436879514760508, 0.07604254613620942, 0.07043787256650647]}}
{"id": "80db3b6f-b125-4c6f-ac5d-6d2bfc27c472", "fitness": 0.07373259148486518, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Refine inertia weight and interaction dynamics to enhance convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.9  # Adjusted inertia weight for dynamic adaptation\n        self.c1 = 1.6  # Adjusted to balance individual learning\n        self.c2 = 1.7\n        self.c3 = 0.6\n        self.c4 = 0.45  # Increase interaction strength for swarm collaboration\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.08  # Refined to trigger diversity strategies sooner\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.15, 0.4, 0.9)  # Refined adjustment factor\n            self.c1 = max(1.0, self.c1 * 1.2)\n            self.c2 = min(2.0, self.c2 * 0.8)\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07373 with standard deviation 0.00237.", "error": "", "parent_ids": ["b45a05bb-0adb-4592-a767-e6e8576524b9"], "operator": null, "metadata": {"aucs": [0.07464857881628972, 0.07606328096817994, 0.07048591467012588]}}
{"id": "ac8a1176-d9cc-4f40-9ee9-3e2597436162", "fitness": 0.07103277521369526, "name": "LayeredSwarmOptimizer", "description": "Introduce multi-layered learning strategies and trajectory analysis to enhance exploratory behavior and convergence precision.", "code": "import numpy as np\n\nclass LayeredSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.9\n        self.c1 = 1.6\n        self.c2 = 1.7\n        self.c3 = 0.6\n        self.c4 = 0.45\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.08\n        self.learning_layers = 3\n        self.layer_amplification = 0.2\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n        layer_strengths = [1 + self.layer_amplification * l for l in range(self.learning_layers)]\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            for layer in layer_strengths:\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best - positions[i]) * layer +\n                                 self.c3 * r3 * (local_best - positions[i]) * layer)\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.15, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.2)\n            self.c2 = min(2.0, self.c2 * 0.8)\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)", "configspace": "", "generation": 92, "feedback": "The algorithm LayeredSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07103 with standard deviation 0.00358.", "error": "", "parent_ids": ["80db3b6f-b125-4c6f-ac5d-6d2bfc27c472"], "operator": null, "metadata": {"aucs": [0.0734046448390907, 0.07372128078203177, 0.0659724000199633]}}
{"id": "971139a8-b264-478d-8c73-8b493314a732", "fitness": 0.07373259148486518, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Enhance interaction by slightly increasing the feedback factor to refine collaborative behavior.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.9\n        self.c1 = 1.6\n        self.c2 = 1.7\n        self.c3 = 0.6\n        self.c4 = 0.45\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.21  # Slightly increase feedback factor for enhanced collaboration\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.08\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.15, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.2)\n            self.c2 = min(2.0, self.c2 * 0.8)\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07373 with standard deviation 0.00237.", "error": "", "parent_ids": ["80db3b6f-b125-4c6f-ac5d-6d2bfc27c472"], "operator": null, "metadata": {"aucs": [0.07464857881628972, 0.07606328096817994, 0.07048591467012588]}}
{"id": "64fc45b0-69b0-4b24-8644-84dba3ef7d06", "fitness": 0.07373259148486518, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Introduce a sigmoid function to adapt inertia weight dynamically based on stagnation counter to optimize convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.9  # Adjusted inertia weight for dynamic adaptation\n        self.c1 = 1.6  # Adjusted to balance individual learning\n        self.c2 = 1.7\n        self.c3 = 0.6\n        self.c4 = 0.45  # Increase interaction strength for swarm collaboration\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.08  # Refined to trigger diversity strategies sooner\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        sigmoid_factor = 1 / (1 + np.exp(-(stagnation_counter - self.stagnation_threshold / 2)))  # New line for adaptive inertia weight\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * (1.15 * sigmoid_factor), 0.4, 0.9)  # Updated with sigmoid factor\n            self.c1 = max(1.0, self.c1 * 1.2)\n            self.c2 = min(2.0, self.c2 * 0.8)\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07373 with standard deviation 0.00237.", "error": "", "parent_ids": ["80db3b6f-b125-4c6f-ac5d-6d2bfc27c472"], "operator": null, "metadata": {"aucs": [0.07464857881628972, 0.07606328096817994, 0.07048591467012588]}}
{"id": "76fb8116-4511-4ffe-b532-62ecae7f386c", "fitness": 0.07174266548294957, "name": "SelfOrganizingSwarmOptimizer", "description": "Enhance swarm diversity and dynamic adaptability by implementing a self-organizing velocity and position update strategy.", "code": "import numpy as np\n\nclass SelfOrganizingSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w_min, self.w_max = 0.4, 0.9\n        self.w = self.w_max\n        self.c1, self.c2 = 1.5, 1.5\n        self.c3, self.c4 = 0.6, 0.45\n        self.func_evals = 0\n        self.v_max = None\n        self.adaptive_rate = 0.2\n        self.stagnation_threshold = 30\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.2\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3, r4 = np.random.rand(4)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]) +\n                             self.c4 * r4 * (np.random.uniform(lb, ub) - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 0.9\n\n        for i in range(self.particles_per_swarm):\n            r5 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r5 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.2, self.w_min, self.w_max)\n            self.c1 = max(1.0, self.c1 * 1.15)\n            self.c2 = min(2.0, self.c2 * 0.85)\n        else:\n            self.w = np.clip(self.w * 0.85, self.w_min, self.w_max)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.15)\n\n        self.c3 *= 1.02\n        self.c4 = min(0.6, self.c4 * 1.05)", "configspace": "", "generation": 95, "feedback": "The algorithm SelfOrganizingSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07174 with standard deviation 0.00599.", "error": "", "parent_ids": ["80db3b6f-b125-4c6f-ac5d-6d2bfc27c472"], "operator": null, "metadata": {"aucs": [0.07359382988435537, 0.07797890999369139, 0.06365525657080195]}}
{"id": "71b0786d-ff8d-407b-a32f-829e782b9c1d", "fitness": 0.07230755222817065, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Introduce adaptive strategy diversification with dynamic role adjustment and convergence acceleration for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.7  # Dynamic inertia weight for improved convergence\n        self.c1 = 1.5  # Balance personal and global experience\n        self.c2 = 1.8  # Increase global attraction for faster convergence\n        self.c3 = 0.55\n        self.c4 = 0.5  # Higher interaction strength for better collaboration\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.25\n        self.stagnation_threshold = 40  # Lower threshold for quicker response\n        self.diversity_threshold = 0.1  # Trigger diversity strategies earlier\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.4  # Increased velocity cap for exploration\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity))\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.1, 0.4, 0.9)  # Adjust inertia weight more dynamically\n            self.c1 = max(1.0, self.c1 * 1.15)\n            self.c2 = min(2.0, self.c2 * 0.85)\n        else:\n            self.w = np.clip(self.w * 0.85, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.02  # Slightly increase local influence\n        self.c4 = min(0.6, self.c4 * 1.05)  # Gradually adapt interaction strength", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07231 with standard deviation 0.00373.", "error": "", "parent_ids": ["80db3b6f-b125-4c6f-ac5d-6d2bfc27c472"], "operator": null, "metadata": {"aucs": [0.07314629320817956, 0.07639902027338563, 0.06737734320294675]}}
{"id": "3929f6ad-9763-4357-8813-a7c1570f9e9e", "fitness": 0.0690465603077619, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Integrate secondary learning factor to enhance swarm's cooperative learning.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.9  # Adjusted inertia weight for dynamic adaptation\n        self.c1 = 1.6  # Adjusted to balance individual learning\n        self.c2 = 1.7\n        self.c3 = 0.6\n        self.c4 = 0.45  # Increase interaction strength for swarm collaboration\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.08  # Refined to trigger diversity strategies sooner\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]) +\n                             self.c4 * np.random.rand() * (swarm_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.15, 0.4, 0.9)  # Refined adjustment factor\n            self.c1 = max(1.0, self.c1 * 1.2)\n            self.c2 = min(2.0, self.c2 * 0.8)\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06905 with standard deviation 0.00017.", "error": "", "parent_ids": ["80db3b6f-b125-4c6f-ac5d-6d2bfc27c472"], "operator": null, "metadata": {"aucs": [0.06913080140103556, 0.0691987767028105, 0.06881010281943967]}}
{"id": "af3fb8ea-c7fc-43df-87f6-b722f4ef8385", "fitness": 0.06566264791704686, "name": "PhaseAdaptiveSwarmOptimizer", "description": "Implemented multi-phase exploration and exploitation switching to enhance convergence in dynamic environments.", "code": "import numpy as np\n\nclass PhaseAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.9  # Inertia weight for dynamic adaptation\n        self.c1 = 1.6  # Personal learning coefficient\n        self.c2 = 1.7  # Global learning coefficient\n        self.c3 = 0.6  # Local swarm learning coefficient\n        self.c4 = 0.45  # Swarm collaboration interaction strength\n        self.func_evals = 0\n        self.v_max = None\n        self.stagnation_threshold = 50\n        self.exploration_phase_length = self.budget // 3\n        self.exploitation_phase_length = self.budget // 3\n        self.phase_switch_threshold = 0.05\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n        phase = 'exploration'\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n\n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub, phase)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n            phase = self._phase_management(global_best_value, previous_global_best_value, phase)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub, phase):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            if phase == 'exploration':\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                                 self.c3 * r3 * (local_best - positions[i]))\n            else:  # exploitation\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c2 * r2 * (global_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.1\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.phase_switch_threshold:\n            self.w = np.clip(self.w * 1.15, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 1.2)\n            self.c2 = min(2.0, self.c2 * 0.8)\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)\n\n    def _phase_management(self, global_best_value, previous_global_best_value, current_phase):\n        if self.func_evals % (self.exploration_phase_length + self.exploitation_phase_length) < self.exploration_phase_length:\n            return 'exploration'\n        else:\n            return 'exploitation'", "configspace": "", "generation": 98, "feedback": "The algorithm PhaseAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06566 with standard deviation 0.00281.", "error": "", "parent_ids": ["80db3b6f-b125-4c6f-ac5d-6d2bfc27c472"], "operator": null, "metadata": {"aucs": [0.0677063481506448, 0.06759412709817259, 0.061687468502323206]}}
{"id": "7656810c-6eae-4d7a-b566-4bec45b62140", "fitness": 0.07378105214334862, "name": "EnhancedAdaptiveSwarmOptimizer", "description": "Enhance collaboration by tweaking interaction dynamics for improved swarm efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = 5\n        self.particles_per_swarm = 10\n        self.w = 0.9  # Adjusted inertia weight for dynamic adaptation\n        self.c1 = 1.6  # Adjusted to balance individual learning\n        self.c2 = 1.7\n        self.c3 = 0.6\n        self.c4 = 0.45  # Increase interaction strength for swarm collaboration\n        self.func_evals = 0\n        self.v_max = None\n        self.feedback_factor = 0.2\n        self.stagnation_threshold = 50\n        self.diversity_threshold = 0.08  # Refined to trigger diversity strategies sooner\n\n    def __call__(self, func):\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        self.v_max = (ub - lb) * 0.3\n\n        swarms = [self._initialize_swarm(lb, ub) for _ in range(self.num_swarms)]\n        global_best = None\n        global_best_value = np.inf\n        stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            previous_global_best_value = global_best_value\n            swarm_performance = []\n            for swarm_index, swarm in enumerate(swarms):\n                swarm_best, swarm_best_value = self._evaluate_swarm(swarm, func)\n                swarm_performance.append(swarm_best_value)\n                \n                if swarm_best_value < global_best_value:\n                    global_best_value = swarm_best_value\n                    global_best = swarm_best\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n\n                self._update_swarm(swarm, swarm_best, global_best, lb, ub)\n                self._collaborative_interaction(swarms, swarm_index, global_best, swarm_performance)\n\n            self._adaptive_adjustment(swarms, global_best_value, previous_global_best_value, stagnation_counter)\n\n        return global_best\n\n    def _initialize_swarm(self, lb, ub):\n        positions = np.random.uniform(lb, ub, (self.particles_per_swarm, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.particles_per_swarm, self.dim))\n        personal_bests = np.copy(positions)\n        personal_best_values = np.full(self.particles_per_swarm, np.inf)\n        return {\"positions\": positions, \"velocities\": velocities, \n                \"personal_bests\": personal_bests, \"personal_best_values\": personal_best_values}\n\n    def _evaluate_swarm(self, swarm, func):\n        positions = swarm[\"positions\"]\n        personal_bests = swarm[\"personal_bests\"]\n        personal_best_values = swarm[\"personal_best_values\"]\n\n        for i, position in enumerate(positions):\n            if self.func_evals >= self.budget:\n                break\n            value = func(position)\n            self.func_evals += 1\n\n            if value < personal_best_values[i]:\n                personal_best_values[i] = value\n                personal_bests[i] = position\n\n        best_idx = np.argmin(personal_best_values)\n        return personal_bests[best_idx], personal_best_values[best_idx]\n\n    def _update_swarm(self, swarm, swarm_best, global_best, lb, ub):\n        positions = swarm[\"positions\"]\n        velocities = swarm[\"velocities\"]\n        personal_bests = swarm[\"personal_bests\"]\n\n        local_best = np.mean(personal_bests, axis=0)\n\n        for i in range(self.particles_per_swarm):\n            r1, r2, r3 = np.random.rand(3)\n            velocities[i] = (self.w * velocities[i] +\n                             self.c1 * r1 * (personal_bests[i] - positions[i]) +\n                             self.c2 * r2 * (global_best - positions[i]) +\n                             self.c3 * r3 * (local_best - positions[i]))\n            velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n            positions[i] += velocities[i]\n            positions[i] = np.clip(positions[i], lb, ub)\n\n    def _collaborative_interaction(self, swarms, swarm_index, global_best, swarm_performance):\n        if swarm_index == 0:\n            return\n        previous_swarm = swarms[swarm_index - 1]\n        current_swarm = swarms[swarm_index]\n        performance_diversity = np.std(swarm_performance) / (np.mean(swarm_performance) + 1e-9)\n        interaction_strength = self.c4 * (1 + np.tanh(performance_diversity)) * 1.15\n\n        for i in range(self.particles_per_swarm):\n            r4 = np.random.rand()\n            interaction_factor = np.random.rand() * np.exp(-performance_diversity)\n            current_swarm['positions'][i] += interaction_factor * (previous_swarm['positions'][i] - current_swarm['positions'][i]) \\\n                                             + interaction_strength * r4 * (global_best - current_swarm['positions'][i])\n\n    def _adaptive_adjustment(self, swarms, global_best_value, previous_global_best_value, stagnation_counter):\n        if global_best_value < 1e-6:\n            self.num_swarms = max(1, self.num_swarms - 1)\n            self.particles_per_swarm = min(20, self.particles_per_swarm + 1)\n        else:\n            self.num_swarms = min(10, self.num_swarms + 1)\n            self.particles_per_swarm = max(5, self.particles_per_swarm - 1)\n\n        diversity = np.mean([np.var(swarm[\"positions\"]) for swarm in swarms])\n\n        if stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n            self.w = np.clip(self.w * 1.15, 0.4, 0.9)  # Refined adjustment factor\n            self.c1 = max(1.0, self.c1 * 1.2)\n            self.c2 = min(2.0, self.c2 * 0.8)\n        else:\n            self.w = np.clip(self.w * 0.8, 0.4, 0.9)\n            self.c1 = max(1.0, self.c1 * 0.9)\n            self.c2 = min(2.0, self.c2 * 1.1)\n\n        self.c3 *= 1.05\n        self.c4 = min(0.6, self.c4 * 1.1)", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedAdaptiveSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07378 with standard deviation 0.00239.", "error": "", "parent_ids": ["80db3b6f-b125-4c6f-ac5d-6d2bfc27c472"], "operator": null, "metadata": {"aucs": [0.0747829578079029, 0.0760742839520171, 0.07048591467012588]}}
