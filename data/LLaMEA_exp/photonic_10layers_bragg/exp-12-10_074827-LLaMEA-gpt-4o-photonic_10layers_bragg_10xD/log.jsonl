{"id": "3193c402-5bd1-4ef9-b346-c3a3d093d813", "fitness": 0.06576797387263304, "name": "HybridPSOSA", "description": "A hybrid metaheuristic algorithm combining Particle Swarm Optimization (PSO) and Simulated Annealing (SA) to dynamically balance exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06577 with standard deviation 0.00451.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06281390784228436, 0.06235150345932727, 0.07213851031628749]}}
{"id": "6ae18a19-9dc5-4286-b73a-6c0016219fe2", "fitness": 0.06576656614425273, "name": "HybridPSOSA", "description": "Improved HybridPSOSA by slightly increasing the inertia weight for better exploration in initial stages of the search.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.75  # Increased for better exploration\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 1, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06577 with standard deviation 0.00450.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06279495362129739, 0.062372818790851325, 0.07213192602060947]}}
{"id": "5b9c48d5-943a-4acc-89c6-ac83aa5a0462", "fitness": -Infinity, "name": "EnhancedHybridPSO_DE_SA", "description": "An enhanced hybrid metaheuristic integrating Differential Evolution (DE) with Particle Swarm Optimization and Simulated Annealing for improved diversity and convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSO_DE_SA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.f = 0.8  # DE scaling factor\n        self.cr = 0.9  # DE crossover probability\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        def differential_evolution(i):\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = particles[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), bounds[:, 0], bounds[:, 1])\n            trial = np.where(np.random.rand(self.dim) < self.cr, mutant, particles[i])\n            trial_value = func(trial)\n            self.eval_count += 1\n            if trial_value < personal_best_values[i]:\n                personal_best_positions[i] = trial\n                personal_best_values[i] = trial_value\n                if trial_value < global_best_value:\n                    global_best_position = trial\n                    global_best_value = trial_value\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n                differential_evolution(i)\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 2, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'global_best_value' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'global_best_value' referenced before assignment\")", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {}}
{"id": "6ad3da80-4680-44ba-9d07-883963fab2e0", "fitness": 0.06576797387263304, "name": "HybridPSOSA", "description": "A hybrid metaheuristic algorithm combining PSO and SA with an increased personal influence for better local exploitation.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0  # Increased personal influence\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 3, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06577 with standard deviation 0.00451.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06281390784228436, 0.06235150345932727, 0.07213851031628749]}}
{"id": "d9cf4e04-cfc1-44a4-8c45-5208875440c0", "fitness": 0.06576152858967994, "name": "EnhancedHybridPSOSA", "description": "Enhanced HybridPSOSA integrates adaptive inertia weight and a mutation strategy to improve convergence and robustness in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.9\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, inertia_weight):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        def adaptive_inertia_weight():\n            return 0.4 + (0.5 * (self.budget - self.eval_count) / self.budget)\n\n        while self.eval_count < self.budget:\n            inertia_weight = adaptive_inertia_weight()\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, inertia_weight)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step with mutation\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06576 with standard deviation 0.00449.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06273730164942248, 0.0624358728660559, 0.07211141125356146]}}
{"id": "9d33c454-fa24-4e0e-b3a5-2e9b89496f4f", "fitness": 0.06576152858967994, "name": "EnhancedHybridPSOSA", "description": "Enhanced HybridPSOSA incorporating adaptive inertia weight and dynamically adjusted cooling to improve convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight_max = 0.9\n        self.inertia_weight_min = 0.4\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate_initial = 0.99\n        self.cooling_rate_final = 0.9\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n        \n        def update_particle_velocity(velocity, particle, personal_best, global_best, iteration_ratio):\n            inertia_weight = self.inertia_weight_max - (self.inertia_weight_max - self.inertia_weight_min) * iteration_ratio\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            iteration_ratio = self.eval_count / self.budget\n            \n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, iteration_ratio)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            cooling_rate = self.cooling_rate_initial - (self.cooling_rate_initial - self.cooling_rate_final) * iteration_ratio\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06576 with standard deviation 0.00449.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06273730164942248, 0.0624358728660559, 0.07211141125356146]}}
{"id": "68ed8793-233a-41a5-b69c-3d2dbbaaa09c", "fitness": 0.062112713107006355, "name": "EnhancedHybridPSOSADE", "description": "An enhanced hybrid algorithm combining PSO, SA, and Differential Evolution (DE) to improve convergence speed and solution quality in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.F = 0.8 # DE mutation factor\n        self.CR = 0.9 # DE crossover probability\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # DE mutation and crossover\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                x1, x2, x3 = particles[idxs]\n                mutant_vector = np.clip(x1 + self.F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n                crossover = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover, mutant_vector, particles[i])\n                trial_value = func(trial_vector)\n                self.eval_count += 1\n\n                if trial_value < personal_best_values[i]:\n                    personal_best_positions[i] = trial_vector\n                    personal_best_values[i] = trial_value\n\n                    if trial_value < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_value\n                \n                # Update velocities and positions\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedHybridPSOSADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06211 with standard deviation 0.00122.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06375035696739961, 0.06082761150177507, 0.061760170851844376]}}
{"id": "5cdf164c-4d1d-4b16-974c-cbbd772e2fea", "fitness": 0.06576152858967994, "name": "EnhancedHybridPSOSA", "description": "An enhanced hybrid algorithm combining improved Particle Swarm Optimization (PSO) with adaptive inertia weight and Simulated Annealing (SA) utilizing dynamic cooling for effective exploration and exploitation balance in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.9\n        self.final_inertia_weight = 0.4\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.dynamic_cooling_rate = 0.995\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, inertia_weight):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            inertia_weight = (self.final_inertia_weight +\n                              (self.inertia_weight - self.final_inertia_weight) *\n                              (self.budget - self.eval_count) / self.budget)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, inertia_weight)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step with dynamic cooling rate\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.dynamic_cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06576 with standard deviation 0.00449.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06273730164942248, 0.0624358728660559, 0.07211141125356146]}}
{"id": "3aa35e66-108a-405b-a078-c8b0a159cf86", "fitness": 0.06576152858967994, "name": "EnhancedHybridPSOSA", "description": "Enhanced HybridPSOSA integrates an adaptive inertia mechanism and elite particle archive for improved convergence and diversity in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.initial_inertia_weight = 0.9\n        self.final_inertia_weight = 0.4\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.elite_size = 5\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n        \n        elite_archive = particles[np.argsort(personal_best_values)[:self.elite_size]]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, inertia_weight):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            inertia_weight = (self.initial_inertia_weight - self.final_inertia_weight) * \\\n                             ((self.budget - self.eval_count) / self.budget) + self.final_inertia_weight\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, inertia_weight)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n            \n            # Elite Archive Update\n            candidate_elite = particles[np.argsort(personal_best_values)[:self.elite_size]]\n            elite_archive = np.vstack((elite_archive, candidate_elite))\n            elite_values = np.array([func(e) for e in elite_archive])\n            elite_archive = elite_archive[np.argsort(elite_values)[:self.elite_size]]\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06576 with standard deviation 0.00449.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06273730164942248, 0.0624358728660559, 0.07211141125356146]}}
{"id": "a21bc466-628d-4bd1-adfc-64919d7a5236", "fitness": 0.06576502268968554, "name": "HybridPSOSA", "description": "Minor adjustment in the inertia weight to improve global convergence behavior.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.8  # Changed from 0.7 to 0.8 to potentially enhance convergence\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 9, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06577 with standard deviation 0.00450.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06277586772307242, 0.06239398560397491, 0.07212521474200928]}}
{"id": "ed8aa75d-4566-4c85-aac5-2812560e3bbc", "fitness": 0.06308493719594173, "name": "EnhancedHybridPSOSA", "description": "An enhanced hybrid metaheuristic that combines Particle Swarm Optimization (PSO) and Simulated Annealing (SA) with dynamic parameter adaptation to improve exploration and exploitation balance in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.9\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 2.0\n        self.initial_temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n        temperature = self.initial_temperature\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, inertia_weight, cognitive_coefficient, social_coefficient):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = social_coefficient * r2 * (global_best - particle)\n            new_velocity = (inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, self.inertia_weight, self.cognitive_coefficient, self.social_coefficient)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step with adaptive temperature\n            if np.random.rand() < temperature:\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            temperature *= self.cooling_rate\n            self.inertia_weight = 0.4 + 0.5 * (self.eval_count / self.budget)  # Adaptive inertia weight\n            self.cognitive_coefficient = 2.5 - 1.5 * (self.eval_count / self.budget)  # Adaptive cognitive coefficient\n            self.social_coefficient = 1.5 + 1.0 * (self.eval_count / self.budget)  # Adaptive social coefficient\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06308 with standard deviation 0.00363.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.059231323451529594, 0.06206961652844745, 0.06795387160784816]}}
{"id": "63f13266-6812-4ebd-a8e3-5fb3085148ff", "fitness": 0.06576797387263304, "name": "EnhancedHybridPSOSA", "description": "A refined hybrid metaheuristic algorithm combining PSO, SA, and adaptive mutation to enhance convergence speed and solution quality in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.mutation_probability = 0.1\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            # Adaptive mutation step\n            if np.random.rand() < self.mutation_probability:\n                mutation = np.random.normal(0, 0.1, self.dim)\n                mutated_candidate = global_best_position + mutation\n                mutated_candidate = np.clip(mutated_candidate, bounds[:, 0], bounds[:, 1])\n                mutated_value = func(mutated_candidate)\n                self.eval_count += 1\n\n                if mutated_value < global_best_value:\n                    global_best_position = mutated_candidate\n                    global_best_value = mutated_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06577 with standard deviation 0.00451.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06281390784228436, 0.06235150345932727, 0.07213851031628749]}}
{"id": "2a96deb8-dcfc-4d72-a341-a419ee22b3fa", "fitness": 0.06576171618916533, "name": "HybridPSOSARefined", "description": "A refined hybrid PSO and SA algorithm integrating an adaptive inertia weight and dynamic cooling schedule to enhance exploration-exploitation balance in black box optimization.", "code": "import numpy as np\n\nclass HybridPSOSARefined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.9\n        self.min_inertia_weight = 0.4\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.initial_temperature = 1.0\n        self.temperature = self.initial_temperature\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        self.eval_count += len(particles)\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            self.inertia_weight = max(self.min_inertia_weight, self.inertia_weight - (0.5 / self.budget))\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-0.1, 0.1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n            if self.temperature < 0.01:\n                self.temperature = self.initial_temperature\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 12, "feedback": "The algorithm HybridPSOSARefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06576 with standard deviation 0.00449.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06273924242107709, 0.0624337926560673, 0.07211211349035163]}}
{"id": "20913c2a-49b0-449e-bf34-ae2a58d65f36", "fitness": 0.06576797387263304, "name": "HybridPSOSA", "description": "Enhanced HybridPSOSA to improve exploration by adding random particle reinitialization when stagnation is detected.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity)\n            return new_velocity\n\n        stagnation_counter = 0\n        while self.eval_count < self.budget:\n            previous_global_best_value = global_best_value\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n            if global_best_value == previous_global_best_value:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            \n            if stagnation_counter > 10:  # Change 1 line: random reinitialization for stagnation\n                particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n                stagnation_counter = 0\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 13, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06577 with standard deviation 0.00451.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06281390784228436, 0.06235150345932727, 0.07213851031628749]}}
{"id": "e27f7697-498a-4f8d-bd2d-bad3b172bb7b", "fitness": 0.06600842804757812, "name": "HybridPSOSA", "description": "A hybrid optimization algorithm enhancing exploration by incorporating a random velocity scaling factor in the PSO component.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.8, 1.2)  # Added line: random velocity scaling factor\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 14, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06601 with standard deviation 0.00441.", "error": "", "parent_ids": ["3193c402-5bd1-4ef9-b346-c3a3d093d813"], "operator": null, "metadata": {"aucs": [0.06051168731136203, 0.06620468595921947, 0.07130891087215285]}}
{"id": "fa1feaac-0eec-4b28-9d9e-80920d7c3020", "fitness": 0.06624853919631528, "name": "HybridPSOSA", "description": "A hybrid optimization algorithm with enhanced exploration by incorporating adaptive random velocity scaling in the PSO component.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.5, 1.5)  # Modified line: adaptive random velocity scaling factor\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 15, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06625 with standard deviation 0.00387.", "error": "", "parent_ids": ["e27f7697-498a-4f8d-bd2d-bad3b172bb7b"], "operator": null, "metadata": {"aucs": [0.062173118485015055, 0.06513068161991842, 0.07144181748401235]}}
{"id": "a241c5b7-bdcc-4aee-8d98-aee2a103a02e", "fitness": 0.06830045544105956, "name": "HybridPSOSA", "description": "Improved the exploration capability by increasing the range of adaptive random velocity scaling in the PSO component.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)  # Modified line: expanded adaptive random velocity scaling factor\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 16, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06830 with standard deviation 0.00275.", "error": "", "parent_ids": ["fa1feaac-0eec-4b28-9d9e-80920d7c3020"], "operator": null, "metadata": {"aucs": [0.0646116395842532, 0.06908348334718017, 0.0712062433917453]}}
{"id": "d9c46768-0d82-45ad-86cf-6f3a7c248ab4", "fitness": 0.0682804877712686, "name": "HybridPSOSA", "description": "Introduced an adaptive inertia weight to enhance convergence by adjusting the balance between exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            dynamic_inertia_weight = self.inertia_weight * (self.budget - self.eval_count) / self.budget  # Modified line: adaptive inertia weight\n            new_velocity = (dynamic_inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 17, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06828 with standard deviation 0.00275.", "error": "", "parent_ids": ["a241c5b7-bdcc-4aee-8d98-aee2a103a02e"], "operator": null, "metadata": {"aucs": [0.06458859195328004, 0.06905254952450901, 0.07120032183601677]}}
{"id": "ffdd3e55-5d31-483e-90ad-8254f654e5b2", "fitness": 0.06832478746289901, "name": "HybridPSOSA", "description": "Enhanced convergence by introducing a dynamic inertia weight that decreases over time to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)  # Modified line: expanded adaptive random velocity scaling factor\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Added dynamic inertia weight adjustment\n                self.inertia_weight = 0.9 - 0.8 * (self.eval_count / self.budget)  \n                \n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 18, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06832 with standard deviation 0.00276.", "error": "", "parent_ids": ["a241c5b7-bdcc-4aee-8d98-aee2a103a02e"], "operator": null, "metadata": {"aucs": [0.0646111880148229, 0.06912554878991528, 0.07123762558395885]}}
{"id": "d9f96d67-221a-4487-81a1-0cca34e0425e", "fitness": 0.0673768215921207, "name": "RefinedHybridPSOSA", "description": "Improved the balance between exploration and exploitation using adaptive strategies that dynamically adjust learning coefficients and inertia weight based on performance.", "code": "import numpy as np\n\nclass RefinedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.9\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 2.0\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.5, 1.5)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Adaptive inertia weight adjustment based on improvement\n                if self.eval_count > 0 and self.eval_count % 10 == 0:\n                    recent_improvement = np.min(personal_best_values) - global_best_value\n                    self.inertia_weight = max(0.4, min(0.9, self.inertia_weight + 0.1 * recent_improvement))\n\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step with adaptive perturbation\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 19, "feedback": "The algorithm RefinedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06738 with standard deviation 0.00654.", "error": "", "parent_ids": ["ffdd3e55-5d31-483e-90ad-8254f654e5b2"], "operator": null, "metadata": {"aucs": [0.06406037675955989, 0.06155884822763402, 0.07651123978916818]}}
{"id": "dfbbe5d6-b35d-45bd-ad23-cabc402defa1", "fitness": 0.06832478746289901, "name": "HybridPSOSA", "description": "Incorporate local search through random walks when particles stagnate to improve global exploration and local exploitation balance.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.stagnation_threshold = 10\n        self.stagnation_counter = np.zeros(self.population_size)\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                self.inertia_weight = 0.9 - 0.8 * (self.eval_count / self.budget)\n                \n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n                    self.stagnation_counter[i] = 0\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n                else:\n                    self.stagnation_counter[i] += 1\n\n                if self.stagnation_counter[i] > self.stagnation_threshold:\n                    # Perform a random walk for stagnated particles\n                    walk_step = np.random.uniform(-0.1, 0.1, self.dim)\n                    particles[i] += walk_step\n                    particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                    new_value = func(particles[i])\n                    self.eval_count += 1\n                    if new_value < personal_best_values[i]:\n                        personal_best_positions[i] = particles[i]\n                        personal_best_values[i] = new_value\n\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 20, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06832 with standard deviation 0.00276.", "error": "", "parent_ids": ["ffdd3e55-5d31-483e-90ad-8254f654e5b2"], "operator": null, "metadata": {"aucs": [0.0646111880148229, 0.06912554878991528, 0.07123762558395885]}}
{"id": "7693d3f7-71db-476e-9f59-2909c7025da9", "fitness": 0.05891047114616421, "name": "EnhancedHybridPSOSA", "description": "Introduce an adaptive learning rate for both cognitive and social components to enhance convergence and prevent premature stagnation.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, eval_progress):\n            r1, r2 = np.random.rand(2)\n            adaptive_cognitive = self.cognitive_coefficient * (1 - eval_progress)\n            adaptive_social = self.social_coefficient * eval_progress\n            cognitive_velocity = adaptive_cognitive * r1 * (personal_best - particle)\n            social_velocity = adaptive_social * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            eval_progress = self.eval_count / self.budget\n            for i in range(self.population_size):\n                self.inertia_weight = 0.9 - 0.8 * eval_progress\n\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, eval_progress)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05891 with standard deviation 0.00189.", "error": "", "parent_ids": ["ffdd3e55-5d31-483e-90ad-8254f654e5b2"], "operator": null, "metadata": {"aucs": [0.056302929683362035, 0.06073135462691004, 0.05969712912822056]}}
{"id": "fa91c898-90d7-4014-864e-ec52701af7aa", "fitness": 0.06073827093825598, "name": "HybridPSOSA_Adaptive", "description": "Introducing a hybrid adaptive learning rate for dynamic balancing of exploration and exploitation in PSO and SA, enhanced by non-linear cooling schedule in Simulated Annealing.", "code": "import numpy as np\n\nclass HybridPSOSA_Adaptive:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.learning_rate = 0.5\n        self.min_learning_rate = 0.01\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.temperature = 1.0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Dynamic inertia weight and adaptive learning rate\n                self.inertia_weight = 0.9 - 0.8 * (self.eval_count / self.budget)\n                self.learning_rate = self.min_learning_rate + (0.5 - self.min_learning_rate) * np.exp(-5.0 * (self.eval_count / self.budget))\n                \n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i] * self.learning_rate\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Enhanced Simulated Annealing step with non-linear cooling\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation * self.learning_rate\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / (self.temperature**2)):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 22, "feedback": "The algorithm HybridPSOSA_Adaptive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06074 with standard deviation 0.00315.", "error": "", "parent_ids": ["ffdd3e55-5d31-483e-90ad-8254f654e5b2"], "operator": null, "metadata": {"aucs": [0.056302929683362035, 0.06325685771983358, 0.06265502541157231]}}
{"id": "149920ac-2d57-4ae1-899a-7b7968ba7ae3", "fitness": 0.06832478746289901, "name": "HybridPSOSA", "description": "Introduced stochasticity in the cooling rate to enhance exploration in the HybridPSOSA algorithm.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)  # Modified line: expanded adaptive random velocity scaling factor\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Added dynamic inertia weight adjustment\n                self.inertia_weight = 0.9 - 0.8 * (self.eval_count / self.budget)  \n                \n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Simulated Annealing step\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate * np.random.uniform(0.95, 1.05)  # Introduced stochasticity in the cooling rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 23, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06832 with standard deviation 0.00276.", "error": "", "parent_ids": ["ffdd3e55-5d31-483e-90ad-8254f654e5b2"], "operator": null, "metadata": {"aucs": [0.0646111880148229, 0.06912554878991528, 0.07123762558395885]}}
{"id": "843b1849-a9ce-4bbd-bc39-ac3ad246bb57", "fitness": 0.06859829247926419, "name": "EnhancedHybridPSOSA", "description": "Introduce a dynamic social learning factor and enhanced exploration through adaptive velocity scaling and simulated annealing to improve convergence.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                self.inertia_weight = 0.9 - 0.8 * (self.eval_count / self.budget)  \n                \n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06860 with standard deviation 0.00693.", "error": "", "parent_ids": ["ffdd3e55-5d31-483e-90ad-8254f654e5b2"], "operator": null, "metadata": {"aucs": [0.059000752713981086, 0.07512663254617724, 0.07166749217763424]}}
{"id": "a96839e7-06bf-424b-bbe4-0cd1402b457d", "fitness": 0.06859829247926419, "name": "EnhancedHybridPSOSA", "description": "Introduce a convergence acceleration factor by dynamically adjusting the temperature cooling rate.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                self.inertia_weight = 0.9 - 0.8 * (self.eval_count / self.budget)  \n                \n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation = np.random.uniform(-1, 1, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate * (1 - self.eval_count / self.budget)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06860 with standard deviation 0.00693.", "error": "", "parent_ids": ["843b1849-a9ce-4bbd-bc39-ac3ad246bb57"], "operator": null, "metadata": {"aucs": [0.059000752713981086, 0.07512663254617724, 0.07166749217763424]}}
{"id": "1a11e10a-ad1b-47a3-8023-2f069a819b0d", "fitness": 0.06861839040061513, "name": "EnhancedHybridPSOSA", "description": "Integrate a self-adaptive inertia weight and use diversity-based perturbations to enhance exploration and exploitation balance for improved convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06862 with standard deviation 0.00691.", "error": "", "parent_ids": ["843b1849-a9ce-4bbd-bc39-ac3ad246bb57"], "operator": null, "metadata": {"aucs": [0.059054489318181025, 0.07513298466359142, 0.07166769722007293]}}
{"id": "5e794c24-3523-44ce-a25d-8ae0d1b9f4c4", "fitness": 0.06861839040061513, "name": "EnhancedHybridPSOSA", "description": "Enhance convergence by incorporating a dynamic particle neighborhood strategy and adaptive mutation based on evaluation progress for improved local search refinement.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.mutation_rate = 0.1\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        def mutate_particle(particle, best, scale):\n            mutation_vector = np.random.normal(0, scale, self.dim)\n            mutated_particle = particle + mutation_vector * np.random.rand() * self.mutation_rate\n            return np.clip(mutated_particle, bounds[:, 0], bounds[:, 1])\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                neighborhood_best_idx = np.argmin(personal_best_values)\n                neighborhood_best_position = personal_best_positions[neighborhood_best_idx]\n\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], neighborhood_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                candidate = mutate_particle(global_best_position, global_best_position, perturbation_strength)\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06862 with standard deviation 0.00691.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.059054489318181025, 0.07513298466359142, 0.07166769722007293]}}
{"id": "d8594835-2ff7-488c-84a7-6382a4b31aaa", "fitness": 0.06861839040061513, "name": "EnhancedHybridPSOGA", "description": "Adaptive Hybrid Particle Swarm Optimization with Gradient-Inspired Perturbations to Enhance Convergence and Exploitation Capabilities in Black Box Optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        def compute_gradient(func, position, epsilon=1e-8):\n            gradient = np.zeros(self.dim)\n            current_value = func(position)\n            for d in range(self.dim):\n                perturb = np.zeros(self.dim)\n                perturb[d] = epsilon\n                gradient[d] = (func(position + perturb) - current_value) / epsilon\n                self.eval_count += 1\n                if self.eval_count >= self.budget:\n                    break\n            return gradient\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                gradient = compute_gradient(func, global_best_position)\n                perturbation = gradient * perturbation_strength\n                candidate = global_best_position - perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedHybridPSOGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06862 with standard deviation 0.00691.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.059054489318181025, 0.07513298466359142, 0.07166769722007293]}}
{"id": "97fc0653-d8ad-47c5-848e-b9bebb4f6c88", "fitness": 0.06861839040061513, "name": "AdaptiveEnhancedHybridPSOSA", "description": "Introduce adaptive exploration-exploitation strategy by integrating a feedback loop to dynamically adjust coefficients based on convergence rate for improved black box optimization. ", "code": "import numpy as np\n\nclass AdaptiveEnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.epsilon = 1e-8\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        prev_global_best_value = global_best_value\n        convergence_rate = 0\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, convergence_rate):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, convergence_rate)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        prev_global_best_value = global_best_value\n                        global_best_value = current_value\n\n            convergence_rate = np.abs(global_best_value - prev_global_best_value) / (np.abs(global_best_value) + self.epsilon)\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 29, "feedback": "The algorithm AdaptiveEnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06862 with standard deviation 0.00691.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.059054489318181025, 0.07513298466359142, 0.07166769722007293]}}
{"id": "061654a2-a3ed-49ee-85c7-685d1307e6ca", "fitness": 0.06716212581042358, "name": "AdvancedHybridPSODE", "description": "Introduce an adaptive mutation strategy and hybridize with differential evolution to enhance exploration and exploitation capabilities dynamically.", "code": "import numpy as np\n\nclass AdvancedHybridPSODE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        def differential_evolution_mutation(target_idx):\n            indices = [idx for idx in range(self.population_size) if idx != target_idx]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant_vector = particles[a] + self.mutation_factor * (particles[b] - particles[c])\n            mutant_vector = np.clip(mutant_vector, bounds[:, 0], bounds[:, 1])\n            return mutant_vector\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n                # Apply differential evolution mutation\n                mutant_vector = differential_evolution_mutation(i)\n                trial_vector = np.where(np.random.rand(self.dim) < self.crossover_rate, mutant_vector, particles[i])\n                trial_value = func(trial_vector)\n                self.eval_count += 1\n\n                if trial_value < personal_best_values[i]:\n                    personal_best_positions[i] = trial_vector\n                    personal_best_values[i] = trial_value\n\n                    if trial_value < global_best_value:\n                        global_best_position = trial_vector\n                        global_best_value = trial_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 30, "feedback": "The algorithm AdvancedHybridPSODE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06716 with standard deviation 0.00787.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.056544913509154804, 0.06957425633441616, 0.07536720758769977]}}
{"id": "d60ee021-6758-4d58-b08a-d9f5d6a3dd84", "fitness": 0.06861839040061513, "name": "EnhancedHybridPSOSA", "description": "Introduce a dynamic velocity scaling factor based on convergence rate and an adaptive mutation strategy to further enhance exploration-exploitation balance for improved optimization performance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.velocity_scaling_factor = 1.0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7) * self.velocity_scaling_factor\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        previous_global_best_value = global_best_value\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n            \n            # Adjust velocity scaling factor based on convergence rate\n            if self.eval_count > 0 and previous_global_best_value > global_best_value:\n                improvement_rate = (previous_global_best_value - global_best_value) / previous_global_best_value\n                self.velocity_scaling_factor = 1 + improvement_rate\n            else:\n                self.velocity_scaling_factor = 1.0\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Adaptive mutation strategy\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                mutation_strength = perturbation_strength * (1 + np.random.uniform(-0.5, 0.5))\n                perturbation = np.random.normal(0, mutation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n            previous_global_best_value = global_best_value\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06862 with standard deviation 0.00691.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.059054489318181025, 0.07513298466359142, 0.07166769722007293]}}
{"id": "30ff3c1d-3653-4f28-9180-5b569bca0e19", "fitness": 0.06861839040061513, "name": "EnhancedHybridPSO_DE", "description": "Introduce a differential evolution strategy to complement PSO's exploration and balance between global and local search by dynamically adjusting mutation factors and crossover rates based on population diversity.", "code": "import numpy as np\n\nclass EnhancedHybridPSO_DE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n            self.mutation_factor = 0.5 + 0.5 * diversity\n            self.crossover_rate = 0.5 + 0.5 * (1 - diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Differential Evolution Mutation and Crossover\n            for i in range(self.population_size):\n                if np.random.rand() < self.crossover_rate:\n                    indices = np.random.choice(range(self.population_size), 3, replace=False)\n                    x1, x2, x3 = particles[indices]\n                    mutant_vector = x1 + self.mutation_factor * (x2 - x3)\n                    mutant_vector = np.clip(mutant_vector, bounds[:, 0], bounds[:, 1])\n                    if func(mutant_vector) < func(particles[i]):\n                        particles[i] = mutant_vector\n                        self.eval_count += 1\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedHybridPSO_DE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06862 with standard deviation 0.00691.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.059054489318181025, 0.07513298466359142, 0.07166769722007293]}}
{"id": "ca5d9acf-3ab2-4ad5-b3ec-e1165a6cab50", "fitness": 0.06861839040061513, "name": "EnhancedHybridPSOSA", "description": "Enhanced exploration by varying cognitive and social coefficients dynamically and introducing an adaptive perturbation mechanism for better convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_cognitive_coefficient = self.cognitive_coefficient * (1 + self.eval_count / (2 * self.budget))  # Change 1\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = dynamic_cognitive_coefficient * r1 * (personal_best - particle)  # Change 2\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength * np.sqrt(self.cooling_rate), self.dim)  # Change 3\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06862 with standard deviation 0.00691.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.059054489318181025, 0.07513298466359142, 0.07166769722007293]}}
{"id": "8090e834-4e9c-4c92-9277-76b945307f80", "fitness": 0.06861839040061513, "name": "EnhancedHybridPSOSA", "description": "Introduce adaptive personal learning coefficients and a refined perturbation strategy to enhance both convergence speed and exploration potential.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            dynamic_cognitive_coefficient = self.cognitive_coefficient + 0.5 * (self.eval_count / self.budget)\n            cognitive_velocity = dynamic_cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength * (1 - self.eval_count / self.budget), self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06862 with standard deviation 0.00691.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.059054489318181025, 0.07513298466359142, 0.07166769722007293]}}
{"id": "e04ba559-d7d6-4e7f-bfed-95cd1fe5caa3", "fitness": 0.06861839040061513, "name": "EnhancedHybridPSOSA", "description": "Enhance the balance between exploration and exploitation by incorporating differential evolution-inspired perturbations and adaptive inertia weight scaling based on convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            convergence_rate = np.abs(global_best_value - np.min(personal_best_values)) / (self.eval_count + 1)\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity) * (1 + convergence_rate)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                candidate_indices = np.random.choice(self.population_size, 3, replace=False)\n                perturbation = particles[candidate_indices[0]] + 0.5 * (particles[candidate_indices[1]] - particles[candidate_indices[2]])\n                candidate = global_best_position + perturbation_strength * perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06862 with standard deviation 0.00691.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.059054489318181025, 0.07513298466359142, 0.07166769722007293]}}
{"id": "9fbf2175-e1a2-45f3-ba70-d7ff09e90791", "fitness": 0.06119406043703598, "name": "EnhancedHybridPSOResampling", "description": "Introduce adaptive velocity clamping and particle resampling based on stagnation to improve convergence and robustness in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOResampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.stagnation_threshold = 5\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n        no_improvement_count = 0\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            random_scaling = np.random.uniform(0.3, 1.7)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * random_scaling\n            velocity_clamp = np.ptp(bounds, axis=1) * 0.1\n            return np.clip(new_velocity, -velocity_clamp, velocity_clamp)\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n                        no_improvement_count = 0\n                else:\n                    no_improvement_count += 1\n\n                if no_improvement_count >= self.stagnation_threshold:\n                    particles[i] = np.random.uniform(bounds[:, 0], bounds[:, 1], self.dim)\n                    no_improvement_count = 0\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value or np.random.rand() < np.exp((global_best_value - candidate_value) / self.temperature):\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedHybridPSOResampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06119 with standard deviation 0.00335.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.05805271331441997, 0.06584266424996921, 0.05968680374671875]}}
{"id": "2069b6b2-26a8-4ba5-8916-8d608313c3cf", "fitness": 0.0728686386547217, "name": "EnhancedHybridPSOSA", "description": "Integrate adaptive velocity scaling and stochastic neighborhood search to refine convergence and solution quality in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0  # Changed to enhance personal exploration\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Adjusted to slow down cooling for better exploration\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))  # New adaptive scaling\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                for _ in range(3):  # Perform multiple neighborhood searches\n                    perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                    perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                    candidate = global_best_position + perturbation\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    candidate_value = func(candidate)\n                    self.eval_count += 1\n\n                    if candidate_value < global_best_value:\n                        global_best_position = candidate\n                        global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07287 with standard deviation 0.00287.", "error": "", "parent_ids": ["1a11e10a-ad1b-47a3-8023-2f069a819b0d"], "operator": null, "metadata": {"aucs": [0.07006780904297083, 0.07681271868492312, 0.07172538823627117]}}
{"id": "9bbec74b-4cea-4f50-9ab9-8e6506d8b944", "fitness": 0.07271331650874906, "name": "EnhancedHybridPSOSA", "description": "Introduce adaptive mutation and diversified exploitation mechanisms in PSO to enhance exploration and convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def adaptive_mutation(particle):\n            mutation_strength = np.std(particles, axis=0) * (1 - self.eval_count / self.budget)\n            mutation = np.random.normal(0, mutation_strength, self.dim)\n            return np.clip(particle + mutation, bounds[:, 0], bounds[:, 1])\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n\n                # Apply adaptive mutation occasionally\n                if np.random.rand() < 0.1:\n                    particles[i] = adaptive_mutation(particles[i])\n\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                for _ in range(3):\n                    perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                    perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                    candidate = global_best_position + perturbation\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    candidate_value = func(candidate)\n                    self.eval_count += 1\n\n                    if candidate_value < global_best_value:\n                        global_best_position = candidate\n                        global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07271 with standard deviation 0.00600.", "error": "", "parent_ids": ["2069b6b2-26a8-4ba5-8916-8d608313c3cf"], "operator": null, "metadata": {"aucs": [0.06438255851249164, 0.07825909680954457, 0.07549829420421095]}}
{"id": "b529bde6-d70a-4a26-9446-138c4169c6f9", "fitness": 0.0728686386547217, "name": "AdvancedDynamicPSOSA", "description": "Introduce adaptive diversity control and self-adaptive parameter tuning for enhanced exploration-exploitation balance in black box optimization.", "code": "import numpy as np\n\nclass AdvancedDynamicPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 1.5\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            adaptive_cognitive = self.cognitive_coefficient * (1 + self.eval_count / self.budget)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = adaptive_cognitive * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if diversity < self.diversity_threshold:\n                perturb_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturb_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 39, "feedback": "The algorithm AdvancedDynamicPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07287 with standard deviation 0.00287.", "error": "", "parent_ids": ["2069b6b2-26a8-4ba5-8916-8d608313c3cf"], "operator": null, "metadata": {"aucs": [0.07006780904297083, 0.07681271868492312, 0.07172538823627117]}}
{"id": "c64bea52-22cc-4fbb-9a14-f5216502bc91", "fitness": 0.0728686386547217, "name": "EnhancedHybridPSOSA", "description": "Integrate adaptive inertia weight scheduling and multi-modal perturbations to enhance convergence robustness and solution diversity in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.eval_count = 0\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.neighborhood_search_count = 5  # Increased for better local exploration\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, inertia_weight):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, self.inertia_weight)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                for _ in range(self.neighborhood_search_count):\n                    perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                    perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                    candidate = global_best_position + perturbation\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    candidate_value = func(candidate)\n                    self.eval_count += 1\n\n                    if candidate_value < global_best_value:\n                        global_best_position = candidate\n                        global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07287 with standard deviation 0.00287.", "error": "", "parent_ids": ["2069b6b2-26a8-4ba5-8916-8d608313c3cf"], "operator": null, "metadata": {"aucs": [0.07006780904297083, 0.07681271868492312, 0.07172538823627117]}}
{"id": "e114070f-36f5-40ad-a691-339227f8e0d6", "fitness": 0.06902775629448443, "name": "EnhancedHybridPSOSA", "description": "Introduce dynamic inertia and adaptive neighborhood search based on fitness landscape to enhance exploration and exploitation balance in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        # Dynamically set inertia based on landscape ruggedness\n        self.dynamic_inertia = lambda diversity: self.inertia_max - diversity * (self.inertia_max - self.inertia_min)\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, inertia_weight):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            inertia_weight = self.dynamic_inertia(diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, inertia_weight)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                for _ in range(3):\n                    perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                    perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                    adaptive_perturbation = self.temperature * perturbation  # Adaptive perturbation based on cooling\n                    candidate = global_best_position + adaptive_perturbation\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    candidate_value = func(candidate)\n                    self.eval_count += 1\n\n                    if candidate_value < global_best_value:\n                        global_best_position = candidate\n                        global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06903 with standard deviation 0.00509.", "error": "", "parent_ids": ["2069b6b2-26a8-4ba5-8916-8d608313c3cf"], "operator": null, "metadata": {"aucs": [0.06794095779354636, 0.07573476731921203, 0.06340754377069491]}}
{"id": "12eaed79-30a1-4cb1-9b31-269cf2394033", "fitness": 0.0728686386547217, "name": "EnhancedHybridPSOSA", "description": "Introduce dynamic inertia adjustment and elitist perturbation strategies for improved convergence and solution exploration in black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.elitism_factor = 0.05  # New factor for elitist perturbation\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                elite_count = max(1, int(self.elitism_factor * self.population_size))\n                elite_indices = np.argsort(personal_best_values)[:elite_count]\n\n                for elite_index in elite_indices:\n                    perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                    perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                    candidate = personal_best_positions[elite_index] + perturbation\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    candidate_value = func(candidate)\n                    self.eval_count += 1\n\n                    if candidate_value < global_best_value:\n                        global_best_position = candidate\n                        global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07287 with standard deviation 0.00287.", "error": "", "parent_ids": ["2069b6b2-26a8-4ba5-8916-8d608313c3cf"], "operator": null, "metadata": {"aucs": [0.07006780904297083, 0.07681271868492312, 0.07172538823627117]}}
{"id": "abe6e4db-0b6d-4302-afd1-57ddf1fade0e", "fitness": 0.0728686386547217, "name": "EnhancedHybridAdaptivePSO", "description": "Incorporate adaptive inertia and diversity-aware perturbation strategies to balance exploration and exploitation for improved black box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.93  # Slightly adjusted for more gradual cooling\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength_base = np.std(particles - global_best_position, axis=0).mean()\n                diversity_adjustment = 1 + max(0, diversity - self.diversity_threshold)\n                perturbation_strength = perturbation_strength_base * diversity_adjustment\n\n                for _ in range(3):\n                    perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                    candidate = global_best_position + perturbation\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    candidate_value = func(candidate)\n                    self.eval_count += 1\n\n                    if candidate_value < global_best_value:\n                        global_best_position = candidate\n                        global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedHybridAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07287 with standard deviation 0.00287.", "error": "", "parent_ids": ["2069b6b2-26a8-4ba5-8916-8d608313c3cf"], "operator": null, "metadata": {"aucs": [0.07006780904297083, 0.07681271868492312, 0.07172538823627117]}}
{"id": "be45fdc6-d966-4264-b84c-eab56526c02c", "fitness": 0.07357027767467376, "name": "EnhancedHybridPSORBM", "description": "Hybridize Particle Swarm Optimization with a Resilience-Based Mutation Strategy to Enhance Robustness in Black Box Optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSORBM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.93  # Adjusted cooling rate slightly for exploration\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.1:  # Introduce random resilience-based mutation\n                    mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridPSORBM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07357 with standard deviation 0.00254.", "error": "", "parent_ids": ["2069b6b2-26a8-4ba5-8916-8d608313c3cf"], "operator": null, "metadata": {"aucs": [0.06998629556093205, 0.07509324416213348, 0.07563129330095575]}}
{"id": "ae01cdc8-73af-46db-80f9-35654107f523", "fitness": 0.058905481486323165, "name": "EnhancedHybridPSORBM", "description": "Integrate a dynamic crowding distance mechanism for improved diversity and a self-adaptive learning factor to further refine the EnhancedHybridPSORBM's exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSORBM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.93\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.learning_rate = 0.01  # Added learning rate for self-adaption\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, crowding_distance):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale * crowding_distance\n            return new_velocity\n\n        def calculate_crowding_distance(particles):\n            sorted_indices = np.argsort([func(p) for p in particles])\n            crowding_distance = np.zeros(self.population_size)\n            for dim in range(self.dim):\n                sorted_particles = particles[sorted_indices, dim]\n                dim_min = np.min(sorted_particles)\n                dim_max = np.max(sorted_particles)\n                if dim_max - dim_min == 0:\n                    continue\n                crowding_distance[sorted_indices[0]] = np.inf\n                crowding_distance[sorted_indices[-1]] = np.inf\n                for i in range(1, self.population_size - 1):\n                    crowding_distance[sorted_indices[i]] += (\n                        (sorted_particles[i + 1] - sorted_particles[i - 1]) / (dim_max - dim_min)\n                    )\n            return crowding_distance\n\n        while self.eval_count < self.budget:\n            crowding_distance = calculate_crowding_distance(particles)\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(\n                    velocities[i], particles[i], personal_best_positions[i], global_best_position, crowding_distance[i])\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.1:\n                    mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.social_coefficient += self.learning_rate * (1 - 2 * np.random.rand())  # Self-adapt social coefficient\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedHybridPSORBM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05891 with standard deviation 0.00189.", "error": "", "parent_ids": ["be45fdc6-d966-4264-b84c-eab56526c02c"], "operator": null, "metadata": {"aucs": [0.056302929683362035, 0.06073135462691004, 0.05968216014869743]}}
{"id": "06056da1-dc32-43cd-b021-308f15e6f4f6", "fitness": 0.0683445892907643, "name": "EnhancedHybridPSOADE", "description": "Integrate Adaptive Differential Evolution into Enhanced Hybrid PSO to Increase Exploration while Maintaining Convergence Speed in Black Box Optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOADE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.93\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.F = 0.5  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def differential_evolution(p):\n            idxs = [idx for idx in range(self.population_size) if idx != p]\n            a, b, c = particles[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), bounds[:, 0], bounds[:, 1])\n            cross_points = np.random.rand(self.dim) < self.CR\n            trial = np.where(cross_points, mutant, particles[p])\n            trial_value = func(trial)\n            return trial, trial_value\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                if np.random.rand() < 0.3:  # Introduce a probability to apply DE\n                    trial, trial_value = differential_evolution(i)\n                    if trial_value < personal_best_values[i]:\n                        personal_best_positions[i] = trial\n                        personal_best_values[i] = trial_value\n\n                        if trial_value < global_best_value:\n                            global_best_position = trial\n                            global_best_value = trial_value\n                else:\n                    velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                    particles[i] += velocities[i]\n\n                    if np.random.rand() < 0.1:\n                        mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n                        mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                        particles[i] += mutation\n\n                    particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                    current_value = func(particles[i])\n                    self.eval_count += 1\n\n                    if current_value < personal_best_values[i]:\n                        personal_best_positions[i] = particles[i]\n                        personal_best_values[i] = current_value\n\n                        if current_value < global_best_value:\n                            global_best_position = particles[i]\n                            global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedHybridPSOADE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06834 with standard deviation 0.00689.", "error": "", "parent_ids": ["be45fdc6-d966-4264-b84c-eab56526c02c"], "operator": null, "metadata": {"aucs": [0.05868681439095713, 0.07430358274324067, 0.07204337073809508]}}
{"id": "a9c262d3-6738-40fa-998b-93235a79497c", "fitness": 0.07357027767467376, "name": "EnhancedHybridPSORBMWithANS", "description": "Integrate Adaptive Neighborhood Search into Enhanced Hybrid PSO with Resilience-Based Mutation for Dynamic Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSORBMWithANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.93  # Adjusted cooling rate slightly for exploration\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.neighborhood_factor = 0.2  # Factor to determine neighborhood size\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.1:  # Introduce random resilience-based mutation\n                    mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            # Adaptive Neighborhood Search\n            for i in range(self.population_size):\n                if np.random.rand() < self.neighborhood_factor:\n                    neighborhood_indices = np.random.choice(self.population_size, size=int(self.neighborhood_factor * self.population_size), replace=False)\n                    neighborhood_best_idx = neighborhood_indices[np.argmin(personal_best_values[neighborhood_indices])]\n                    neighborhood_best_position = personal_best_positions[neighborhood_best_idx]\n                    neighborhood_velocity = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], neighborhood_best_position)\n                    particles[i] += neighborhood_velocity\n                    particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                    neighborhood_value = func(particles[i])\n                    self.eval_count += 1\n                    if neighborhood_value < personal_best_values[i]:\n                        personal_best_positions[i] = particles[i]\n                        personal_best_values[i] = neighborhood_value\n                        if neighborhood_value < global_best_value:\n                            global_best_position = particles[i]\n                            global_best_value = neighborhood_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(particles - global_best_position, axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedHybridPSORBMWithANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07357 with standard deviation 0.00254.", "error": "", "parent_ids": ["be45fdc6-d966-4264-b84c-eab56526c02c"], "operator": null, "metadata": {"aucs": [0.06998629556093205, 0.07509324416213348, 0.07563129330095575]}}
{"id": "f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98", "fitness": 0.07397515331103392, "name": "EnhancedHybridPSOANS", "description": "Integrate Adaptive Neighborhood Search and Dynamic Mutation Strategy into Particle Swarm Optimization to Enhance Exploration and Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2  # Updated for better exploration\n        self.social_coefficient = 1.7  # Updated for better exploitation\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Adjusted cooling rate further for stability\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:  # Increased probability of dynamic mutation\n                    mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Increased mutation strength\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()  # Absolute value for better spread\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07398 with standard deviation 0.00501.", "error": "", "parent_ids": ["be45fdc6-d966-4264-b84c-eab56526c02c"], "operator": null, "metadata": {"aucs": [0.0669717352560204, 0.07843663214662311, 0.07651709253045824]}}
{"id": "6e466c05-5aab-4b7c-885c-4c38c372c755", "fitness": 0.07397515331103392, "name": "EnhancedHybridPSOANS_v2", "description": "Optimize exploration and exploitation balance by dynamically adapting particle behaviors based on swarm diversity and introducing a novel perturbation mechanism for global guidance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS_v2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.diversity_threshold = 0.05\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                \n                if np.random.rand() < 0.15:\n                    mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0])\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if diversity < self.diversity_threshold and np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedHybridPSOANS_v2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07398 with standard deviation 0.00501.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.0669717352560204, 0.07843663214662311, 0.07651709253045824]}}
{"id": "ffdd9167-38ac-4f23-bd0d-3202f94f013a", "fitness": 0.06993468315160427, "name": "EnhancedHybridPSOANS", "description": "Introduce Adaptive Cooling and Dynamic Mutation Rate for Enhanced Exploration and Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.2:  # Adjusted probability of dynamic mutation\n                    mutation_strength = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Increased mutation strength\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= (self.cooling_rate - 0.01)  # Adaptive cooling\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06993 with standard deviation 0.00601.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.06559729007720816, 0.07843663214662311, 0.06577012723098152]}}
{"id": "0db9642a-0184-49b0-b160-0896f7efb062", "fitness": 0.061123207192430994, "name": "EnhancedPSODNA", "description": "Hybridize Particle Swarm Optimization with Dynamic Neighborhood Adaptation and Context-Aware Mutation for Enhanced Convergence.", "code": "import numpy as np\n\nclass EnhancedPSODNA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                local_neighbors = np.random.choice(self.population_size, 5, replace=False)\n                local_best_idx = local_neighbors[np.argmin(personal_best_values[local_neighbors])]\n                local_best_position = personal_best_positions[local_best_idx]\n\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], local_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0])\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedPSODNA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06112 with standard deviation 0.00164.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.05897040679170573, 0.06145928514660559, 0.06293992963898165]}}
{"id": "fab8fa3e-0db6-4f2c-9b3a-138aa2518822", "fitness": 0.07335794995834637, "name": "EnhancedHybridPSOANS", "description": "Enhance the existing strategy by dynamically adjusting the mutation probability to improve exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2  # Updated for better exploration\n        self.social_coefficient = 1.7  # Updated for better exploitation\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Adjusted cooling rate further for stability\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < (0.15 + 0.05 * np.sin(self.eval_count / self.budget * np.pi)):  # Dynamically adjust mutation probability\n                    mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Increased mutation strength\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()  # Absolute value for better spread\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 52, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07336 with standard deviation 0.00477.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.0669717352560204, 0.07843663214662311, 0.07466548247239557]}}
{"id": "f7be47d9-ccb7-437f-9981-2ba7f769f859", "fitness": 0.07267337815331913, "name": "EnhancedQuantumHybridPSO", "description": "Introduce Adaptive Quantum-Inspired Mutation and Multi-Swarm Strategy to Enhance Diversity and Convergence in Particle Swarm Optimization.", "code": "import numpy as np\n\nclass EnhancedQuantumHybridPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.temperature = 1.0\n        self.cooling_rate = 0.9\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.quantum_exponent = 0.01\n        self.num_swarms = 3\n        self.swarm_pop = self.population_size // self.num_swarms\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        swarms = [np.random.uniform(bounds[:, 0], bounds[:, 1], (self.swarm_pop, self.dim)) for _ in range(self.num_swarms)]\n        velocities = [np.random.uniform(-1, 1, (self.swarm_pop, self.dim)) for _ in range(self.num_swarms)]\n        personal_best_positions = [np.copy(swarm) for swarm in swarms]\n        personal_best_values = [np.array([func(p) for p in swarm]) for swarm in swarms]\n\n        global_best_position = None\n        global_best_value = float('inf')\n\n        for swarm_idx in range(self.num_swarms):\n            global_best_idx = np.argmin(personal_best_values[swarm_idx])\n            candidate_best_position = personal_best_positions[swarm_idx][global_best_idx, :]\n            candidate_best_value = personal_best_values[swarm_idx][global_best_idx]\n\n            if candidate_best_value < global_best_value:\n                global_best_position = candidate_best_position\n                global_best_value = candidate_best_value\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for swarm_idx in range(self.num_swarms):\n                swarm = swarms[swarm_idx]\n                velocity = velocities[swarm_idx]\n                personal_best_pos = personal_best_positions[swarm_idx]\n                personal_best_vals = personal_best_values[swarm_idx]\n\n                for i in range(self.swarm_pop):\n                    velocity[i] = update_particle_velocity(velocity[i], swarm[i], personal_best_pos[i], global_best_position)\n                    swarm[i] += velocity[i]\n\n                    if np.random.rand() < 0.1:\n                        mutation_strength = self.quantum_exponent * (np.abs(bounds[:, 1] - bounds[:, 0]))\n                        quantum_mutation = np.random.normal(0, mutation_strength, self.dim)\n                        swarm[i] += quantum_mutation * np.random.choice([-1, 1], self.dim)\n\n                    swarm[i] = np.clip(swarm[i], bounds[:, 0], bounds[:, 1])\n                    current_value = func(swarm[i])\n                    self.eval_count += 1\n\n                    if current_value < personal_best_vals[i]:\n                        personal_best_pos[i] = swarm[i]\n                        personal_best_vals[i] = current_value\n\n                        if current_value < global_best_value:\n                            global_best_position = swarm[i]\n                            global_best_value = current_value\n\n                if np.random.rand() < self.temperature:\n                    perturbation_strength = np.std(np.abs(swarm - global_best_position), axis=0).mean()\n                    perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                    candidate = global_best_position + perturbation\n                    candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                    candidate_value = func(candidate)\n                    self.eval_count += 1\n\n                    if candidate_value < global_best_value:\n                        global_best_position = candidate\n                        global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedQuantumHybridPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07267 with standard deviation 0.00168.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.07056653416962233, 0.07277686583877752, 0.07467673445155754]}}
{"id": "f829159b-8af4-4d2d-b7f8-05aee142b809", "fitness": 0.06561145493614313, "name": "EnhancedHierarchicalPSOLearning", "description": "Introduce Hierarchical Learning and Adaptive Exploration in Particle Swarm Optimization to Enhance Convergence for Diverse Black Box Problems.", "code": "import numpy as np\n\nclass EnhancedHierarchicalPSOLearning:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.9\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.cognitive_coefficient = 2.0\n        self.social_coefficient = 1.5\n        self.learning_step = 0.1\n        self.adaptive_exploration_rate = 0.25\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = self.inertia_weight * velocity + cognitive_velocity + social_velocity\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                exploration_factor = np.exp(-self.eval_count / (0.5 * self.budget))\n                \n                if np.random.rand() < exploration_factor * self.adaptive_exploration_rate:\n                    learning_factor = np.random.uniform(-self.learning_step, self.learning_step, self.dim)\n                    particles[i] += learning_factor * (particles[i] - global_best_position)\n                    \n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget))\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedHierarchicalPSOLearning got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06561 with standard deviation 0.00159.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.06628739135372497, 0.06341471954089228, 0.06713225391381217]}}
{"id": "42c77ca1-4a74-4735-bc60-452bdf0989c8", "fitness": 0.07062214090239609, "name": "EnhancedHybridPSOANS", "description": "Enhance the adaptive scale in velocity update and increase mutation probability to improve exploration.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2  # Updated for better exploration\n        self.social_coefficient = 1.7  # Updated for better exploitation\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Adjusted cooling rate further for stability\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.15 * self.budget)))  # Adjusted for enhanced exploration\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.2:  # Increased probability of dynamic mutation\n                    mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Increased mutation strength\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()  # Absolute value for better spread\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 55, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07062 with standard deviation 0.00543.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.06843384561265875, 0.07809085825062623, 0.06534171884390327]}}
{"id": "57309cb1-4e79-4eb3-b55a-a9bee88eb63c", "fitness": 0.06561964581977446, "name": "EnhancedHybridPSOANSV2", "description": "Integrate Adaptive Memory Techniques and Strategic Velocity Reinitialization in Particle Swarm Optimization to Enhance Convergence Speed and Solution Quality.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANSV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.memory_pool = []  # Memory pool to hold best particles for diversification\n        self.memory_size = 5\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n        \n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            new_velocity = self.inertia_weight * velocity + cognitive_velocity + social_velocity\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n                        self.memory_pool.append(global_best_position)\n                        if len(self.memory_pool) > self.memory_size:\n                            self.memory_pool.pop(0)\n\n            if np.random.rand() < 0.1:  # Probability of reinitializing velocities\n                for i in range(self.population_size):\n                    velocities[i] = np.random.uniform(-1, 1, self.dim)\n\n            if len(self.memory_pool) > 0 and np.random.rand() < 0.2:\n                candidate = self.memory_pool[np.random.choice(len(self.memory_pool))]\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedHybridPSOANSV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06562 with standard deviation 0.00227.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.06366634132735816, 0.06438310213049514, 0.06880949400147007]}}
{"id": "eea73af0-1a4e-41bc-9844-30101a7134b9", "fitness": 0.06916986140155046, "name": "EnhancedHybridPSOANS", "description": "Improve convergence by refining velocity update dynamics with time-varying coefficients.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            # Changed line for adaptive cognitive coefficient\n            dynamic_cognitive_coefficient = self.cognitive_coefficient * (self.eval_count / self.budget)\n            cognitive_velocity = dynamic_cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            # Changed line for velocity scaling\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * (adaptive_scale * 0.8)\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0])\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06917 with standard deviation 0.00501.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.06965262299001496, 0.07504564852570761, 0.06281131268892881]}}
{"id": "32590a68-7585-4e32-b234-ac650295743e", "fitness": 0.06441384143219135, "name": "EnhancedHybridPSOANS", "description": "Enrich exploration by dynamically adjusting mutation probability based on diversity.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2  # Updated for better exploration\n        self.social_coefficient = 1.7  # Updated for better exploitation\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Adjusted cooling rate further for stability\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.10 + 0.20 * diversity:  # Dynamically adjusted mutation probability based on diversity\n                    mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Increased mutation strength\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()  # Absolute value for better spread\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06441 with standard deviation 0.00551.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.05963367168251421, 0.0721301433198791, 0.06147770929418073]}}
{"id": "6fae9cf9-45f2-4d7c-9179-367160622523", "fitness": 0.07229490920744726, "name": "EnhancedHybridPSOANS", "description": "Implement a dynamic adjustment for mutation probability based on the diversity of the population to enhance exploration in low-diversity scenarios.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2  # Updated for better exploration\n        self.social_coefficient = 1.7  # Updated for better exploitation\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Adjusted cooling rate further for stability\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                dynamic_mutation_probability = 0.15 + (0.1 * (1 - diversity))  # New line: Adjust mutation probability based on diversity\n                if np.random.rand() < dynamic_mutation_probability:  # Adjusted mutation probability\n                    mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Increased mutation strength\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()  # Absolute value for better spread\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07229 with standard deviation 0.00248.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.07039508407747719, 0.07068721968369696, 0.07580242386116764]}}
{"id": "13efb427-9650-4a6a-89c8-2f520a6855b4", "fitness": 0.07261570358275082, "name": "EnhancedHybridPSOANS", "description": "Enhance convergence by tuning inertia weight and exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.1  # Slightly reduced for balance\n        self.social_coefficient = 1.8  # Slightly increased for better exploitation\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Adjusted cooling rate further for stability\n        self.eval_count = 0\n        self.inertia_min = 0.3  # Lowered minimum inertia for better convergence\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:  # Increased probability of dynamic mutation\n                    mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0])  # Increased mutation strength\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()  # Absolute value for better spread\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07262 with standard deviation 0.00491.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.06595887264243183, 0.07765797783541994, 0.0742302602704007]}}
{"id": "792ef2fb-deaf-4083-a4c2-84cb29d447f2", "fitness": 0.06906700439455253, "name": "RefinedHybridPSOANS", "description": "Introduce Elite Preservation and Adaptive Inertia with Self-adaptive Mutation Strategy for Enhanced Convergence and Stability in Particle Swarm Optimization.", "code": "import numpy as np\n\nclass RefinedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.elite_fraction = 0.1  # Fraction of elite particles preserved\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.zeros((self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        elite_count = max(1, int(self.population_size * self.elite_fraction))\n        \n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n            \n            # Preserve elite particles\n            elite_indices = np.argsort(personal_best_values)[:elite_count]\n            elite_particles = personal_best_positions[elite_indices]\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation_strength = 0.05 * (bounds[:, 1] - bounds[:, 0])  # Self-adaptive mutation strength\n                    mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            # Reintroduce elite particles to ensure stability\n            particles[:elite_count] = elite_particles\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 61, "feedback": "The algorithm RefinedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06907 with standard deviation 0.00181.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.07147092223586415, 0.06861597431400634, 0.06711411663378708]}}
{"id": "5e4b738d-6f60-42ab-b603-c3e66dc58536", "fitness": 0.06309177919429303, "name": "TimeAdaptivePSO", "description": "Introduce a Time-Adaptive Particle Swarm Optimization with Dynamic Restarts to Enhance Global and Local Search Balance in Optimization.", "code": "import numpy as np\n\nclass TimeAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.9\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 2.0\n        self.eval_count = 0\n        self.reinit_threshold = 0.1\n        self.reinit_probability = 0.2\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_inertia_weight = self.inertia_weight * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = dynamic_inertia_weight * velocity + cognitive_velocity + social_velocity\n            return new_velocity\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            if diversity < self.reinit_threshold and np.random.rand() < self.reinit_probability:\n                particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n                velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n                personal_best_positions = np.copy(particles)\n                personal_best_values = np.array([func(p) for p in particles])\n                global_best_idx = np.argmin(personal_best_values)\n                global_best_position = personal_best_positions[global_best_idx, :]\n                global_best_value = personal_best_values[global_best_idx]\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 62, "feedback": "The algorithm TimeAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06309 with standard deviation 0.00365.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.05924472828776639, 0.06203156717140956, 0.06799904212370311]}}
{"id": "956d1496-0873-4e66-a279-35cea0b455c3", "fitness": 0.07547028430023332, "name": "EnhancedHybridPSOANS", "description": "Introduce a Gradient-Informed Dynamic Mutation Strategy to enhance local search capabilities within EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07547 with standard deviation 0.00294.", "error": "", "parent_ids": ["f3adf5e2-fdd5-46ec-864b-08c0ea9b8d98"], "operator": null, "metadata": {"aucs": [0.0714571282236186, 0.07843663214662311, 0.07651709253045824]}}
{"id": "89fb7921-7f34-41d3-b6e8-e4aa12f33d51", "fitness": 0.0697589288906539, "name": "EnhancedHybridPSOANS", "description": "Introduce Adaptive Neighborhood Strategy and Inertia Weight Oscillation to balance exploration and exploitation in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.neighborhood_size = 3\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        def calculate_local_best(particle_idx):\n            neighbors = np.argsort(personal_best_values)[max(0, particle_idx-self.neighborhood_size):\n                                                         min(self.population_size, particle_idx+self.neighborhood_size+1)]\n            best_neighbor_idx = neighbors[np.argmin(personal_best_values[neighbors])]\n            return personal_best_positions[best_neighbor_idx]\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n            self.inertia_weight *= (1 + 0.5 * np.sin(2 * np.pi * self.eval_count / self.budget))\n\n            for i in range(self.population_size):\n                local_best_position = calculate_local_best(i)\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], local_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06976 with standard deviation 0.00653.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.060530300547185334, 0.07419886299826084, 0.07454762312651553]}}
{"id": "a4e0e565-60c7-4ae9-ada5-27545d1f9e31", "fitness": 0.06955374918759964, "name": "EnhancedHybridPSOANS", "description": "Incorporate a Diversity-Boosted Recombination Mechanism to further enhance exploration and exploitation balance in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        def diversity_boosted_recombination(particle, personal_best, global_best):\n            random_particle = particles[np.random.randint(self.population_size)]\n            recomb_coeff = np.random.rand()\n            recombined = recomb_coeff * particle + (1 - recomb_coeff) * random_particle\n            return recombined\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                if np.random.rand() < 0.05:\n                    recombined_particle = diversity_boosted_recombination(particles[i], personal_best_positions[i], global_best_position)\n                    particles[i] = np.clip(recombined_particle, bounds[:, 0], bounds[:, 1])\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06955 with standard deviation 0.00242.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.07054519425861061, 0.06622362313188435, 0.07189243017230396]}}
{"id": "d2c961b5-f408-46ae-aa41-7ec81e6a902f", "fitness": 0.07521253306483548, "name": "EnhancedHybridPSOANS", "description": "Introduce adaptive gradient influence based on evaluation progress to improve convergence speed in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = (0.5 + 0.5 * (self.eval_count / self.budget)) * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07521 with standard deviation 0.00330.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.07068387451742508, 0.07843663214662311, 0.07651709253045824]}}
{"id": "3c8464c9-104e-4e88-bbb3-76001d7db465", "fitness": 0.07363269305324309, "name": "EnhancedHybridPSOANS", "description": "Integrate a Differential Evolution-inspired oscillating convergence mechanism to dynamically enhance exploration-exploitation balance in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.oscillation_amplitude = 0.5\n        self.oscillation_frequency = 0.8\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            oscillation = np.sin(self.oscillation_frequency * np.pi * self.eval_count / self.budget) * self.oscillation_amplitude\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * (adaptive_scale + oscillation)\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07363 with standard deviation 0.00229.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.07080248105042974, 0.07642142001125651, 0.073674178098043]}}
{"id": "966cfa69-c356-455f-9c1a-f1bf880a285b", "fitness": 0.07547028430023332, "name": "EnhancedHybridPSOANS", "description": "Introduce a dynamic inertia weight adjustment based on convergence speed to enhance exploration and exploitation balance in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            convergence_factor = np.abs(global_best_value - np.mean(personal_best_values)) / np.std(personal_best_values)\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity) * convergence_factor\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 68, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07547 with standard deviation 0.00294.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.0714571282236186, 0.07843663214662311, 0.07651709253045824]}}
{"id": "007b5c9b-be99-4b25-9c5b-b45886b44b12", "fitness": 0.06985569278997428, "name": "EnhancedHybridPSOANS", "description": "Refine EnhancedHybridPSOANS by introducing a diversity-aware inertia adjustment mechanism to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget + diversity * 0.5))  # Adjusted line\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06986 with standard deviation 0.00322.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.0689480586928195, 0.06644729193773402, 0.07417172773936931]}}
{"id": "9e786f0f-57da-465c-b9db-1a663a0fd0bb", "fitness": 0.07169265993966083, "name": "EnhancedHybridPSOANS", "description": "Introduce an adaptive mutation strength scaling based on diversity to enhance search efficiency in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            diversity = np.std(particles, axis=0).mean()\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 + diversity)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07169 with standard deviation 0.00822.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.06012425514190112, 0.07843663214662311, 0.07651709253045824]}}
{"id": "b8645672-9257-4d10-9123-8bdea8f22c56", "fitness": -Infinity, "name": "EnhancedHybridPSOANS", "description": "Incorporate Adaptive Differential Evolution and a Self-Learning Mechanism into EnhancedHybridPSOANS for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def adaptive_differential_evolution(particles, global_best):\n            trial_population = np.copy(particles)\n            for i in range(self.population_size):\n                idxs = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = particles[np.random.choice(idxs, 3, replace=False)]\n                mutant_vector = a + self.mutation_factor * (b - c)\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_population[i] = np.where(cross_points, mutant_vector, particles[i])\n                trial_population[i] = np.clip(trial_population[i], bounds[:, 0], bounds[:, 1])\n                trial_value = func(trial_population[i])\n                self.eval_count += 1\n                if trial_value < personal_best_values[i]:\n                    personal_best_positions[i] = trial_population[i]\n                    personal_best_values[i] = trial_value\n                    if trial_value < global_best_value:\n                        global_best_position = trial_population[i]\n                        global_best_value = trial_value\n            return trial_population\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n            particles = adaptive_differential_evolution(particles, global_best_position)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 71, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'global_best_value' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'global_best_value' referenced before assignment\")", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {}}
{"id": "ac971e49-dd5d-46d5-9e19-8c619b200fb4", "fitness": 0.06890639113130086, "name": "EnhancedHybridPSOANS", "description": "Introduce Adaptive Particle Interaction Dynamics to strengthen convergence by dynamically adjusting particle influence and exploration/exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.convergence_pressure = 0.1\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best, influence_factor):\n            r1, r2 = np.random.rand(2)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle) * influence_factor\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def adaptive_influence_diversity():\n            diversity_factor = np.std(particles, axis=0).mean()\n            return 1 + self.convergence_pressure * (1 - diversity_factor / (bounds[:,1] - bounds[:,0]).mean())\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            influence_factor = adaptive_influence_diversity()\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position, influence_factor)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06891 with standard deviation 0.00375.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.07098776833990783, 0.0636455466410295, 0.07208585841296522]}}
{"id": "5019eeda-2692-43ce-a596-6fbe69a55da7", "fitness": 0.049429831513013944, "name": "EnhancedHybridPSOANS", "description": "Integrate Adaptive Inertia Weight Adjustment based on Fitness Improvement Rate to improve convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.previous_global_best_value = float('inf')  # New line for fitness improvement tracking\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            fitness_improvement = abs(global_best_value - self.previous_global_best_value) / max(abs(self.previous_global_best_value), 1e-10)  # New line for fitness improvement\n            self.inertia_weight = (self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)) * (1 + 0.5 * fitness_improvement)  # Updated line\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n                        self.previous_global_best_value = global_best_value  # New line\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n                    self.previous_global_best_value = global_best_value  # New line\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.04943 with standard deviation 0.00192.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.04678085277932842, 0.05128064379649844, 0.050227997963214976]}}
{"id": "ee883991-e082-42d4-bf17-264bd8398a96", "fitness": 0.06759162705485622, "name": "EnhancedHybridPSOANS", "description": "Enhance global exploration by integrating adaptive inertia weight and dynamic gradient perturbation into EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.9  # Changed line\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.7 * gradient_direction  # Changed line\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.2:  # Changed line\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate * 0.98  # Changed line\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06759 with standard deviation 0.00821.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.05856812178696402, 0.07843663214662311, 0.06577012723098152]}}
{"id": "ea9add74-d28b-42c9-a0cf-8ab2810b4c86", "fitness": 0.07547028430023332, "name": "EnhancedHybridPSOANS", "description": "Integrate a Novel Adaptive Diversity Control Mechanism within EnhancedHybridPSOANS to Boost Convergence.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.diversity_threshold = 0.1  # New parameter for adaptive diversity control\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if diversity < self.diversity_threshold:  # Adaptive diversity control\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07547 with standard deviation 0.00294.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.0714571282236186, 0.07843663214662311, 0.07651709253045824]}}
{"id": "868b0eb5-66f2-411a-b2c4-3f983ba22ad8", "fitness": 0.07547028430023332, "name": "EnhancedHybridPSOANS", "description": "Implement an Adaptive Memory Strategy and Diversity Preservation Mechanism in EnhancedHybridPSOANS to improve convergence and exploration.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.memory_size = 10\n        self.memory = []\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        def diversity_preservation(particles):\n            diversity_threshold = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            for i in range(self.population_size):\n                if np.linalg.norm(particles[i] - global_best_position) < diversity_threshold.mean():\n                    particles[i] = np.random.uniform(bounds[:, 0], bounds[:, 1], self.dim)\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n                        self.memory.append((global_best_position, global_best_value))\n                        if len(self.memory) > self.memory_size:\n                            self.memory.pop(0)\n\n            diversity_preservation(particles)\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n                    self.memory.append((global_best_position, global_best_value))\n                    if len(self.memory) > self.memory_size:\n                        self.memory.pop(0)\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07547 with standard deviation 0.00294.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.0714571282236186, 0.07843663214662311, 0.07651709253045824]}}
{"id": "bbd4b1d0-25a2-4f9b-a7bc-d85cde17d769", "fitness": 0.07169265993966083, "name": "EnhancedHybridPSOANS", "description": "Introduce a Diversity-Driven Adaptive Mutation Mechanism to balance exploration and exploitation in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best, diversity_factor):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = diversity_factor * gradient_direction \n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    diversity_factor = 0.5 * (1.0 - diversity)\n                    mutation = gradient_informed_mutation(particles[i], global_best_position, diversity_factor)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07169 with standard deviation 0.00822.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.06012425514190112, 0.07843663214662311, 0.07651709253045824]}}
{"id": "4bda7fe5-8557-4633-a4bf-46294c659c65", "fitness": 0.07547028430023332, "name": "EnhancedHybridPSOANS", "description": "Incorporate a temperature-based inertia weight adaptation to enhance exploration and exploitation balance in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity) * (1 - self.temperature)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07547 with standard deviation 0.00294.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.0714571282236186, 0.07843663214662311, 0.07651709253045824]}}
{"id": "8d607d3d-4440-4465-969e-b1daef5c1012", "fitness": 0.0662681520108968, "name": "EnhancedHybridPSOANS", "description": "Utilize a Dynamic Dual Inertia Weight Strategy to enhance exploration-exploitation balance and a density-aware mutation for improved local convergence within EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight_exploration = 0.9\n        self.inertia_weight_exploitation = 0.4\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            inertia_weight_dynamic = self.inertia_weight_exploration + (self.eval_count / self.budget) * (self.inertia_weight_exploitation - self.inertia_weight_exploration)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = self.social_coefficient * r2 * (global_best - particle)\n            new_velocity = inertia_weight_dynamic * velocity + cognitive_velocity + social_velocity\n            return new_velocity\n\n        def density_aware_mutation(particle, global_best):\n            mutation_strength = 0.2 * (bounds[:, 1] - bounds[:, 0])\n            density_factor = np.mean(np.linalg.norm(particles - particle, axis=1))\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim) * (1 / (1 + density_factor))\n            return mutation\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = density_aware_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06627 with standard deviation 0.00162.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.06842122309081089, 0.06585313493595679, 0.06453009800592269]}}
{"id": "7f04042c-18f6-404d-9b66-1c367aacf422", "fitness": 0.07547028430023332, "name": "EnhancedHybridPSOANS", "description": "Adjust the cognitive coefficient dynamically based on evaluation count to balance exploration and exploitation more effectively.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            # Change 1 line: Adjust the cognitive coefficient dynamically\n            dynamic_cognitive_coefficient = self.cognitive_coefficient * (1 + self.eval_count / self.budget)\n            cognitive_velocity = dynamic_cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def gradient_informed_mutation(particle, global_best):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0])\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                if np.random.rand() < 0.15:\n                    mutation = gradient_informed_mutation(particles[i], global_best_position)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07547 with standard deviation 0.00294.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.0714571282236186, 0.07843663214662311, 0.07651709253045824]}}
{"id": "7a1ebef1-4ef5-4031-824b-ddfa3dfaf426", "fitness": 0.07611058047677359, "name": "EnhancedHybridPSOANS", "description": "Integrate Adaptive Cognitive-Social Scaling and Fitness-Driven Mutation Intensity to EnhancedHybridPSOANS for improved convergence precision.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["956d1496-0873-4e66-a279-35cea0b455c3"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "86ac6cf1-c81e-4af0-9c7f-3746c1acdbbc", "fitness": 0.07592323221190515, "name": "EnhancedHybridPSOANS", "description": "Enhanced global search by introducing exponential scaling in mutation strength for better exploration.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * np.exp(-value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07592 with standard deviation 0.00389.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07088708973877578, 0.08036551436648143, 0.07651709253045824]}}
{"id": "77a2b4b0-fa3a-4f3a-a58e-dc4eb5860f48", "fitness": 0.06757102359248894, "name": "EnhancedHybridPSOANSRefined", "description": "Introduce Dynamic Adaptive Learning Rates and Two-Phase Exploration-Exploitation Balance to EnhancedHybridPSOANS for accelerated convergence and global optimality.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANSRefined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient_initial = 2.5\n        self.social_coefficient_initial = 1.5\n        self.social_coefficient_final = 2.0\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_cognitive_coefficient = self.cognitive_coefficient_initial * (1 - self.eval_count / self.budget)\n            dynamic_social_coefficient = self.social_coefficient_initial + (self.social_coefficient_final - self.social_coefficient_initial) * (self.eval_count / self.budget)\n            cognitive_velocity = dynamic_cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            new_velocity = self.inertia_weight * velocity + cognitive_velocity + social_velocity\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n        \n        # Two-phase exploration-exploitation strategy\n        exploration_phase_end = int(0.4 * self.budget)\n\n        while self.eval_count < self.budget:\n            if self.eval_count < exploration_phase_end:\n                self.inertia_weight = self.inertia_max\n            else:\n                self.inertia_weight = self.inertia_min + (self.inertia_max - self.inertia_min) * (1 - (self.eval_count - exploration_phase_end) / (self.budget - exploration_phase_end))\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedHybridPSOANSRefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06757 with standard deviation 0.00232.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.06889282080806813, 0.06431031836545253, 0.06950993160394614]}}
{"id": "2e31ecda-7f1f-4c83-a35e-6c64ca83886b", "fitness": 0.07611058047677359, "name": "EnhancedHybridPSOANS", "description": "Incorporate dynamic inertia weight scaling based on current diversity to enhance convergence precision.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 84, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "6bef20f8-404d-417b-8749-6d73b560233b", "fitness": 0.07605470214962888, "name": "EnhancedHybridPSOANS", "description": "Adjusted fitness-driven mutation strength scaling for enhanced adaptability in dynamic environments.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07605 with standard deviation 0.00376.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07123331308907677, 0.08041370082935162, 0.07651709253045824]}}
{"id": "bb4d1e76-6ef7-4a87-a08c-b7d0c9336d3b", "fitness": 0.07611058047677359, "name": "EnhancedHybridPSOANS_Improved", "description": "Implement a Dynamic Levy Flight-Based Perturbation in EnhancedHybridPSOANS to enhance exploration and avoid premature convergence.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        def levy_flight(Lambda):\n            u = np.random.normal(0, 1, self.dim) * (0.01 / np.abs(np.random.normal(0, 1, self.dim))**(1/Lambda))\n            return u\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = levy_flight(1.5) * perturbation_strength\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedHybridPSOANS_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "706db1a0-5d6f-404f-bde4-b3951bb02b99", "fitness": 0.07605470214962888, "name": "EnhancedHybridPSOANS", "description": "Introduce a local search enhancement with a slight boost in mutation to refine convergence precision.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.15 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)  # Slight boost in mutation strength\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07605 with standard deviation 0.00376.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07123331308907677, 0.08041370082935162, 0.07651709253045824]}}
{"id": "55077349-4e09-42bf-9b26-9311a4930b04", "fitness": 0.07611058047677359, "name": "EnhancedHybridPSOANS", "description": "Introduce Hierarchical Dynamic Inertia Weighting and Adaptive Perturbation to EnhancedHybridPSOANS for superior convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength * (1 + diversity), self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "b7956475-c92a-48ba-a677-ce779b9b42ec", "fitness": 0.059570399869280255, "name": "EnhancedHybridPSOANS", "description": "Incorporate Multi-Objective Fitness Evaluation and Dynamic Population Resizing in EnhancedHybridPSOANS to enhance adaptability and precision.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        def resize_population(diversity):\n            new_population_size = int(self.population_size * (1 + diversity))\n            if new_population_size != self.population_size:\n                self.population_size = new_population_size\n                return True\n            return False\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            if resize_population(diversity):\n                particles = np.vstack((particles, np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size - len(particles), self.dim))))\n                velocities = np.vstack((velocities, np.random.uniform(-1, 1, (self.population_size - len(velocities), self.dim))))\n                personal_best_positions = np.vstack((personal_best_positions, particles[len(personal_best_positions):]))\n                personal_best_values = np.hstack((personal_best_values, np.array([func(p) for p in particles[len(personal_best_values):]])))\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05957 with standard deviation 0.00100.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.0582976848322333, 0.06073135462691004, 0.05968216014869743]}}
{"id": "6dc9c64a-89ec-41c4-b48c-c4454aed9292", "fitness": 0.07495988411664027, "name": "EnhancedHybridPSOANS", "description": "Introduce a small adaptive mutation factor to enhance exploration capability in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim) * (0.8 + 0.2 * np.random.rand(self.dim)) # Change made here\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07496 with standard deviation 0.00546.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.0674535260650092, 0.08028393556745794, 0.07714219071745365]}}
{"id": "1fd79be9-8034-4865-ba55-8dd9ac2df3d9", "fitness": 0.05898272524776423, "name": "EnhancedHybridPSOANS", "description": "Introduce a feedback-based inertia adjustment and hybrid exploration strategy for EnhancedHybridPSOANS to improve convergence stability.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            feedback = 1 + (global_best_value - np.mean(personal_best_values)) / (np.std(personal_best_values) + 1e-9)\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale * feedback\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05898 with standard deviation 0.00187.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.056429026031228346, 0.06083698956336692, 0.05968216014869743]}}
{"id": "7e97663b-b751-4661-b967-27d13b5b09a9", "fitness": 0.07611058047677359, "name": "EnhancedHybridPSOANS", "description": "Tuning cooling rate for improved exploration-exploitation balance in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.92  # Tuned cooling rate\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "ba321153-05b4-47f8-98cb-00f19b264bcb", "fitness": 0.07611058047677359, "name": "EnhancedHybridPSOANS", "description": "Introduce a slight increase in the cognitive coefficient for improved local search adaptability.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.3  # Increased from 2.2 for better exploitation\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "b82e1885-0f10-4d8a-858c-67b61afda581", "fitness": 0.07611058047677359, "name": "EnhancedHybridPSOANS", "description": "Refine inertia weight reduction by incorporating temperature-dependent cooling for balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity * self.temperature)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "fbe34f55-a610-44ef-a4a6-c28df6321994", "fitness": 0.07611058047677359, "name": "EnhancedHybridPSOANS", "description": "Introduce a dynamic cognitive coefficient for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            dynamic_cognitive_coefficient = self.cognitive_coefficient * (0.5 + 0.5 * (1 - self.eval_count / self.budget))\n            cognitive_velocity = dynamic_cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "74dfe8ba-371c-40ae-89a9-ae9c886e3948", "fitness": 0.07611058047677359, "name": "RefinedEnhancedHybridPSOANS", "description": "Enhance exploration and exploitation balance by integrating nonlinear inertia reduction and adaptive neighborhood search for improved convergence in EnhancedHybridPSOANS.", "code": "import numpy as np\n\nclass RefinedEnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.9\n        self.inertia_min = 0.4\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = self.cognitive_coefficient * r1 * (personal_best - particle)\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_min + (self.inertia_weight - self.inertia_min) * np.exp(-10 * self.eval_count / self.budget)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 96, "feedback": "The algorithm RefinedEnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "df42b8e4-29cd-44c6-9e64-93cac14076a2", "fitness": 0.07611058047677359, "name": "EnhancedHybridPSOANS", "description": "Introduce a dynamic inertia weight strategy to improve convergence by adapting inertia based on diversity and evaluation count.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07611 with standard deviation 0.00404.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.07092603688400656, 0.08077583977497194, 0.07662986477134226]}}
{"id": "6f71f7a1-2deb-416e-8660-f61521bb5105", "fitness": 0.07295948939352219, "name": "EnhancedHybridPSOANS", "description": "Introduce a dynamic mutation rate adjustment based on convergence speed to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def fitness_driven_mutation(particle, global_best, value_diff):\n            convergence_speed = abs(value_diff) / max(self.eval_count / self.budget, 1e-9)  # Dynamic mutation rate adjustment\n            mutation_strength = 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff) * (1 - convergence_speed)\n            gradient_direction = global_best - particle\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim)\n            gradient_influence = 0.5 * gradient_direction\n            return mutation + gradient_influence\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < 0.15:\n                    mutation = fitness_driven_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07296 with standard deviation 0.00612.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.06441887161884707, 0.07843663214662311, 0.07602296441509637]}}
{"id": "c2ba2754-8e0b-43cb-af0d-ed682feaef3c", "fitness": 0.07416206936171692, "name": "EnhancedHybridPSOANS", "description": "Introduce Adaptive Mutation with Feedback-Driven Selection Pressure to EnhancedHybridPSOANS for superior convergence stability and quality.", "code": "import numpy as np\n\nclass EnhancedHybridPSOANS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.inertia_weight = 0.7\n        self.cognitive_coefficient = 2.2\n        self.social_coefficient = 1.7\n        self.temperature = 1.0\n        self.cooling_rate = 0.95\n        self.eval_count = 0\n        self.inertia_min = 0.4\n        self.inertia_max = 0.9\n        self.mutation_rate = 0.15\n        self.selection_pressure = 0.5\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(particles)\n        personal_best_values = np.array([func(p) for p in particles])\n        global_best_idx = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_idx, :]\n        global_best_value = personal_best_values[global_best_idx]\n\n        def update_particle_velocity(velocity, particle, personal_best, global_best):\n            r1, r2 = np.random.rand(2)\n            dynamic_social_coefficient = self.social_coefficient * (1 - self.eval_count / self.budget)\n            cognitive_velocity = (self.cognitive_coefficient * r1 * (personal_best - particle)) * (0.5 + 0.5 * (self.eval_count / self.budget))\n            social_velocity = dynamic_social_coefficient * r2 * (global_best - particle)\n            adaptive_scale = 1 / (1 + np.exp(-self.eval_count / (0.2 * self.budget)))\n            new_velocity = (self.inertia_weight * velocity + cognitive_velocity + social_velocity) * adaptive_scale\n            return new_velocity\n\n        def adaptive_mutation(particle, global_best, value_diff):\n            mutation_strength = np.maximum(0.01, 0.1 * (bounds[:, 1] - bounds[:, 0]) * (1 - value_diff))\n            feedback_factor = np.tanh(self.selection_pressure * value_diff)\n            mutation = np.random.uniform(-mutation_strength, mutation_strength, self.dim) * feedback_factor\n            return mutation\n\n        while self.eval_count < self.budget:\n            diversity = np.std(particles, axis=0).mean()\n            self.inertia_weight = self.inertia_max - ((self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)) * (1 + diversity)\n\n            for i in range(self.population_size):\n                velocities[i] = update_particle_velocity(velocities[i], particles[i], personal_best_positions[i], global_best_position)\n                particles[i] += velocities[i]\n\n                value_diff = (personal_best_values[i] - global_best_value) / max(personal_best_values[i], global_best_value, 1e-9)\n                if np.random.rand() < self.mutation_rate:\n                    mutation = adaptive_mutation(particles[i], global_best_position, value_diff)\n                    particles[i] += mutation\n\n                particles[i] = np.clip(particles[i], bounds[:, 0], bounds[:, 1])\n                current_value = func(particles[i])\n                self.eval_count += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = particles[i]\n                    personal_best_values[i] = current_value\n\n                    if current_value < global_best_value:\n                        global_best_position = particles[i]\n                        global_best_value = current_value\n\n            if np.random.rand() < self.temperature:\n                perturbation_strength = np.std(np.abs(particles - global_best_position), axis=0).mean()\n                perturbation = np.random.normal(0, perturbation_strength, self.dim)\n                candidate = global_best_position + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_value = func(candidate)\n                self.eval_count += 1\n\n                if candidate_value < global_best_value:\n                    global_best_position = candidate\n                    global_best_value = candidate_value\n\n            self.temperature *= self.cooling_rate\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 99, "feedback": "The algorithm EnhancedHybridPSOANS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07416 with standard deviation 0.00658.", "error": "", "parent_ids": ["7a1ebef1-4ef5-4031-824b-ddfa3dfaf426"], "operator": null, "metadata": {"aucs": [0.06514874578609764, 0.08069185149047076, 0.07664561080858234]}}
