{"id": "65012567-a4fb-40f7-8435-a8ff348d9b6a", "fitness": 0.0882778976101883, "name": "HybridMetaheuristicOptimizer", "description": "A novel hybrid metaheuristic algorithm combining differential evolution and simulated annealing for efficient exploration and exploitation in black box optimization.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters for DE and SA\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        # Search loop\n        while evaluations < self.budget:\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Simulated Annealing Step\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n                \n                evaluations += 1\n\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 0, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08828 with standard deviation 0.05246.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1563261782830706, 0.16610614381970956, 0.14623882027114488, 0.08775307879346828, 0.07958352794693069, 0.0686366520814603, 0.03261947914586416, 0.033513817103343224, 0.023723381046703018]}}
{"id": "c8fb93ab-cc1d-414f-acbe-9d4e1421db95", "fitness": 0.09012869807959303, "name": "HybridMetaheuristicOptimizer", "description": "A refined hybrid metaheuristic optimizer with an enhanced exploration strategy for improved solution quality in black box optimization.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters for DE and SA\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr + 0.05  # Slightly increased crossover probability\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        # Search loop\n        while evaluations < self.budget:\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Simulated Annealing Step\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n                \n                evaluations += 1\n\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 1, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09013 with standard deviation 0.05133.", "error": "", "parent_ids": ["65012567-a4fb-40f7-8435-a8ff348d9b6a"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.15496102411855783, 0.08594193586186327, 0.07991149574720158, 0.07287160808344317, 0.032817741288129865, 0.026202435322045692, 0.04197380864018341]}}
{"id": "bd184732-b927-404b-a859-49b884251890", "fitness": 0.09150180410852811, "name": "HybridMetaheuristicOptimizer", "description": "An improved hybrid optimizer integrating adaptive parameter tuning for better convergence performance in black box optimization.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters for DE and SA\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr + 0.05\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.99 + 0.02 * np.random.rand()))\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n            \n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Simulated Annealing Step\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n                \n                evaluations += 1\n\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 2, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09150 with standard deviation 0.05835.", "error": "", "parent_ids": ["c8fb93ab-cc1d-414f-acbe-9d4e1421db95"], "operator": null, "metadata": {"aucs": [0.15572868577149968, 0.16610614381970956, 0.18152788548444798, 0.09121778453517804, 0.08011702364135276, 0.06245877514527742, 0.03566059669464372, 0.026343422260395433, 0.024355919624248434]}}
{"id": "a6e476c5-eb15-4990-8e1f-a24ed2e57ae2", "fitness": 0.08942164656197993, "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptive parameter tuning by incorporating self-adaptive mutation strength adjustment in DE for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters for DE and SA\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr + 0.05\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.99 + 0.02 * np.random.rand() * (1 if np.random.rand() > 0.5 else -1)))\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n            \n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Simulated Annealing Step\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n                \n                evaluations += 1\n\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 3, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08942 with standard deviation 0.04980.", "error": "", "parent_ids": ["bd184732-b927-404b-a859-49b884251890"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.14660329738847155, 0.08700100179524983, 0.07940004781166177, 0.0715476109630162, 0.03263644676382094, 0.036474421397562296, 0.03465375928312431]}}
{"id": "cdc06bb6-2840-4fb1-904b-e493bfaec239", "fitness": 0.09150180410852811, "name": "HybridMetaheuristicOptimizer", "description": "A refined hybrid optimizer introducing strategic diversity preservation through occasional random reinitialization to enhance explorative behavior in black box optimization.", "code": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters for DE and SA\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr + 0.05\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.99 + 0.02 * np.random.rand()))\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n            \n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Simulated Annealing Step\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n                \n                evaluations += 1\n\n            # Occasional reinitialization to preserve diversity\n            if evaluations < self.budget and evaluations % (2 * population_size) == 0:\n                random_idx = np.random.randint(0, population_size)\n                population[random_idx] = np.random.uniform(lb, ub, self.dim)\n                fitness[random_idx] = func(population[random_idx])\n                evaluations += 1\n\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 4, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09150 with standard deviation 0.05835.", "error": "", "parent_ids": ["bd184732-b927-404b-a859-49b884251890"], "operator": null, "metadata": {"aucs": [0.15572868577149968, 0.16610614381970956, 0.18152788548444798, 0.09121778453517804, 0.08011702364135276, 0.06245877514527742, 0.03566059669464372, 0.026343422260395433, 0.024355919624248434]}}
{"id": "cb18071c-a112-4958-8f7c-5b1685104349", "fitness": 0.09286450016253549, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Neighborhood Search and Diversity Preservation for robust black-box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        neighborhood_radius = 0.1  # Initial radius for neighborhood search\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr + 0.05\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.99 + 0.02 * np.random.rand()))\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Adaptive Neighborhood Search and Diversity Preservation\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Simulated Annealing Step\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            # Update cooling schedule and neighborhood radius\n            temp *= cooling_rate\n            neighborhood_radius *= 0.99  # Decrease radius to refine search\n\n        return best_solution", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09286 with standard deviation 0.04737.", "error": "", "parent_ids": ["bd184732-b927-404b-a859-49b884251890"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.14773810823341071, 0.08836261774980458, 0.0805463019017828, 0.07321864148490564, 0.06076375522424293, 0.028253663274832475, 0.04041917993892774]}}
{"id": "c2d6b6ed-7f50-4077-a930-782809c9cd7e", "fitness": 0.0909103677294258, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer integrating Adaptive Memory Programming and Dynamic Neighborhood Scaling for enhanced exploration-exploitation balance in black-box optimization.", "code": "import numpy as np\nimport collections\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters\n        F, Cr = 0.8, 0.9  # DE parameters\n        temp, cooling_rate = 1.0, 0.99  # SA parameters\n        neighborhood_radius = 0.1\n        memory_size = 5\n        memory = collections.deque(maxlen=memory_size)  # Adaptive Memory for best solutions\n        \n        def differential_evolution(pop, fit):\n            indices = np.arange(population_size)\n            for i in indices:\n                a, b, c = np.random.choice(indices[indices != i], 3, replace=False)\n                mutant = np.clip(pop[a] + F * (pop[b] - pop[c]), lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr + 0.05\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i], fit[i] = trial, trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                memory.append(new_solution)\n                return new_solution, new_fitness\n            return ind, fit\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.99 + 0.02 * np.random.rand()))\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Adaptive Neighborhood Search and Diversity Preservation\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution, best_fitness = population[i], fitness[i]\n\n                # Simulated Annealing Step\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i], fitness[i] = new_solution, new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution, best_fitness = new_solution, new_fitness\n\n                evaluations += 1\n\n            # Utilize adaptive memory to refine search\n            if memory:\n                for sol in memory:\n                    perturbed_solution = sol + np.random.normal(0, neighborhood_radius, self.dim)\n                    perturbed_solution = np.clip(perturbed_solution, lb, ub)\n                    perturbed_fitness = func(perturbed_solution)\n                    if perturbed_fitness < best_fitness:\n                        best_solution, best_fitness = perturbed_solution, perturbed_fitness\n\n            # Update cooling schedule and neighborhood radius\n            temp *= cooling_rate\n            neighborhood_radius *= 0.99\n\n        return best_solution", "configspace": "", "generation": 6, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09091 with standard deviation 0.05057.", "error": "", "parent_ids": ["cb18071c-a112-4958-8f7c-5b1685104349"], "operator": null, "metadata": {"aucs": [0.1560824668081242, 0.16610614381970956, 0.15024883021831292, 0.08594193586186327, 0.08303483737739037, 0.0677392128945361, 0.0333240896943966, 0.03408189256967031, 0.04163390032082892]}}
{"id": "5c019f6c-224b-43b9-8d7d-b2d258759a0c", "fitness": 0.09286450016253549, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined Adaptive Cooling in Enhanced Hybrid Metaheuristic Optimizer for more effective temperature control in simulated annealing, boosting convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.98  # Modified cooling rate\n        neighborhood_radius = 0.1  # Initial radius for neighborhood search\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr + 0.05\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.99 + 0.02 * np.random.rand()))\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Adaptive Neighborhood Search and Diversity Preservation\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Simulated Annealing Step\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            # Update cooling schedule and neighborhood radius\n            temp *= cooling_rate\n            neighborhood_radius *= 0.99  # Decrease radius to refine search\n\n        return best_solution", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09286 with standard deviation 0.04737.", "error": "", "parent_ids": ["cb18071c-a112-4958-8f7c-5b1685104349"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.14773810823341071, 0.08836261774980458, 0.0805463019017828, 0.07321864148490564, 0.06076375522424293, 0.028253663274832475, 0.04041917993892774]}}
{"id": "64dc8039-d941-4063-8143-5af351c664e8", "fitness": 0.09271974167224027, "name": "AdvancedSynergisticMetaheuristic", "description": "Advanced Synergistic Metaheuristic Integrating Dynamic Parameter Tuning and Multi-Phase Search for Enhanced Black-Box Optimization Efficiency.", "code": "import numpy as np\n\nclass AdvancedSynergisticMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 15 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters\n        F = 0.7  # DE mutation factor\n        Cr = 0.8  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.98\n        neighborhood_radius = 0.2  # Initial radius for neighborhood search\n        phase_change_threshold = 0.2 * self.budget  # Define a phase change threshold\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr + 0.05\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        # Search loop\n        while evaluations < self.budget:\n            if evaluations < phase_change_threshold:\n                # Phase 1: Exploration\n                Cr = 0.9  # Increase crossover probability for exploration\n                neighborhood_radius = 0.2\n                \n            else:\n                # Phase 2: Exploitation\n                Cr = 0.6  # Decrease crossover probability for exploitation\n                neighborhood_radius = 0.05\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Adaptive Neighborhood Search and Diversity Preservation\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Simulated Annealing Step\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            # Update cooling schedule and neighborhood radius\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm AdvancedSynergisticMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09272 with standard deviation 0.05486.", "error": "", "parent_ids": ["cb18071c-a112-4958-8f7c-5b1685104349"], "operator": null, "metadata": {"aucs": [0.16312473778499925, 0.16610614381970956, 0.1689384968193144, 0.08594193586186327, 0.07940004781166177, 0.06486706500354866, 0.032576917293202556, 0.031455266120916314, 0.042067064534946685]}}
{"id": "268698d3-c1b1-477c-b4de-d4a42cb76a2a", "fitness": 0.09325229680995213, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Neighborhood Search, Diversity Preservation, and Optimized Mutation Strategy for robust black-box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        neighborhood_radius = 0.1  # Initial radius for neighborhood search\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c]) + 0.001 * (best_solution - pop[i])  # Changed this line\n                mutant = np.clip(mutant, lb, ub)\n                cross_points = np.random.rand(self.dim) < Cr + 0.05\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.99 + 0.02 * np.random.rand()))\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Adaptive Neighborhood Search and Diversity Preservation\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Simulated Annealing Step\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            # Update cooling schedule and neighborhood radius\n            temp *= cooling_rate\n            neighborhood_radius *= 0.99  # Decrease radius to refine search\n\n        return best_solution", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09325 with standard deviation 0.04986.", "error": "", "parent_ids": ["cb18071c-a112-4958-8f7c-5b1685104349"], "operator": null, "metadata": {"aucs": [0.1524759575840111, 0.16610614381970956, 0.16045282735323096, 0.08594193586186327, 0.07940004781166177, 0.06477926239652954, 0.060807302087196646, 0.028946777651318367, 0.040360416724048]}}
{"id": "608726cd-2363-4344-b6ac-9ed8bffa15e6", "fitness": 0.09337421793961083, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Dynamic Dimensional Crossover Strategy for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        neighborhood_radius = 0.1  # Initial radius for neighborhood search\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c]) + 0.001 * (best_solution - pop[i])\n                mutant = np.clip(mutant, lb, ub)\n                # Modified this line to use dynamic dimensional crossover\n                Cr_dynamic = Cr * (1 + (evaluations / self.budget))  \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.99 + 0.02 * np.random.rand()))\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Adaptive Neighborhood Search and Diversity Preservation\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Simulated Annealing Step\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            # Update cooling schedule and neighborhood radius\n            temp *= cooling_rate\n            neighborhood_radius *= 0.99  # Decrease radius to refine search\n\n        return best_solution", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09337 with standard deviation 0.05123.", "error": "", "parent_ids": ["268698d3-c1b1-477c-b4de-d4a42cb76a2a"], "operator": null, "metadata": {"aucs": [0.15144859180909864, 0.1730243661773767, 0.1586799428688359, 0.08594193586186327, 0.08297942734050567, 0.07287742216764415, 0.04045529031747508, 0.03521636104888648, 0.03974462386481159]}}
{"id": "4beb9527-bdf6-4615-ace9-164c898284a9", "fitness": 0.0951547245258316, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Mutation Factor Scaling for improved performance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        neighborhood_radius = 0.1  # Initial radius for neighborhood search\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c]) + 0.001 * (best_solution - pop[i])\n                mutant = np.clip(mutant, lb, ub)\n                # Modified this line to use dynamic dimensional crossover\n                Cr_dynamic = Cr * (1 + (evaluations / self.budget))  \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.95 + 0.05 * np.random.rand()))  # Modified line: Adjusted scaling factor\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Adaptive Neighborhood Search and Diversity Preservation\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Simulated Annealing Step\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            # Update cooling schedule and neighborhood radius\n            temp *= cooling_rate\n            neighborhood_radius *= 0.99  # Decrease radius to refine search\n\n        return best_solution", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09515 with standard deviation 0.04879.", "error": "", "parent_ids": ["608726cd-2363-4344-b6ac-9ed8bffa15e6"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.14799319015758705, 0.10136703872686992, 0.08352760029442541, 0.09772032414490484, 0.038133839811926196, 0.032086914058854665, 0.03908537988300387]}}
{"id": "81c3efd1-3ce8-4671-a839-6fb4bb90d78e", "fitness": 0.0951547245258316, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduce dynamic adjustment for mutation factor based on fitness improvement rate to enhance convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters\n        F = 0.8  # DE mutation factor\n        Cr = 0.9  # DE crossover probability\n        temp = 1.0  # Initial temperature for SA\n        cooling_rate = 0.99\n        neighborhood_radius = 0.1  # Initial radius for neighborhood search\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c]) + 0.001 * (best_solution - pop[i])\n                mutant = np.clip(mutant, lb, ub)\n                # Modified this line to use dynamic dimensional crossover\n                Cr_dynamic = Cr * (1 + (evaluations / self.budget))  \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        # Search loop\n        while evaluations < self.budget:\n            # Adaptive adjustment\n            F = max(0.5, F * (0.95 + 0.05 * np.random.rand()))  # Adjusted line: Improved F calculation\n            Cr = min(1.0, Cr * (0.99 + 0.02 * np.random.rand()))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Adaptive Neighborhood Search and Diversity Preservation\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Simulated Annealing Step\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            # Update cooling schedule and neighborhood radius\n            temp *= cooling_rate\n            neighborhood_radius *= 0.99  # Decrease radius to refine search\n\n        return best_solution", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09515 with standard deviation 0.04879.", "error": "", "parent_ids": ["4beb9527-bdf6-4615-ace9-164c898284a9"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.14799319015758705, 0.10136703872686992, 0.08352760029442541, 0.09772032414490484, 0.038133839811926196, 0.032086914058854665, 0.03908537988300387]}}
{"id": "f38bf6ec-124c-486c-aa48-3154df7926df", "fitness": -Infinity, "name": "AdaptiveHybridMetaheuristicOptimizer", "description": "Adaptive Hybrid Metaheuristic Optimizer with Strategic Diversity Management and Improved Adaptive Mechanisms for Enhanced Performance.", "code": "import numpy as np\n\nclass AdaptiveHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        \n        # Parameters\n        F = 0.8\n        Cr = 0.9\n        temp = 1.0\n        cooling_rate = 0.99\n        neighborhood_radius = 0.1\n        \n        def differential_evolution(pop, fit):\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F * (pop[b] - pop[c]) + 0.001 * (best_solution - pop[i])\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr * (1 - evaluations / self.budget)  # Adjusted dynamic crossover\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        # Improved Diversity Management\n        def manage_diversity(pop):\n            centroid = np.mean(pop, axis=0)\n            for i in range(population_size):\n                if np.linalg.norm(pop[i] - centroid) < 0.1 * (ub - lb):\n                    pop[i] = np.random.uniform(lb, ub, self.dim)\n\n        while evaluations < self.budget:\n            F = max(0.4, F * (0.9 + 0.1 * np.random.rand()))\n            Cr = min(1.0, Cr * (0.98 + 0.04 * np.random.rand()))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            manage_diversity(population)  # Invoke diversity management\n            temp *= cooling_rate\n            neighborhood_radius *= 0.99\n\n        return best_solution", "configspace": "", "generation": 13, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["4beb9527-bdf6-4615-ace9-164c898284a9"], "operator": null, "metadata": {}}
{"id": "3f0b33b2-f25a-40f7-a949-b58c86accc97", "fitness": 0.131033058461792, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer with Adaptive Strategy Control for Enhanced Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 14, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13103 with standard deviation 0.13550.", "error": "", "parent_ids": ["4beb9527-bdf6-4615-ace9-164c898284a9"], "operator": null, "metadata": {"aucs": [0.4930823458972421, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "d8f8ac95-1833-4539-aa68-2ec0d1505ed0", "fitness": 0.09409614199707164, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Enhanced Adaptive Hybrid Optimizer with Dynamic Neighborhood and Self-Adaptive Parameter Control for Improved BBOB Function Performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n                neighborhood_radius = np.random.uniform(0.05, 0.15)\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09410 with standard deviation 0.05013.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.1703790224631011, 0.15343324628860144, 0.08813143045648564, 0.08832700229324719, 0.06860960703630803, 0.03520547752560155, 0.030675463000612746, 0.05715766634847064]}}
{"id": "986e6c56-8df3-44ff-a624-7897ba32dd11", "fitness": 0.09310223285505852, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Strategy and Local Search Intensification for Improved Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        local_search_intensity = 5\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Intensified local search\n        def local_search(ind, fit):\n            best_local = ind\n            best_local_fit = fit\n            for _ in range(local_search_intensity):\n                candidate = ind + np.random.normal(0, neighborhood_radius, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n                candidate_fit = func(candidate)\n                if candidate_fit < best_local_fit:\n                    best_local = candidate\n                    best_local_fit = candidate_fit\n            return best_local, best_local_fit\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                population[i], fitness[i] = local_search(population[i], fitness[i])\n\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09310 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15085619094835379, 0.16610614381970956, 0.15343324628860144, 0.10072716225478529, 0.07940004781166177, 0.06743107064827847, 0.037731059749716134, 0.027699943745730038, 0.05453523042869013]}}
{"id": "168ba9a4-9e5a-4736-a13a-edb2f9261773", "fitness": 0.09145753263149813, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "An enhanced hybrid optimizer with adaptive mutation scaling to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim) * (1 + 0.1 * np.random.rand())  # Slightly changed line\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 17, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09146 with standard deviation 0.04979.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1504851687680271, 0.16610614381970956, 0.15359922689178673, 0.08719477366586381, 0.08327402853019694, 0.0686302282724992, 0.03593538806478691, 0.029229636744444232, 0.04866319892616866]}}
{"id": "81d8d5fc-0d35-413f-899f-a6ac632d0566", "fitness": -Infinity, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Dynamic Adaptive Evolutionary Strategy with Self-Tuning Mechanisms and Diversity Preservation for Enhanced Black Box Optimization Performance.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        max_diversity = calculate_diversity(population)\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n            else:\n                F_base = 0.8\n                Cr_base = 0.9\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n            max_diversity = max(max_diversity, diversity)\n\n        return best_solution", "configspace": "", "generation": 18, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'calculate_diversity' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'calculate_diversity' referenced before assignment\")", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {}}
{"id": "1a38f3dd-4d7b-40b2-916d-51d892553ef8", "fitness": 0.09943940837637173, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Optimizer with Improved Mutation Strategy for Diverse Solutions.  ", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + np.random.normal(0, 0.1))\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 19, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09944 with standard deviation 0.06782.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.23699410011896804, 0.17282051044271074, 0.14456650381593217, 0.08594193586186327, 0.0809294802785071, 0.0812537111479168, 0.037549559648628605, 0.02911980878323861, 0.025779065289580294]}}
{"id": "b65244c6-311d-4879-8ad2-947dea9c4ba4", "fitness": 0.09473969143956103, "name": "EnhancedAdaptiveHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic with Dynamic Exploitation-Exploration Balance for Superior Black Box Optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        exploitation_phase = True\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                exploitation_phase = True\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n            else:\n                exploitation_phase = False\n                neighborhood_radius = min(0.2, neighborhood_radius + 0.05)\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n\n                if not exploitation_phase:\n                    new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                    new_solution = np.clip(new_solution, lb, ub)\n                    new_fitness = func(new_solution)\n                    delta = new_fitness - fitness[i]\n\n                    if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                        population[i] = new_solution\n                        fitness[i] = new_fitness\n                        if new_fitness < best_fitness:\n                            best_solution = new_solution\n                            best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.98\n\n        return best_solution", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedAdaptiveHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09474 with standard deviation 0.05015.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16199652740627546, 0.16610614381970956, 0.1538268264830549, 0.09101385348081326, 0.08316717002579754, 0.07032633490617068, 0.03505734220320367, 0.03157875814186317, 0.05958426648916104]}}
{"id": "17ca9c88-4310-473a-b5f2-7590e59425d0", "fitness": -Infinity, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer with Adaptive Diversity Control and Dynamic Weighting for Enhanced BBOB Performance.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        init_neighborhood_radius = 0.1\n        neighborhood_radius = init_neighborhood_radius\n        diversity_threshold = 0.1\n        adapt_step = 0.01\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F_dynamic = F_base * (1 + adapt_step * (np.random.rand() - 0.5))\n                mutant = pop[a] + F_dynamic * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n                neighborhood_radius = max(0.05, neighborhood_radius * 1.05)\n            else:\n                neighborhood_radius = init_neighborhood_radius * (1 - 0.5 * (diversity / diversity_threshold))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 21, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {}}
{"id": "d9c7b791-4d6d-4dbf-8e96-64dd568c603e", "fitness": 0.11148466625902292, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Incorporate adaptive mutation scaling in differential evolution to enhance exploration.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F_dynamic = F_base * (1 + 0.1 * np.random.rand())  # Adjusted line for adaptive mutation scaling\n                mutant = pop[a] + F_dynamic * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 22, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11148 with standard deviation 0.09139.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.3327152863655146, 0.16805628509661574, 0.15069802229808904, 0.08650985892169205, 0.10069136722728689, 0.06764778217673317, 0.03740215948058945, 0.028656320481649233, 0.03098491428303607]}}
{"id": "3b70423c-c55b-4041-a277-5a3fb48e04d3", "fitness": 0.08827018784817312, "name": "EnhancedMultiStrategyAdaptiveOptimizer", "description": "Enhanced Multi-Strategy Adaptive Metaheuristic Optimizer with Diversity Preservation and Dynamic Parameter Tuning for BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        exploration_weight = 0.5\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        def dual_strategies_selection():\n            if np.random.rand() < exploration_weight:\n                return differential_evolution\n            else:\n                return adaptive_neighborhood_search\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            selected_strategy = dual_strategies_selection()\n            if selected_strategy == differential_evolution:\n                differential_evolution(population, fitness)\n                evaluations += population_size\n            else:\n                for i in range(population_size):\n                    if evaluations >= self.budget:\n                        break\n                    population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                    evaluations += 1\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n                \n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedMultiStrategyAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08827 with standard deviation 0.05337.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15646008031037018, 0.16610614381970956, 0.15001127332073516, 0.0881082448254904, 0.0845842428660809, 0.05873343823342414, 0.03702205610643261, 0.029992627265494676, 0.023413583885820377]}}
{"id": "ec273694-b920-45bc-a174-6228af5c117c", "fitness": 0.08905926224989094, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer integrating dynamic multi-strategy adaptation and adaptive learning for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = min(50, 10 * self.dim)\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        mutation_strategy = 0\n\n        def differential_evolution(pop, fit, strategy):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                \n                if strategy == 0:\n                    mutant = pop[a] + F_base * (pop[b] - pop[c])\n                else:\n                    mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n                mutation_strategy = 1 - mutation_strategy  # Toggle strategy\n\n            differential_evolution(population, fitness, mutation_strategy)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.2 * (1 - evaluations / self.budget), self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08906 with standard deviation 0.05451.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1475615912022744, 0.16610614381970956, 0.15958451870620383, 0.09686656382103376, 0.08247585477992558, 0.07059778126352911, 0.022532907037846495, 0.03310870519398956, 0.022699294424506133]}}
{"id": "1bb3ac69-2bab-4f38-a5cd-30413626346e", "fitness": 0.09097813389776693, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Dynamic Adaptive Strategy Control and Novel Neighborhood Exploration for Superior BBOB Performance. ", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            perturbation = neighborhood_radius * np.random.randn(self.dim)\n            new_solution = ind + perturbation\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n\n            if diversity < diversity_threshold:\n                F_base = np.clip(F_base + 0.1 * (0.5 - np.random.rand()), 0.5, 1.0)\n                Cr_base = np.clip(Cr_base + 0.1 * (0.5 - np.random.rand()), 0.5, 1.0)\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n\n                # Simulated Annealing Exploration\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n            Cr_base = Cr_base * (1 - float(evaluations) / self.budget)\n\n        return best_solution", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09098 with standard deviation 0.05205.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15602853637997038, 0.16610614381970956, 0.15682440384309448, 0.08971761181027538, 0.08028366191804814, 0.06421512662816042, 0.03307179315335296, 0.03572390276769677, 0.03683202475959435]}}
{"id": "e3cff050-153c-45a2-916a-e38e6b0a3e91", "fitness": 0.09280874576836036, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Advanced Hybrid Metaheuristic with Dynamic F Adjustment and Adaptive Cooling for Optimized Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F_dynamic = F_base * (1 + np.random.rand() * 0.1)  # Line changed\n                mutant = pop[a] + F_dynamic * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate * 1.01  # Line changed\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 26, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09281 with standard deviation 0.05366.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16463200194955163, 0.16805628509661574, 0.15069802229808904, 0.08650985892169205, 0.10069136722728689, 0.06764778217673317, 0.03740215948058945, 0.028656320481649233, 0.03098491428303607]}}
{"id": "537512dc-e4a1-4fe8-9b96-4d1d5637e02c", "fitness": 0.09315774806279503, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Strategy with Temperature-Controlled Exploration for Improved Convergence.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, temp * 0.1, self.dim)  # Modified: temp affecting exploration\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 27, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09316 with standard deviation 0.04952.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08801065984467449, 0.08483129667280598, 0.06818800618846688, 0.03528886203027626, 0.030457488810933686, 0.05715766634847064]}}
{"id": "4f9e0b91-2ecb-4127-8aa1-e31447649778", "fitness": 0.09471106148144393, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with slightly adjusted differential mutation scaling for improved convergence.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.85  # Slightly adjusted mutation scaling factor for improvement\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 28, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09471 with standard deviation 0.04682.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15220358144778035, 0.16610614381970956, 0.14722775464137294, 0.0959194663360119, 0.07952568472522259, 0.07905916855074069, 0.04397130155166307, 0.029899185321202615, 0.05848726693929163]}}
{"id": "98098026-4811-47bd-b02d-a3f9119bf7e3", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhance temperature reduction for more dynamic simulated annealing.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.98  # Changed from 0.95 to 0.98 for a slower reduction\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 29, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "1144a0d2-af4b-42fe-be51-422b9ba4c647", "fitness": 0.09017804572585171, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer with Adaptive Strategy Control and Enhanced Mutation Strategy for Improved Exploration on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F_dynamic = F_base * (1 - evaluations / self.budget) # Enhanced mutation strategy\n                mutant = pop[a] + F_dynamic * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 30, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09018 with standard deviation 0.05091.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16676545625537942, 0.15324206832372822, 0.09082162620034784, 0.07974237545832208, 0.0649851030220372, 0.035774920742544625, 0.02962390003839077, 0.0402748716567124]}}
{"id": "4b65265a-3e78-4c6a-a104-5968b8d13aeb", "fitness": 0.08920812847570875, "name": "EnhancedAdaptiveHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic Optimizer with Population Clustering and Temperature-based Diversity Control for Improved Performance on BBOB Functions.", "code": "import numpy as np\nimport random\n\nclass EnhancedAdaptiveHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 15 * self.dim  # Increased population size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.93  # Adjusted cooling rate\n        neighborhood_radius = 0.15  # Increased radius\n        diversity_threshold = 0.15  # Adjusted threshold\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        def population_clustering(pop):\n            clusters = {}\n            for ind in pop:\n                cluster_id = tuple(np.round(ind / neighborhood_radius))\n                if cluster_id not in clusters:\n                    clusters[cluster_id] = []\n                clusters[cluster_id].append(ind)\n            return clusters\n\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            clusters = population_clustering(population)\n            for cluster in clusters.values():\n                if len(cluster) > 1:\n                    random.shuffle(cluster)  # Shuffle within cluster\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedAdaptiveHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08921 with standard deviation 0.05549.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16312473778499925, 0.16741767307256972, 0.1563681592527295, 0.08594193586186327, 0.07940004781166177, 0.06259997875213896, 0.03279456359564015, 0.03175041319032745, 0.023475646959448593]}}
{"id": "843902db-0e25-495c-8fa7-9e7ce63a355f", "fitness": 0.09206919692309269, "name": "EnhancedDynamicMultiPopulationOptimizer", "description": "Enhanced Dynamic Multi-Population Strategy with Adaptive Memory for Improved Diversity and Convergence on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedDynamicMultiPopulationOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        num_populations = 3\n        sub_population_size = population_size // num_populations\n        populations = [np.random.uniform(lb, ub, (sub_population_size, self.dim)) for _ in range(num_populations)]\n        fitness = [np.array([func(ind) for ind in pop]) for pop in populations]\n        memory = []\n        best_solution, best_fitness = None, np.inf\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        memory_size = 5\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(sub_population_size):\n                indices = list(range(sub_population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, 0.1, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            for j in range(num_populations):\n                if evaluations >= self.budget:\n                    break\n\n                diversity = calculate_diversity(populations[j])\n                if diversity < 0.1:\n                    F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                    Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n                differential_evolution(populations[j], fitness[j])\n                evaluations += sub_population_size\n\n                for i in range(sub_population_size):\n                    if evaluations >= self.budget:\n                        break\n\n                    populations[j][i], fitness[j][i] = adaptive_neighborhood_search(populations[j][i], fitness[j][i])\n                    if fitness[j][i] < best_fitness:\n                        best_solution = populations[j][i]\n                        best_fitness = fitness[j][i]\n\n                    new_solution = populations[j][i] + np.random.normal(0, 0.1, self.dim)\n                    new_solution = np.clip(new_solution, lb, ub)\n                    new_fitness = func(new_solution)\n                    delta = new_fitness - fitness[j][i]\n\n                    if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                        populations[j][i] = new_solution\n                        fitness[j][i] = new_fitness\n                        if new_fitness < best_fitness:\n                            best_solution = new_solution\n                            best_fitness = new_fitness\n\n                    evaluations += 1\n\n                temp *= cooling_rate\n\n            # Manage memory and adapt\n            memory.append(best_fitness)\n            if len(memory) > memory_size:\n                memory.pop(0)\n            if len(memory) == memory_size and memory[-1] >= memory[0]:\n                F_base = max(0.5, F_base + 0.1)\n                Cr_base = min(1.0, Cr_base - 0.05)\n\n        return best_solution", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedDynamicMultiPopulationOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09207 with standard deviation 0.04885.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15527443268208363, 0.16610614381970956, 0.1471367286539329, 0.08622649020341566, 0.07940004781166177, 0.07531946964208158, 0.05122176913042009, 0.04018390139352335, 0.027753788971005666]}}
{"id": "4effca9d-9fea-4552-97ca-94467c146887", "fitness": 0.09288724523926135, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Improved Adaptive Strategy with Enhanced Exploration to Boost Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit or np.random.rand() < 0.05:  # allowing some exploration\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 33, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09289 with standard deviation 0.05141.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15355050897554579, 0.16610614381970956, 0.16065398385046836, 0.09465961980569171, 0.08137384996473818, 0.06748219384022813, 0.03770271505051648, 0.02904942052305115, 0.04540677132340287]}}
{"id": "6181d246-35f4-4a57-a1ce-615cdc0ed450", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Improved Adaptive Parameters Control and Diversity Management.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.1 * np.random.rand())  # Reduced adjustment to F_base\n                Cr_base = min(1.0, Cr_base - 0.05 * np.random.rand())  # Reduced adjustment to Cr_base\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 34, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "cc470207-6276-460d-9cca-ad93d24255c9", "fitness": 0.09095721480780986, "name": "EnhancedAdaptiveMetaheuristicOptimizer", "description": "Enhanced Adaptive Metaheuristic Optimizer with Multi-phase Strategy and Dynamic Parameter Adjustment for Efficient Exploration and Exploitation on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * (evaluations / self.budget))\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09096 with standard deviation 0.05228.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15630998867959578, 0.16610614381970956, 0.15633357383025293, 0.09092044971713531, 0.07953008880305223, 0.06641583623509284, 0.032989210751470766, 0.03007424875632514, 0.039935392677654113]}}
{"id": "d399c11f-999a-4145-afcf-0511d681c8de", "fitness": 0.09280874576836036, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer using Adaptive Exploration-Exploitation Balance for Improved Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        exploration_factor = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + exploration_factor * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n                exploration_factor *= 1.1\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09281 with standard deviation 0.05366.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16463200194955163, 0.16805628509661574, 0.15069802229808904, 0.08650985892169205, 0.10069136722728689, 0.06764778217673317, 0.03740215948058945, 0.028656320481649233, 0.03098491428303607]}}
{"id": "8eebdecc-9e6f-43ff-8204-d249f00f5157", "fitness": 0.09445197453593414, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer with Enhanced Local Search via Adaptive Neighborhood Radius.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.15  # Changed from 0.1 to 0.15\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.9  # Changed from 0.95 to 0.9\n\n        return best_solution", "configspace": "", "generation": 37, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09445 with standard deviation 0.05038.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16199652740627546, 0.16610614381970956, 0.1534641072459919, 0.08922958436696682, 0.08316717002579754, 0.07244041835608606, 0.034752334421532804, 0.029208376431426597, 0.05970310874962048]}}
{"id": "e131fb2c-e754-49bb-a8e6-0511ebe432ca", "fitness": 0.09631790083094326, "name": "EnhancedAdaptiveHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic Optimizer with Dynamic Component Blending and Improved Local Search Mechanisms for Superior Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        def blend_with_best(pop, best_sol):\n            blend_rate = 0.1 * np.random.rand()\n            return pop + blend_rate * (best_sol - pop)\n        \n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Blend population with the current best solution\n            population = blend_with_best(population, best_solution)\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Improved local search\n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedAdaptiveHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09632 with standard deviation 0.04745.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15539547767450257, 0.16610614381970956, 0.15343324628860144, 0.09826704887907323, 0.08371193977787317, 0.0728536680961619, 0.04451082987010391, 0.035364740504251535, 0.05721801256821202]}}
{"id": "3081cc68-c23c-4205-8b71-97f4387fbd22", "fitness": 0.09708861871681722, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Diversity-Controlled Hybrid Metaheuristic Optimizer with Adaptive Cooling and Exploration Mechanisms.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.9  # Increased to enhance exploration\n        Cr_base = 0.85  # Adjusted to balance exploration and exploitation\n        temp = 1.0\n        cooling_rate = 0.98  # Improved cooling\n        neighborhood_radius = 0.08  # Modified for better local search\n        diversity_threshold = 0.15  # Tweaked for adaptive diversity control\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.6, F_base + 0.2 * np.random.rand())  # Adjusted bounds\n                Cr_base = min(1.0, Cr_base - 0.05 * np.random.rand())  # Adjusted step\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.12, self.dim)  # Enhanced step size\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate  # Enhanced cooling effect\n            neighborhood_radius *= 0.92  # More profound reduction\n\n        return best_solution", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09709 with standard deviation 0.05289.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.17110475153561688, 0.1671090919969992, 0.16475983437507413, 0.08603195981491885, 0.08171676805094708, 0.07191451521096359, 0.033535405163950815, 0.03266225286957258, 0.0649629894333118]}}
{"id": "54ac4714-597c-4dd5-9214-83c35c03d87d", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced the solution update strategy by incorporating an adaptive learning rate for improved convergence speed.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            adaptive_lr = 1 + 0.1 * np.sin(np.pi * evaluations / self.budget)  # Modified line\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 40, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "306df2e3-055d-4635-bf12-0aa776b3b591", "fitness": 0.08872590182170237, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Dynamic Mechanisms for Improved Convergence and Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.92\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        adapt_rate = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.2 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, adapt_rate, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n            adapt_rate *= 0.98  # Gradually decrease the step size in neighborhood search\n\n        return best_solution", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08873 with standard deviation 0.05050.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15298884666795576, 0.16626823842546923, 0.14569720860301438, 0.08594193586186327, 0.08266767795108054, 0.060437998446145724, 0.03297816041427948, 0.033810931975322145, 0.03774211805019079]}}
{"id": "559bab3d-ed51-4b2f-abeb-96f8f2ca084d", "fitness": 0.09360678324446034, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Introduced a slight increase to the neighborhood radius decay rate to enhance exploration capabilities.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.96  # Slightly increased decay rate\n\n        return best_solution", "configspace": "", "generation": 42, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09361 with standard deviation 0.04974.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16766372489089698, 0.15343324628860144, 0.08738895912640043, 0.08831770252475069, 0.06775285884138682, 0.03520666494002955, 0.03059386367839012, 0.05715766634847064]}}
{"id": "dbaf9daf-5605-4b28-beb0-a483cc1ad9c5", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer with Improved Adaptive Strategy for Enhanced Convergence on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 43, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "aed4471c-44a7-45af-b7e2-0b8c17311286", "fitness": 0.09283058732678941, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Dynamic Neighborhood Search and Adaptive Cooling to Improve Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Adaptive dynamic neighborhood search\n                new_solution = population[i] + np.random.normal(0, neighborhood_radius * (1 - evaluations / self.budget), self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09283 with standard deviation 0.04903.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1544422265287949, 0.16610614381970956, 0.1536582078392945, 0.08594193586186327, 0.08146389344253502, 0.06988020792572658, 0.037709065253484275, 0.03644452041062607, 0.04982908485907045]}}
{"id": "16237a57-8ec8-42ed-833c-fd5a9fab7535", "fitness": 0.09554149431173894, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Advanced Hybrid Metaheuristic Optimizer by slightly increasing the neighborhood exploration rate for better diversity handling.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.12, self.dim)  # Slightly increased exploration rate\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 45, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09554 with standard deviation 0.05164.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16361118950977394, 0.16610614381970956, 0.1615756707508319, 0.08908069070365421, 0.08540520140973051, 0.07050015105349394, 0.03659471056068575, 0.030004257554708924, 0.056995433443061794]}}
{"id": "b447061a-6c0c-4300-845f-4142ae7daa41", "fitness": 0.09544702678065886, "name": "EnhancedDualPhaseMetaheuristicOptimizer", "description": "Enhanced Dual-Phase Metaheuristic Optimizer with Adaptive Diversity Control and Simulated Annealing Strategy for Improved Performance.", "code": "import numpy as np\n\nclass EnhancedDualPhaseMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.9\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        exploration_phase_duration = int(self.budget * 0.6)\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if evaluations < exploration_phase_duration:\n                if diversity < diversity_threshold:\n                    F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                    Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n                differential_evolution(population, fitness)\n                evaluations += population_size\n            else:\n                for i in range(population_size):\n                    if evaluations >= self.budget:\n                        break\n                    population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                    if fitness[i] < best_fitness:\n                        best_solution = population[i]\n                        best_fitness = fitness[i]\n                    \n                    new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                    new_solution = np.clip(new_solution, lb, ub)\n                    new_fitness = func(new_solution)\n                    delta = new_fitness - fitness[i]\n\n                    if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                        population[i] = new_solution\n                        fitness[i] = new_fitness\n                        if new_fitness < best_fitness:\n                            best_solution = new_solution\n                            best_fitness = new_fitness\n\n                    evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedDualPhaseMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09545 with standard deviation 0.04549.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15057153904080622, 0.16610614381970956, 0.15343324628860144, 0.08801065819031784, 0.08101554663103383, 0.07230674808240167, 0.047078479129294304, 0.05185630164876731, 0.04864457819499757]}}
{"id": "11ebcc16-42c8-4043-80b8-8fa104917f7e", "fitness": 0.09342073674297177, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Slightly adjusted the neighborhood radius decay rate to enable more adaptable exploration during adaptive neighborhood search.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.975  # Adjusted from 0.95 to 0.975\n\n        return best_solution", "configspace": "", "generation": 47, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09342 with standard deviation 0.04952.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08946053193636838, 0.08666844684552222, 0.06718389871912367, 0.035206246295759525, 0.030624087871974126, 0.05715766634847064]}}
{"id": "7cd42528-1344-45df-a732-1de56af85a6c", "fitness": 0.08889823132585903, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Differential Evolution by introducing a dynamic scaling factor based on diversity to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            diversity = calculate_diversity(pop)\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F_dynamic = F_base * (1 + diversity)\n                mutant = pop[a] + F_dynamic * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())  # Changed line\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 48, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08890 with standard deviation 0.05382.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15236140818118737, 0.1778877041919924, 0.14334954597914085, 0.08709288771802182, 0.07940004781166177, 0.07277814109570768, 0.03429500332606494, 0.029994488117627505, 0.022924855511326947]}}
{"id": "0ca9aba3-127a-4442-8037-835438d26afe", "fitness": 0.09570410845551952, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Learning Rates and Dynamic Population Size for Improved BBOB Performance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        learning_rate_decay = 0.995\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base * learning_rate_decay + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base * learning_rate_decay - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n            population_size = int(10 * self.dim * (1 - evaluations / self.budget))\n\n        return best_solution", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09570 with standard deviation 0.04753.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1504851687680271, 0.16948320846905018, 0.1539777081094431, 0.09627054439591254, 0.08483129667280598, 0.06846029905884954, 0.05111624740499143, 0.03437701209948918, 0.05233549112110669]}}
{"id": "5136f9c2-e350-4e47-8613-d363cb6744f5", "fitness": 0.09333896528179361, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Improved Diversity and Parameter Adaptation Strategy.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.15  # Changed from 0.1 to 0.15\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.25 * np.random.rand())  # Changed increment from 0.2 to 0.25\n                Cr_base = min(1.0, Cr_base - 0.15 * np.random.rand())  # Changed decrement from 0.1 to 0.15\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.97  # Changed from 0.95 to 0.97\n\n        return best_solution", "configspace": "", "generation": 50, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09334 with standard deviation 0.04941.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08594193586186327, 0.08666844684552222, 0.06997653370139989, 0.035206389364536905, 0.0306139627448222, 0.05715766634847064]}}
{"id": "00701043-8a29-4dd6-8c3e-b43c78d19aec", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Differential Evolution with Dynamic Constraints for Improved Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 51, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "5da092c7-64b8-4813-a794-b5585f085b59", "fitness": 0.09409614199707164, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced exploration by adjusting neighborhood radius based on diversity for improved convergence.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 1 + 0.1 * (diversity < diversity_threshold)\n\n        return best_solution", "configspace": "", "generation": 52, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09410 with standard deviation 0.05013.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.1703790224631011, 0.15343324628860144, 0.08813143045648564, 0.08832700229324719, 0.06860960703630803, 0.03520547752560155, 0.030675463000612746, 0.05715766634847064]}}
{"id": "96917d00-4b66-4cf6-8932-9b1bbe9ef8b8", "fitness": 0.09346239364667804, "name": "EnhancedHybridOptimizer", "description": "Enhanced Hybrid Optimizer with Adaptive Learning Rates and Dynamic Neighborhood Adjustments for Improved Convergence on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        alpha = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + alpha * np.random.rand())\n                Cr_base = min(1.0, Cr_base - alpha * np.random.rand())\n                neighborhood_radius *= 1.1  # Increase exploration\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "90c3bae5-5c9a-4110-8d45-92c262fe9459", "fitness": 0.088328536766187, "name": "ImprovedHybridMetaheuristicOptimizer", "description": "Hybrid Metaheuristic with Adaptive Mutation and Learning Rate to Enhance Convergence and Diversity in BBOB Optimization.", "code": "import numpy as np\n\nclass ImprovedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F_dynamic = F_base * (1 + 0.5 * np.random.rand() * (1 - fit[i] / best_fitness))\n                mutant = pop[a] + F_dynamic * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 54, "feedback": "The algorithm ImprovedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08833 with standard deviation 0.05198.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15190180625280314, 0.16610614381970956, 0.1483433583854532, 0.08600862904708007, 0.08348199588287686, 0.06852183656813948, 0.03667506154395728, 0.027604569510557675, 0.026313429885105766]}}
{"id": "d24dc402-6190-487a-a780-cc99f7a1ecad", "fitness": -Infinity, "name": "EnhancedCoevolutionaryOptimizer", "description": "Enhanced Coevolutionary Strategy with Multi-Scale Adaptive Dynamics for Improved Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedCoevolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.9\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.2\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        def coevolutionary_update(pop, fit):\n            nonlocal F_base, Cr_base\n            subpopulations = np.array_split(pop, 2)\n            subfitness = np.array_split(fit, 2)\n            for subpop, subfit in zip(subpopulations, subfitness):\n                differential_evolution(subpop, subfit)\n\n            # Merge subpopulations and introduce coevolutionary effects\n            for i in range(2):\n                for j in range(len(subpopulations[i])):\n                    if np.random.rand() < 0.5:\n                        subpopulations[i][j] += 0.5 * (best_solution - subpopulations[i][j])\n                        subpopulations[i][j] = np.clip(subpopulations[i][j], lb, ub)\n                        subfitness[i][j] = func(subpopulations[i][j])\n\n            population[:len(subpopulations[0])] = subpopulations[0]\n            population[len(subpopulations[0]):] = subpopulations[1]\n            fitness[:len(subfitness[0])] = subfitness[0]\n            fitness[len(subfitness[0]):] = subfitness[1]\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            coevolutionary_update(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 55, "feedback": "An exception occurred: IndexError('index 92 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 92 is out of bounds for axis 0 with size 50')", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {}}
{"id": "93da8b2c-d07a-4802-99f8-9758d3b899fb", "fitness": 0.0980930704241229, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Mutation Factor Scaling for Improved Convergence.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                F_dynamic = F_base * (1 + np.random.rand() * 0.2)  # Change 1\n                mutant = pop[a] + F_dynamic * (pop[b] - pop[c])  # Change 2\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 56, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09809 with standard deviation 0.06724.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.23920951387921274, 0.16610614381970956, 0.1461103287322545, 0.08594193586186327, 0.07940004781166177, 0.061460678529332013, 0.033008459955369496, 0.03382029124803432, 0.03778023397966834]}}
{"id": "c8ee81ac-815f-4894-8213-45a60357a105", "fitness": 0.09265465648879162, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Cooling and Dynamic Neighborhoods for Improved Convergence on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.98  # Adjusted cooling rate for fine-grained cooling\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.15  # Increased threshold for triggering adaptation\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            return (new_solution, new_fitness) if new_fitness < fit else (ind, fit)\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.uniform(-1, 1))  # More dynamic adjustment\n                Cr_base = min(1.0, Cr_base + 0.1 * np.random.uniform(-1, 1))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.05, self.dim)  # Smaller perturbations\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.9  # More aggressive reduction to enhance convergence\n\n        return best_solution", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09265 with standard deviation 0.04974.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15350743836135539, 0.16610614381970956, 0.15541811969527508, 0.09023929518142604, 0.08074571326100799, 0.06744515789187422, 0.03804851584016489, 0.03202474053396964, 0.05035678381434172]}}
{"id": "75ea04df-a37c-4b2f-b91b-1b445059c69a", "fitness": 0.08934533222956337, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic with Improved Mutation Strategy for Robust Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * (np.random.rand() - 0.5))  # Modified mutation strategy\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 58, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08935 with standard deviation 0.05409.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16242141027417, 0.16610614381970956, 0.15300239564780416, 0.08709288771802182, 0.07977772561414553, 0.06289547903333725, 0.03418953623866283, 0.027769574415791642, 0.03085283730442745]}}
{"id": "7e8f5462-cba9-4866-af66-a200d4e409bb", "fitness": -Infinity, "name": "EnhancedAdaptiveHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic Optimizer with Dynamic Learning and Multi-Strategy Integration for Superior BBOB Performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        learning_rate = 0.1  # New parameter for adaptive learning\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        def multi_strategy_integration(pop, fit):\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                rand_choice = np.random.rand()\n                if rand_choice < 0.5:\n                    pop[i], fit[i] = adaptive_neighborhood_search(pop[i], fit[i])\n                else:\n                    new_solution = pop[i] + np.random.normal(0, 0.1, self.dim)\n                    new_solution = np.clip(new_solution, lb, ub)\n                    new_fitness = func(new_solution)\n                    delta = new_fitness - fit[i]\n\n                    if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                        pop[i] = new_solution\n                        fit[i] = new_fitness\n                        if new_fitness < best_fitness:\n                            best_solution = new_solution\n                            best_fitness = new_fitness\n\n                evaluations += 1\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            multi_strategy_integration(population, fitness)\n\n            # Adaptive learning rate update\n            F_base += learning_rate * (np.random.rand() - 0.5)\n            Cr_base += learning_rate * (np.random.rand() - 0.5)\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 59, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'evaluations' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'evaluations' referenced before assignment\")", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {}}
{"id": "5ce35252-92f6-448e-ada5-268aac3ddf6d", "fitness": 0.11045653095274276, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Incorporate a probability factor to enhance the mutation strategy's adaptability.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                adapt_factor = np.random.rand()  # New line added\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand()) * adapt_factor\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 60, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.11046 with standard deviation 0.09175.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.3302885037163008, 0.16610614381970956, 0.1632433720176597, 0.09038533649109304, 0.07940004781166177, 0.06751789754705118, 0.03836659335292436, 0.031169018179445507, 0.027631865638838904]}}
{"id": "07346a62-c14d-4abc-bb8e-4d91eab5b5d9", "fitness": 0.09346239364667804, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Diversity Control and Dynamic Parameter Tuning for Improved Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.3 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.2 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "d19eca3d-5747-4f99-a78f-849fe4629aca", "fitness": 0.09346239364667804, "name": "SynergisticAdaptiveMetaheuristicOptimizer", "description": "Synergistic Adaptive Metaheuristic Optimizer (SAMO) with Improved Diversity Control and Adaptive Cooling for Enhanced Search Efficiency on BBOB Functions.", "code": "import numpy as np\n\nclass SynergisticAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.98  # Enhanced cooling rate\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.15  # Adjusted diversity threshold\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand() * (1.0 - evaluations / self.budget))\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 62, "feedback": "The algorithm SynergisticAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "eeb92296-9656-4700-82c4-4a8d9f141a30", "fitness": 0.09468338631978448, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhancing convergence by slightly adjusting the neighborhood search parameter for better local exploration.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.15  # Adjusted from 0.1 to 0.15\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 63, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09468 with standard deviation 0.05048.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16199652740627546, 0.16610614381970956, 0.15343324628860144, 0.09380318421734901, 0.08316717002579754, 0.07001326067055769, 0.034750959060240394, 0.02921135594869262, 0.05966862944083662]}}
{"id": "d751c216-3bd0-46d1-9d69-4ea270df3db8", "fitness": 0.09565714102841968, "name": "EnhancedAdaptiveHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic Optimizer with Improved Diversity Management and Local Search for Optimal BBOB Function Performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def local_search(ind, fit):\n            best_local_solution = ind\n            best_local_fitness = fit\n            for _ in range(5):  # Conduct a quick local search\n                candidate = ind + np.random.normal(0, 0.05, self.dim)\n                candidate = np.clip(candidate, lb, ub)\n                candidate_fitness = func(candidate)\n                if candidate_fitness < best_local_fitness:\n                    best_local_solution = candidate\n                    best_local_fitness = candidate_fitness\n            return best_local_solution, best_local_fitness\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                population[i], fitness[i] = local_search(population[i], fitness[i])\n                \n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedAdaptiveHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09566 with standard deviation 0.05360.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.17035393564648538, 0.16610614381970956, 0.16529277442347434, 0.08802828620479297, 0.080956761579149, 0.06616549928921578, 0.03844654225868094, 0.027637710937492344, 0.05792661509677688]}}
{"id": "43109c7f-3d2b-4337-86bb-30dfe900a0a3", "fitness": 0.09346239364667804, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Dynamic Population Size and Self-Adaptive Parameters to Improve Convergence on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (initial_population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = initial_population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        dynamic_population_factor = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit, current_pop_size):\n            nonlocal F_base, Cr_base\n            for i in range(current_pop_size):\n                indices = list(range(current_pop_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        current_population_size = initial_population_size\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n                current_population_size = int(initial_population_size * (1 + dynamic_population_factor * np.random.rand()))\n\n            differential_evolution(population, fitness, current_population_size)\n            evaluations += current_population_size\n\n            for i in range(current_population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "6aef0661-8fc7-4ee0-b58b-8f15b9412e3f", "fitness": 0.08934533222956344, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer with Enhanced Differential Evolution Strategy for Improved Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                # Changed to use a more adaptive F scaling\n                F_dynamic = F_base * (1 + 0.5 * np.random.rand() - 0.25) \n                mutant = pop[a] + F_dynamic * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 66, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08935 with standard deviation 0.05409.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16242141027417, 0.16610614381970956, 0.15300239564780416, 0.08709288771802182, 0.07977772561414553, 0.06289547903333792, 0.03418953623866283, 0.027769574415791642, 0.03085283730442745]}}
{"id": "d356b35e-0eed-4ee7-ae3e-92109c166b69", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Improved Adaptive Strategy by Enhancing the Cooling Rate Dynamics for Better Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.98  # Increased cooling rate for faster adaptation\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 67, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "ebb24dd2-6d76-4028-9d35-6f943bcc5508", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced the adaptive strategy control by adjusting the cooling rate to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.90  # Adjusted cooling rate from 0.95 to 0.90 for better performance\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 68, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "33feb19e-fba7-4e7d-82f2-ec9d70b43aa3", "fitness": 0.09406554655631365, "name": "AdaptiveHybridMetaheuristicOptimizer", "description": "Adaptive Hybrid Metaheuristic Optimizer with Multi-Strategy Reinforcement and Dynamic Parameter Calibration for Enhanced Flexibility on BBOB Functions.", "code": "import numpy as np\n\nclass AdaptiveHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        exploration_weight = 0.5\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Adaptive exploration/exploitation balance\n                if np.random.rand() < exploration_weight:\n                    new_solution = population[i] + np.random.normal(0, 0.2, self.dim)\n                else:\n                    new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                \n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n            exploration_weight *= 0.98  # Gradually shift towards exploitation\n\n        return best_solution", "configspace": "", "generation": 69, "feedback": "The algorithm AdaptiveHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09407 with standard deviation 0.05080.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15627569006035635, 0.171109040522609, 0.15654870297876167, 0.09021662320247203, 0.08244128417961982, 0.06545371317624993, 0.03783039643733921, 0.032287634346083505, 0.05442683410333127]}}
{"id": "e9d381c4-6b3e-4cb4-a7fd-5680f3dc07dc", "fitness": 0.09323756233263711, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Strategy Control by Modifying Mutation Factor to Enhance Exploration.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + np.random.rand())  # Modified Mutation Factor\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 70, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09324 with standard deviation 0.04784.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.15215383215687917, 0.09203099708868168, 0.07940004781166177, 0.06742505322302594, 0.04800048024882164, 0.02845145782738079, 0.055197958982370565]}}
{"id": "40b49a4f-f71d-45f5-8b78-05c1ef02df27", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Introduce a small improvement by dynamically adjusting the cooling rate based on budget utilization to enhance the convergence speed of the algorithm.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate * (1 - evaluations / self.budget) # Change here\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 71, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "ab881e6e-9dda-46ab-8bad-b8c6fcdeadf6", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Improve adaptive parameter adjustment by dynamically altering the mutation factor and crossover probability based on diversity changes.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.3 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.15 * np.random.rand())  # Changed line for dynamic adjustment\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 72, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "897c1527-a2a4-4221-af51-e665c6556a11", "fitness": 0.09091475464064774, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhance exploration by dynamically adjusting mutation factor (F_base) based on diversity.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n                \n            # New line added to dynamically adjust F_base\n            F_base = 0.5 + 0.3 * (1 - diversity / diversity_threshold)\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 73, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09091 with standard deviation 0.05635.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16180342021947247, 0.16610614381970956, 0.16522176764351182, 0.08594193586186327, 0.08483129667280598, 0.07008314369549296, 0.032778548230690596, 0.027760005337933435, 0.02370653028434966]}}
{"id": "8ef65835-ae75-4f2c-9469-676458496ae3", "fitness": 0.09266770197739392, "name": "EnhancedAdaptiveHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic Optimizer with Improved Convergence Control for Superior Performance on BBOB Functions by Integrating Novel Diversity and Stability Mechanisms.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        stability_factor = 0.5\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Introduce stability control mechanism\n                stability_control = 1.0 / (1.0 + np.exp(-stability_factor * (evaluations / self.budget - 0.5)))\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n\n                new_solution = population[i] + np.random.normal(0, 0.1 * stability_control, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedAdaptiveHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09267 with standard deviation 0.04886.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15113992002319676, 0.16610614381970956, 0.15343324628860144, 0.08659952827208406, 0.08301608722286902, 0.07357314032381557, 0.037980793005202296, 0.031823325177668726, 0.05033713366339787]}}
{"id": "928061c4-3429-418e-bf2b-2fdc95daa57b", "fitness": 0.08983394920630612, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Strategy and Dynamic Population Control for Improved Performance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Dynamic Population Adjustment\n            if evaluations < self.budget * 0.5 and np.random.rand() < 0.05:\n                indices = np.argsort(fitness)\n                population = population[indices[:population_size // 2]]\n                fitness = fitness[indices[:population_size // 2]]\n                new_individuals = np.random.uniform(lb, ub, (population_size // 2, self.dim))\n                new_fitnesses = np.array([func(ind) for ind in new_individuals])\n                population = np.vstack((population, new_individuals))\n                fitness = np.hstack((fitness, new_fitnesses))\n                evaluations += population_size // 2\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08983 with standard deviation 0.04984.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.14810567600033153, 0.08895515534580811, 0.08144796124862286, 0.06738765702357064, 0.036668805830142426, 0.034626873219167686, 0.03483518053419943]}}
{"id": "a5de2970-a1b7-426d-86ce-40b0758e882c", "fitness": 0.09103075894469263, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Self-Adaptive Differential Evolution and Dynamic Neighborhood Scaling for Improved BBOB Function Optimization.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n                else:\n                    # Self-adaptive modification\n                    F_base = np.random.uniform(0.5, 1)\n                    Cr_base = np.random.uniform(0, 1)\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                neighborhood_radius = min(0.5, neighborhood_radius * 1.1)\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09103 with standard deviation 0.05250.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15719474950306178, 0.16610614381970956, 0.15007961928756852, 0.09411053441671746, 0.08045560270178609, 0.08043013832325285, 0.03302957074397195, 0.029501246910588308, 0.02836922479557713]}}
{"id": "88821567-5f2f-4456-8672-d476b93b10cb", "fitness": 0.08984640334492267, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic Optimizer with Temperature-Adjusted Mutation and Improved Diversity Control for Robust Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base, temp  # Added temp to the nonlocal scope\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                # Adjusted mutation strategy with temperature influence\n                F_temp = F_base * (1 + 0.5 * (1 - np.tanh(temp)))\n                mutant = pop[a] + F_temp * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n                temp *= (1 + 0.1 * np.random.rand())  # Added temperature adjustment\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 77, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08985 with standard deviation 0.05276.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1586221410857518, 0.16610614381970956, 0.15356247351014363, 0.08599707197540252, 0.08058904540035716, 0.06447670950128759, 0.03242013403917121, 0.029311060982954884, 0.037532849789525646]}}
{"id": "c8592688-2280-4fe8-95bd-ed859bf0281b", "fitness": 0.09397506847372751, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Dynamic Multi-phase Strategy and Recalibrated Adaptive Parameters for Improved Convergence on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n        \n        def elite_selection(pop, fit):\n            elite_indices = np.argsort(fit)[:max(1, population_size // 5)]\n            return pop[elite_indices], fit[elite_indices]\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            if evaluations < self.budget * 0.5:  # Introduce elite selection in early phase\n                elite_pop, elite_fit = elite_selection(population, fitness)\n                for elite_ind, elite_fit_ind in zip(elite_pop, elite_fit):\n                    if evaluations >= self.budget:\n                        break\n                    elite_ind, elite_fit_ind = adaptive_neighborhood_search(elite_ind, elite_fit_ind)\n                    if elite_fit_ind < best_fitness:\n                        best_solution = elite_ind\n                        best_fitness = elite_fit_ind\n                    evaluations += 1\n            \n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09398 with standard deviation 0.05149.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15614351880478372, 0.1706159090049466, 0.15849269522136467, 0.08962243864159358, 0.08384357293132338, 0.06901185150007416, 0.03836309365091428, 0.02835925592857036, 0.051323280579976815]}}
{"id": "1bf6899d-abf3-4ff3-b6c7-9754e15702c8", "fitness": 0.0926685494575913, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Parameter Tuning and Stochastic Perturbation for Robust Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.9\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        def stochastic_perturbation(ind):\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)\n            return np.clip(ind + perturbation, lb, ub)\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = stochastic_perturbation(population[i])\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09267 with standard deviation 0.05173.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.16124359984341086, 0.16610614381970956, 0.15610506381695055, 0.08629090081957913, 0.08325481979060412, 0.06478361348580397, 0.037873578147443854, 0.026348698134389226, 0.05201052726043043]}}
{"id": "17e612af-e337-42d1-9c1f-ebe3cfa967b2", "fitness": 0.09586703573455407, "name": "EnhancedAdaptiveHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Hybrid Metaheuristic Optimizer with Dynamic Parameter Adjustment and Multi-phase Search Strategy for Improved BBOB Performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        def adaptive_parameter_adjustment():\n            nonlocal F_base, Cr_base, neighborhood_radius\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n            else:\n                neighborhood_radius = max(0.05, neighborhood_radius * 1.05)\n\n        # Search loop\n        while evaluations < self.budget:\n            adaptive_parameter_adjustment()\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedAdaptiveHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09587 with standard deviation 0.05381.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.17646992546083595, 0.16610614381970956, 0.16065769193167434, 0.08639691263782501, 0.07955694103292654, 0.07049276697286844, 0.03476663592792251, 0.03092861723770679, 0.05742768658951758]}}
{"id": "1b3d7d16-2099-4a69-bd77-3a381141dafc", "fitness": 0.0931205248463335, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Diversity and Temperature Control to Boost Exploration and Exploitation Balance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        diversity_boost = 0.2\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + diversity_boost * np.random.rand())\n                Cr_base = min(1.0, Cr_base - diversity_boost * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate * (1 - diversity / diversity_threshold)\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09312 with standard deviation 0.04954.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08608917664406068, 0.08666844684552222, 0.0681188940717189, 0.03499081983559915, 0.03057396720210248, 0.05715766634847064]}}
{"id": "70ad0a40-b166-4bde-a8d1-64a94eae7135", "fitness": 0.09075398207273253, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Dynamic Adaptation and Parallel Strategy Integration for Black Box Optimization.", "code": "import numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            with ThreadPoolExecutor() as executor:\n                futures = []\n                for i in range(population_size):\n                    futures.append(executor.submit(de_worker, i, pop, fit))\n                for f in futures:\n                    f.result()\n\n        def de_worker(i, pop, fit):\n            indices = list(range(population_size))\n            indices.remove(i)\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n            mutant = np.clip(mutant, lb, ub)\n            Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n            cross_points = np.random.rand(self.dim) < Cr_dynamic\n            trial = np.where(cross_points, mutant, pop[i])\n            trial_fitness = func(trial)\n\n            if trial_fitness < fit[i]:\n                pop[i] = trial\n                fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 82, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09075 with standard deviation 0.05015.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15038449241317298, 0.16610614381970956, 0.15343324628860144, 0.08744316179859057, 0.08036340373309536, 0.0668466990226535, 0.035004627352914874, 0.027749958788309437, 0.049454105437545026]}}
{"id": "888c7700-52eb-4df2-b8d9-7c7e6ddd3b39", "fitness": 0.09346239364667804, "name": "EnhancedMultiStageMetaheuristicOptimizer", "description": "Enhanced Multi-Stage Metaheuristic Optimizer with Adaptive Diversity Control and Focused Exploitation for Optimized Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedMultiStageMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        exploitation_factor = 0.5  # New parameter for focused exploitation\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n                if np.random.rand() < exploitation_factor:\n                    focus_idx = np.random.choice(np.arange(population_size))\n                    population[focus_idx], fitness[focus_idx] = adaptive_neighborhood_search(\n                        population[focus_idx], fitness[focus_idx])\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedMultiStageMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "e38a5cc4-795e-40a0-a99c-951063ad665a", "fitness": 0.09342073674297177, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Minimized neighborhood radius decay to maintain exploration longer in the search process.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.975  # Changed from 0.95 to 0.975\n\n        return best_solution", "configspace": "", "generation": 84, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09342 with standard deviation 0.04952.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08946053193636838, 0.08666844684552222, 0.06718389871912367, 0.035206246295759525, 0.030624087871974126, 0.05715766634847064]}}
{"id": "357ebfe5-4e12-46a3-877c-bc132c6fa15a", "fitness": 0.09394624628784969, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Strategy and Self-Learning Mechanism for Improved Diversification and Exploitation Balance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        learning_rate = 0.05\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        def self_learning_adjustment(diversity):\n            return min(1.0, max(0.5, 1.0 - diversity * learning_rate))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= self_learning_adjustment(diversity)\n\n        return best_solution", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09395 with standard deviation 0.04996.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15719078342751625, 0.08736040875626971, 0.08805846227581415, 0.0690927754969678, 0.03520869434162344, 0.030394919563059175, 0.05715766634847064]}}
{"id": "59fbc798-c25d-421e-9cd6-c554fb7c6a70", "fitness": 0.09174338959105335, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Diversification and Intensification for Improved Convergence on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n\n                if np.random.rand() > 0.5:  # Stochastic intensification\n                    new_solution = population[i] + np.random.normal(0, 0.05, self.dim)\n                else:  # Diversification\n                    new_solution = np.random.uniform(lb, ub, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09174 with standard deviation 0.05017.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1504851687680271, 0.16610614381970956, 0.15561150585589523, 0.09200976756476908, 0.07973006095417023, 0.06766902604501646, 0.03752574213561688, 0.028014318342026057, 0.048538772834249566]}}
{"id": "41808d7b-e646-4e51-868a-c7cb48208b79", "fitness": 0.08912318465520634, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer using Synergistic Strategy Integration and Dynamic Parameter Tuning for Robust BBOB Performance.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.98\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c])\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_tabu_search(ind, fit):\n            candidates = [ind + np.random.normal(0, neighborhood_radius, self.dim) for _ in range(3)]\n            candidates = [np.clip(c, lb, ub) for c in candidates]\n            candidates_fitness = [func(c) for c in candidates]\n            best_candidate_idx = np.argmin(candidates_fitness)\n            if candidates_fitness[best_candidate_idx] < fit:\n                return candidates[best_candidate_idx], candidates_fitness[best_candidate_idx]\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = np.random.uniform(0.5, 1.0)\n                Cr_base = np.random.uniform(0.7, 1.0)\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_tabu_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n\n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.9\n\n        return best_solution", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08912 with standard deviation 0.05086.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.14866339056127886, 0.09537854661641587, 0.07985718917107165, 0.05876561956824622, 0.03512837061429053, 0.03395558342545035, 0.03388172828519109]}}
{"id": "4b1854eb-0b44-4493-83af-90d25c471bf6", "fitness": 0.09327959249534881, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Reinforced Adaptive Cooling for BBOB Optimization.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.92  # Changed from 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.93  # Changed from 0.95\n\n        return best_solution", "configspace": "", "generation": 88, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09328 with standard deviation 0.04951.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08760998852843849, 0.08666844684552222, 0.06785228324606385, 0.03520740825534674, 0.030534786564769933, 0.05715766634847064]}}
{"id": "0f8f0a6f-ef21-4657-8584-5c339161a3ad", "fitness": 0.08831100635262086, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Parameters and Dynamic Strategy Adjustment to Improve Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        exploration_exploitation_balance = 0.5\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n            else:\n                F_base = max(0.6, F_base - 0.1 * np.random.rand())\n                Cr_base = min(0.9, Cr_base + 0.05 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n\n                if np.random.rand() < exploration_exploitation_balance:\n                    population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n\n                new_solution = population[i] + np.random.normal(0, 0.1 * (1 - evaluations / self.budget), self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08831 with standard deviation 0.05196.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549054512375435, 0.16610614381970956, 0.14236411354057366, 0.08594193586186327, 0.08587565665401997, 0.07248151611035525, 0.035810474428795036, 0.02894070667508697, 0.02237305884564056]}}
{"id": "03cec3b6-9a2e-4af9-bf1e-7033fe310ea1", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Refined Adaptive Hybrid Metaheuristic Optimizer with Enhanced Diversity Management for Improved Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand() * (diversity_threshold - diversity))  # Adjusted line\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 90, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "ecfdf858-c300-4ee0-985d-025c39bcfca9", "fitness": -Infinity, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Memory and Niche Preservation for Improved Diversity and Convergence in BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n        memory = np.copy(population)\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Niche Preservation\n        def niche_preservation(memory, niche_radius=0.05):\n            niches = []\n            for ind in memory:\n                for niche in niches:\n                    if np.linalg.norm(ind - niche) < niche_radius:\n                        break\n                else:\n                    niches.append(ind)\n            return niches\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n            niches = niche_preservation(memory)\n            for niche in niches:\n                niche_solution = niche + np.random.normal(0, niche_radius, self.dim)\n                niche_solution = np.clip(niche_solution, lb, ub)\n                niche_fitness = func(niche_solution)\n                if niche_fitness < best_fitness:\n                    best_solution = niche_solution\n                    best_fitness = niche_fitness\n\n        return best_solution", "configspace": "", "generation": 91, "feedback": "An exception occurred: NameError(\"name 'niche_radius' is not defined\").", "error": "NameError(\"name 'niche_radius' is not defined\")", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {}}
{"id": "88dbf498-3963-40ff-875d-6ff25660af69", "fitness": 0.1270688927434571, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "An Enhanced Hybrid Metaheuristic Optimizer with Dynamic Adaptive Strategies and Self-Adjusting Parameters for Superior Convergence on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget)\n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        def adjust_parameters(diversity):\n            nonlocal F_base, Cr_base, neighborhood_radius\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.3 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.2 * np.random.rand())\n                neighborhood_radius = min(0.5, neighborhood_radius + 0.05)\n            else:\n                F_base = max(0.5, F_base - 0.1 * np.random.rand())\n                Cr_base = min(1.0, Cr_base + 0.1 * np.random.rand())\n                neighborhood_radius = max(0.05, neighborhood_radius - 0.05)\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            adjust_parameters(diversity)\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                # Self-adaptive local search\n                new_solution = population[i] + np.random.normal(0, neighborhood_radius, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n\n        return best_solution", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12707 with standard deviation 0.13731.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.49169294524905494, 0.16610614381970956, 0.1483702139985167, 0.09950059841801917, 0.08336977604464046, 0.06318430557563792, 0.03247253635980363, 0.026863141348998032, 0.03206037387673355]}}
{"id": "ada93205-379e-41b7-b6ed-123e6c919dde", "fitness": 0.08627110856306822, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Dynamic Population Sizing and Adaptive Cooling for Improved Convergence on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.98  # Changed for slower cooling\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n\n                # Dynamic mutation scaling\n                if evaluations / self.budget < 0.5:\n                    mutant *= (1 + 0.1 * np.random.randn())\n\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            # Dynamic decreasing of population size\n            if evaluations / self.budget > 0.7:\n                population_size = max(5, population_size // 2)\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08627 with standard deviation 0.05385.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.158485914912895, 0.16610614381970956, 0.14497866060400277, 0.08652651067497563, 0.07965366386176409, 0.0545091725787904, 0.034729414638779876, 0.028789038328200012, 0.022661457648496586]}}
{"id": "d0261459-ab00-4e31-a48e-536bd304ff25", "fitness": 0.0917939516003194, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Adaptive Strategy Control and Dynamic Neighborhood Exploration for Improved Performance on BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        restart_threshold = 0.05\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            radius_adjusted = neighborhood_radius * (1 + 0.5 * np.random.rand()) \n            new_solution = ind + np.random.normal(0, radius_adjusted, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 94, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09179 with standard deviation 0.04931.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1504851687680271, 0.16610614381970956, 0.15357214163262467, 0.08640814416663634, 0.08129876238966405, 0.07022397871364205, 0.03585100098745586, 0.030558382786221028, 0.05164184113889403]}}
{"id": "8a61988c-636d-4dfd-b9de-64ba41c40f47", "fitness": 0.0934360071569908, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Adaptive Strategy Control with Dynamic Parameter Adjustment for Improved Diversity Retention.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.99\n\n        return best_solution", "configspace": "", "generation": 95, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09344 with standard deviation 0.04950.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15350175690092704, 0.087133243666356, 0.08864774735149128, 0.06757058426356, 0.03520579578107452, 0.030654763720111777, 0.05715766634847064]}}
{"id": "5bb37b42-9405-4852-97ff-b307a663167d", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced Diversity Control by Adjusting Population Size Dynamically for Improved Convergence in BBOB Functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n            # Adjust population size dynamically\n            if diversity < diversity_threshold:\n                population_size += 1\n\n        return best_solution", "configspace": "", "generation": 96, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
{"id": "34945d02-a922-4a0d-a232-169c270c73b9", "fitness": 0.0892981521164712, "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with Multi-Phase Adaptive Strategy Control and Diversity-Driven Mutation for Superior Performance on BBOB Functions.", "code": "import numpy as np\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n        mutation_strength = 0.2\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + mutation_strength * np.random.randn())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Multi-Phase Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.3 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.2 * np.random.rand())\n                mutation_strength = min(0.5, mutation_strength + 0.1 * np.random.rand())\n            else:\n                mutation_strength = max(0.1, mutation_strength * 0.9)\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08930 with standard deviation 0.05130.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1503720898352029, 0.16610614381970956, 0.14713559735357307, 0.08594193586186327, 0.08424165378230108, 0.07940871412638995, 0.036846632961997305, 0.029875226160654633, 0.023755375146549018]}}
{"id": "8861f5da-a77e-4c1f-bc9b-1fd0cfe8b49a", "fitness": 0.0942307340052831, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Integrate Lvy flight for enhanced exploration in the Advanced Hybrid Metaheuristic Optimizer.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                # Lvy flight for improved exploration\n                if delta < 0 or np.exp(-delta / temp) > np.random.rand():\n                    step = np.random.standard_cauchy(self.dim) * 0.1  # Changed line\n                    population[i] = np.clip(population[i] + step, lb, ub)\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 98, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09423 with standard deviation 0.04885.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.15082290638575047, 0.16610614381970956, 0.16141399905570086, 0.08791605953959314, 0.07979534913192643, 0.07183814330726268, 0.03902760701215768, 0.040016200808959734, 0.05114019698648742]}}
{"id": "93ab349b-08f5-4747-b69a-71f404361780", "fitness": 0.09346239364667804, "name": "AdvancedHybridMetaheuristicOptimizer", "description": "Enhanced adaptive parameter adjustment for improved exploration-exploitation balance in BBOB functions.", "code": "import numpy as np\n\nclass AdvancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialization\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10 * self.dim\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n        evaluations = population_size\n\n        # Dynamic Parameters\n        F_base = 0.8\n        Cr_base = 0.9\n        temp = 1.0\n        cooling_rate = 0.95\n        neighborhood_radius = 0.1\n        diversity_threshold = 0.1\n\n        def differential_evolution(pop, fit):\n            nonlocal F_base, Cr_base\n            for i in range(population_size):\n                indices = list(range(population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = pop[a] + F_base * (pop[b] - pop[c]) * (1 + 0.5 * np.random.rand())\n                mutant = np.clip(mutant, lb, ub)\n                Cr_dynamic = Cr_base * (1 - evaluations / self.budget) \n                cross_points = np.random.rand(self.dim) < Cr_dynamic\n                trial = np.where(cross_points, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if trial_fitness < fit[i]:\n                    pop[i] = trial\n                    fit[i] = trial_fitness\n\n        def adaptive_neighborhood_search(ind, fit):\n            new_solution = ind + np.random.normal(0, neighborhood_radius, self.dim)\n            new_solution = np.clip(new_solution, lb, ub)\n            new_fitness = func(new_solution)\n            if new_fitness < fit:\n                return new_solution, new_fitness\n            return ind, fit\n\n        def calculate_diversity(pop):\n            return np.mean(np.std(pop, axis=0))\n\n        # Search loop\n        while evaluations < self.budget:\n            diversity = calculate_diversity(population)\n            if diversity < diversity_threshold:\n                F_base = max(0.5, F_base + 0.2 * np.random.rand())\n                Cr_base = min(1.0, Cr_base - 0.1 * np.random.rand())\n\n            differential_evolution(population, fitness)\n            evaluations += population_size\n\n            for i in range(population_size):\n                if evaluations >= self.budget:\n                    break\n                \n                population[i], fitness[i] = adaptive_neighborhood_search(population[i], fitness[i])\n                if fitness[i] < best_fitness:\n                    best_solution = population[i]\n                    best_fitness = fitness[i]\n                \n                new_solution = population[i] + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, lb, ub)\n                new_fitness = func(new_solution)\n                delta = new_fitness - fitness[i]\n\n                if delta < 0 or np.exp(-delta / (temp * 0.95)) > np.random.rand():  # Changed line\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n                    if new_fitness < best_fitness:\n                        best_solution = new_solution\n                        best_fitness = new_fitness\n\n                evaluations += 1\n\n            temp *= cooling_rate\n            neighborhood_radius *= 0.95\n\n        return best_solution", "configspace": "", "generation": 99, "feedback": "The algorithm AdvancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09346 with standard deviation 0.04949.", "error": "", "parent_ids": ["3f0b33b2-f25a-40f7-a949-b58c86accc97"], "operator": null, "metadata": {"aucs": [0.1549463625612164, 0.16610614381970956, 0.15343324628860144, 0.08762556181231718, 0.08835880941238683, 0.06775285884138682, 0.035206926533911, 0.03057396720210248, 0.05715766634847064]}}
