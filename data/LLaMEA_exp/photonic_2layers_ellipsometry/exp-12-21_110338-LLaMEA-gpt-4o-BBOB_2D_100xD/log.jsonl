{"id": "ea3901a3-3ddd-4f6a-890e-ac690b95e0c4", "fitness": 0.16175966190626523, "name": "MultiStrategyOptimizer", "description": "A multi-strategy ensemble optimizer combining differential evolution, particle swarm optimization, and simulated annealing to balance exploration and exploitation in high-dimensional search spaces.", "code": "import numpy as np\n\nclass MultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [self.differential_evolution, self.particle_swarm, self.simulated_annealing]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = np.random.choice(self.strategies, p=self.strategy_weights)\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n\n        return population[np.argmin(fitness)]\n\n    def update_strategy_weights(self, new_fitness):\n        improvements = np.maximum(0, (np.min(new_fitness) - np.array([np.min(new_fitness)])) / np.min(new_fitness))\n        self.strategy_weights += improvements\n        self.strategy_weights /= self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 0, "feedback": "The algorithm MultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16176 with standard deviation 0.24967.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.8503119671685939, 0.14866314767914413, 0.056613759293813404, 0.01520404105403017, 0.0050000000000000044, 0.0050000000000000044, 0.13310103984916366, 0.09938882820401629, 0.1425541739076256]}}
{"id": "29b9f444-dd0f-4b68-bfa6-836f47f6b62b", "fitness": -Infinity, "name": "EnhancedAdaptiveMultiStrategyOptimizer", "description": "Enhanced Adaptive Multi-Strategy Optimizer combining differential evolution, adaptive particle swarm optimization, and adaptive simulated annealing for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [self.differential_evolution, self.adaptive_particle_swarm, self.adaptive_simulated_annealing]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = np.random.choice(self.strategies, p=self.strategy_weights)\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, fitness)\n\n        return population[np.argmin(fitness)]\n\n    def update_strategy_weights(self, new_fitness, fitness):\n        improvements = np.maximum(0, (np.min(fitness) - new_fitness) / np.min(fitness))\n        self.strategy_weights += improvements\n        self.strategy_weights /= self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = np.random.uniform(0.5, 1.0)  # Adaptive scaling factor\n        CR = np.random.uniform(0.7, 1.0)  # Adaptive crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def adaptive_particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = np.random.uniform(1.5, 2.5, 2)  # Adaptive cognitive and social parameters\n        w = 0.5 + np.random.rand() * 0.4  # Adaptive inertia weight\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def adaptive_simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = np.random.uniform(0.97, 0.99)  # Adaptive cooling rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                step_size = np.random.uniform(0.05, 0.1)  # Adaptive step size\n                new_solution = current_solution + np.random.normal(0, step_size, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 1, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (3,) (20,) (3,) ').", "error": "ValueError('operands could not be broadcast together with shapes (3,) (20,) (3,) ')", "parent_ids": ["ea3901a3-3ddd-4f6a-890e-ac690b95e0c4"], "operator": null, "metadata": {}}
{"id": "f2486162-116a-46de-aa15-cb650cdd3176", "fitness": 0.18100965714560008, "name": "MultiStrategyOptimizer", "description": "An improved multi-strategy ensemble optimizer incorporating adaptive mutation rates and velocity clamping for better exploration and exploitation balance in complex search spaces.", "code": "import numpy as np\n\nclass MultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [self.differential_evolution, self.particle_swarm, self.simulated_annealing]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = np.random.choice(self.strategies, p=self.strategy_weights)\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n\n        return population[np.argmin(fitness)]\n\n    def update_strategy_weights(self, new_fitness):\n        improvements = np.maximum(0, (np.min(new_fitness) - np.array([np.min(new_fitness)])) / np.min(new_fitness))\n        self.strategy_weights += improvements\n        self.strategy_weights /= self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.3 * np.random.rand()\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 2, "feedback": "The algorithm MultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18101 with standard deviation 0.29570.", "error": "", "parent_ids": ["ea3901a3-3ddd-4f6a-890e-ac690b95e0c4"], "operator": null, "metadata": {"aucs": [1.0, 0.14866314767914413, 0.056613759293813404, 0.01013404842250798, 0.0050000000000000044, 0.0050000000000000044, 0.1617329568032937, 0.09938882820401629, 0.1425541739076256]}}
{"id": "94ce467c-5520-46af-a83b-640dde698e01", "fitness": -Infinity, "name": "MultiStrategyOptimizer", "description": "A refined multi-strategy ensemble optimizer that introduces adaptive learning rates, a novel selection mechanism, and strategy blending with elite retention for enhanced convergence.", "code": "import numpy as np\n\nclass MultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [self.differential_evolution, self.particle_swarm, self.simulated_annealing]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.learning_rate = 0.1  # Adaptive learning rate\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategies = np.random.choice(self.strategies, p=self.strategy_weights, size=2)\n            new_pop_1, new_fit_1 = chosen_strategies[0](population, fitness, func, bounds)\n            new_pop_2, new_fit_2 = chosen_strategies[1](population, fitness, func, bounds)\n            evals += len(new_fit_1) + len(new_fit_2)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_pop_1 = new_pop_1[:-excess//2]\n                new_fit_1 = new_fit_1[:-excess//2]\n                new_pop_2 = new_pop_2[:-excess//2]\n                new_fit_2 = new_fit_2[:-excess//2]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_pop_1, new_pop_2))\n            combined_fitness = np.hstack((fitness, new_fit_1, new_fit_2))\n\n            best_indices = np.argsort(combined_fitness)[:self.population_size//2]\n            elite_indices = np.argsort(combined_fitness)[:self.population_size//4]\n            population = np.vstack((combined_population[best_indices], combined_population[elite_indices]))\n            fitness = np.hstack((combined_fitness[best_indices], combined_fitness[elite_indices]))\n\n            self.update_strategy_weights(new_fit_1, new_fit_2)\n\n        return population[np.argmin(fitness)]\n\n    def update_strategy_weights(self, new_fit_1, new_fit_2):\n        improvements_1 = np.maximum(0, (np.min(new_fit_1) - np.array([np.min(new_fit_1)])) / np.min(new_fit_1))\n        improvements_2 = np.maximum(0, (np.min(new_fit_2) - np.array([np.min(new_fit_2)])) / np.min(new_fit_2))\n        improvements = (improvements_1 + improvements_2) / 2\n        self.strategy_weights += self.learning_rate * improvements\n        self.strategy_weights /= self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.3 * np.random.rand()\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 3, "feedback": "An exception occurred: IndexError('index 15 is out of bounds for axis 0 with size 15').", "error": "IndexError('index 15 is out of bounds for axis 0 with size 15')", "parent_ids": ["f2486162-116a-46de-aa15-cb650cdd3176"], "operator": null, "metadata": {}}
{"id": "30e7209c-1d9c-4432-bca3-0f8cac3bc581", "fitness": 0.15051746324680546, "name": "EnhancedMultiStrategyOptimizer", "description": "An enhanced multi-strategy optimizer incorporating dynamic learning rates, adaptive swarm diversity, and entropy-based strategy selection for robust exploration-exploitation balancing in diverse search landscapes.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [self.differential_evolution, self.particle_swarm, self.simulated_annealing]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = []\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = np.random.choice(self.strategies, p=self.strategy_weights)\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.adapt_strategy_selection(population)\n\n        return population[np.argmin(fitness)]\n\n    def update_strategy_weights(self, new_fitness):\n        improvements = np.maximum(0, (np.min(new_fitness) - np.array([np.min(new_fitness)])) / np.min(new_fitness))\n        self.strategy_weights += improvements\n        self.strategy_weights /= self.strategy_weights.sum()\n        self.performance_history.append(np.min(new_fitness))\n\n    def adapt_strategy_selection(self, population):\n        entropy = -np.sum(self.strategy_weights * np.log(self.strategy_weights + 1e-10))\n        if entropy < 0.5:\n            diversity = np.mean(np.std(population, axis=0))\n            if diversity < 0.1:\n                self.strategy_weights = np.random.dirichlet(np.ones(len(self.strategies)))\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.3 * np.random.rand()\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15052 with standard deviation 0.21197.", "error": "", "parent_ids": ["f2486162-116a-46de-aa15-cb650cdd3176"], "operator": null, "metadata": {"aucs": [0.7255702549108478, 0.14866314767914413, 0.056613759293813404, 0.01013404842250798, 0.0050000000000000044, 0.0050000000000000044, 0.1617329568032937, 0.09938882820401629, 0.1425541739076256]}}
{"id": "ec800a2c-1521-453f-baa9-d9e5644a0122", "fitness": 0.09522957530958273, "name": "EnhancedMultiStrategyOptimizer", "description": "Introducing dynamic strategy selection and memory-based improvements in a multi-strategy ensemble optimizer for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [self.differential_evolution, self.particle_swarm, self.simulated_annealing]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.strategy_success_rates = np.zeros(len(self.strategies))\n        self.memory_factor = 0.1  # Factor for memory-based improvement\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n        best_fitness = np.min(fitness)\n\n        while evals < self.budget:\n            dynamic_weights = self.strategy_weights * (1 + self.strategy_success_rates)\n            dynamic_weights /= dynamic_weights.sum()\n            chosen_strategy = np.random.choice(self.strategies, p=dynamic_weights)\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_metrics(new_fitness, best_fitness)\n            best_fitness = np.min(fitness)\n\n        return population[np.argmin(fitness)]\n\n    def update_strategy_metrics(self, new_fitness, previous_best_fitness):\n        improvement = previous_best_fitness - np.min(new_fitness)\n        self.strategy_success_rates *= self.memory_factor\n        self.strategy_success_rates += improvement > 0\n\n        self.strategy_weights += improvement\n        self.strategy_weights = np.maximum(self.strategy_weights, 0.1)  # Preventing zero weights\n        self.strategy_weights /= self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.3 * np.random.rand()\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09523 with standard deviation 0.07614.", "error": "", "parent_ids": ["f2486162-116a-46de-aa15-cb650cdd3176"], "operator": null, "metadata": {"aucs": [0.22797926347584352, 0.14866314767914413, 0.056613759293813404, 0.01013404842250798, 0.0050000000000000044, 0.0050000000000000044, 0.1617329568032937, 0.09938882820401629, 0.1425541739076256]}}
{"id": "bce59587-b939-4264-b5c4-297ce86dfb75", "fitness": 0.20681763829666358, "name": "ImprovedMultiStrategyOptimizer", "description": "Introducing an adaptive learning phase to the ensemble optimizer by dynamically adjusting strategy weights based on historical performance and introducing a chaos-driven step for enhanced exploration.", "code": "import numpy as np\n\nclass ImprovedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.3 * np.random.rand()\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 6, "feedback": "The algorithm ImprovedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20682 with standard deviation 0.25224.", "error": "", "parent_ids": ["f2486162-116a-46de-aa15-cb650cdd3176"], "operator": null, "metadata": {"aucs": [0.45014043640953105, 0.8289836160854561, 0.10442164786292818, 0.0050000000000000044, 0.03149556038104717, 0.03813851564136583, 0.1381764419159045, 0.12347481084514544, 0.1415277155285939]}}
{"id": "ec38081c-3565-4887-9d00-3e25b11f1e02", "fitness": 0.20681763829666358, "name": "ImprovedMultiStrategyOptimizer", "description": "Refined the strategy selection by enhancing the chaotic map for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for better exploration-exploitation balance\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.3 * np.random.rand()\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 7, "feedback": "The algorithm ImprovedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20682 with standard deviation 0.25224.", "error": "", "parent_ids": ["bce59587-b939-4264-b5c4-297ce86dfb75"], "operator": null, "metadata": {"aucs": [0.45014043640953105, 0.8289836160854561, 0.10442164786292818, 0.0050000000000000044, 0.03149556038104717, 0.03813851564136583, 0.1381764419159045, 0.12347481084514544, 0.1415277155285939]}}
{"id": "c3eca698-3499-4981-aef8-84b18407d585", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Introducing adaptive dimensional scaling to enhance convergence by dynamically adjusting mutation and crossover rates based on problem dimensionality and current optimization state.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["bce59587-b939-4264-b5c4-297ce86dfb75"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "6a381644-9838-440e-afa4-7215aa5e7ba4", "fitness": 0.2149160885934013, "name": "HybridDualPhaseOptimizer", "description": "Introducing a hybrid dual-phase approach, utilizing chaotic maps for strategy selection and feedback-based adaptation to dynamically blend global and local search capabilities, enhancing convergence efficiency across varying dimensions.", "code": "import numpy as np\n\nclass HybridDualPhaseOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution,\n            self.particle_swarm,\n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.phase_transition = budget // 2  # Transition point for dual-phase strategy\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            if evals < self.phase_transition:\n                chosen_strategy_idx = self.select_strategy_using_chaos()\n            else:\n                chosen_strategy_idx = self.select_strategy_using_feedback(fitness)\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy_using_chaos(self):\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def select_strategy_using_feedback(self, fitness):\n        avg_fitness = np.mean(fitness)\n        feedback_factor = np.random.rand(len(self.strategies))\n        feedback_strategy = np.argmax(feedback_factor * (self.strategy_weights / (1 + avg_fitness)))\n        return feedback_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 9, "feedback": "The algorithm HybridDualPhaseOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21492 with standard deviation 0.22003.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.27742319107443825, 0.009094258100649788, 0.033752199703873775, 0.015348167862475837, 0.11874919606092005, 0.11032753317000266, 0.18693639614824276]}}
{"id": "e2189ccb-fef7-47a7-97c2-6433341f3bf7", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Utilizing chaotic map-driven strategy selection for increased exploration and diversity in multi-strategy optimization.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Improved chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = np.abs(np.sin(3.14 * c))  # Changed chaotic calculation\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "8b199b85-fa80-4da0-a1ba-2e51092912fb", "fitness": 0.17493346389769432, "name": "BanditMultiStrategyOptimizer", "description": "Leveraging multi-armed bandit strategies for dynamic strategy selection to optimize exploration and exploitation balance in black-box optimization.", "code": "import numpy as np\n\nclass BanditMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_rewards = np.zeros(len(self.strategies))\n        self.strategy_counts = np.ones(len(self.strategies))  # Start with 1 to avoid division by zero\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_rewards(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Using Upper Confidence Bound (UCB1) for strategy selection\n        total_counts = np.sum(self.strategy_counts)\n        ucb_values = self.strategy_rewards / self.strategy_counts + \\\n                     np.sqrt(2 * np.log(total_counts) / self.strategy_counts)\n        return np.argmax(ucb_values)\n\n    def update_strategy_rewards(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.strategy_rewards)) / np.min(new_fitness)\n        self.strategy_rewards[strategy_idx] += improvement\n        self.strategy_counts[strategy_idx] += 1\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 11, "feedback": "The algorithm BanditMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17493 with standard deviation 0.21748.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.763904039046996, 0.18289580595521737, 0.09813845776302055, 0.0050000000000000044, 0.03417505220223571, 0.025881552172846045, 0.16643531506236675, 0.13331181419611093, 0.16465913868045534]}}
{"id": "d845661c-392b-48ba-873a-454f3fb3448e", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "EnhancedMultiStrategyOptimizer with chaotic local search and adaptive parameter scaling to bolster exploration and exploitation balance dynamically.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.chaotic_differential_evolution, \n            self.adaptive_particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def chaotic_differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def adaptive_particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 1.5 + np.random.rand(), 1.5 + np.random.rand()  # Adaptive component\n        w = 0.5 + np.random.rand() * 0.5  # Adaptive inertia weight\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "02503fc4-8459-4371-9564-980bef782078", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce a chaotic inertia weight in particle swarm to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.5 + 0.5 * np.random.rand()  # Introduce chaotic inertia weight\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "c6217a40-9891-4528-8e7c-76a73302dcd5", "fitness": -Infinity, "name": "EnhancedMultiStrategyOptimizer", "description": "Introducing adaptive dimensional scaling to enhance convergence by dynamically adjusting mutation and crossover rates based on problem dimensionality and current optimization state.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 9, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": "\n    Refine the strategy of the selected solution to improve it. Make sure you only change 0.9% of the code, which means if the code has 100 lines, you can only change 0.8771929824561403 lines, and the rest of the lines should remain unchanged. This input code has 114 lines, so you can only change 1 lines, the rest 113 lines should remain unchanged. This changing rate 0.9% is the mandatory requirement, you cannot change more or less than this rate.\n    ", "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "c0720ab2-8500-4957-af2b-6459fc5acf05", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhanced strategy by refining the chaotic factor calculation for better exploration/exploitation balance.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Improved chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = np.abs(np.sin(2 * np.pi * c))  # Adjusted chaotic factor\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "0dcab32e-d7c7-445f-90e4-fc2519f6d1b4", "fitness": 0.20436173063399213, "name": "ReinforcedHyperHeuristicOptimizer", "description": "Utilizing a dynamic hyper-heuristic selector with reinforcement learning to refine strategy choices based on past performance, enhancing adaptability across varying optimization landscapes.", "code": "import numpy as np\n\nclass ReinforcedHyperHeuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.epsilon = 0.1  # Exploration factor\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        if np.random.rand() < self.epsilon:\n            return np.random.choice(len(self.strategies))  # Exploration\n        else:\n            return np.argmax(self.strategy_weights)  # Exploitation\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-10)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / (self.performance_history.sum() + 1e-10)\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / (np.max(fitness) + 1e-10))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 16, "feedback": "The algorithm ReinforcedHyperHeuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20436 with standard deviation 0.20767.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.43113926082524723, 0.6703246006484638, 0.25653185778535315, 0.0050000000000000044, 0.018811798310464423, 0.05201378396970857, 0.13265607591413953, 0.08584180210484549, 0.1869363961477073]}}
{"id": "a016c4a6-231e-48e4-a440-93dc0effa4ac", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance strategy selection using a chaotic logistic map for better exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Updated chaotic map for better exploration and exploitation\n        c = np.random.rand()\n        chaotic_factor = 4 * c * (1 - c)\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "703b34ce-291e-476e-84db-7f832c6e2130", "fitness": 0.1477614846412706, "name": "EnhancedMultiStrategyOptimizer", "description": "Minor enhancement through the dynamic adjustment of the crossover rate in the differential evolution strategy based on fitness variance to improve exploration.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9 * (1 - np.std(fitness) / (np.mean(fitness) + 1e-6))  # Dynamic CR adjustment\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14776 with standard deviation 0.19092.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.15621979438608835, 0.6599791365171043, 0.0979085323852884, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.12388902667558499, 0.11336098674162809, 0.1634958850657411]}}
{"id": "e28d1144-cd3a-4eb9-a8fa-0497270ed00e", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Introducing adaptive learning rate in Particle Swarm Optimization to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "3f15aaa2-d785-4889-a1ca-ffee109f6b57", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Adjusting the velocity clamping factor dynamically in the particle swarm strategy based on current fitness variance to improve exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0]) * np.std(fitness) / np.mean(fitness)  # Dynamic velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "8a95427e-23a6-4bf4-a3fc-a6323e52f5a5", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance convergence by updating the inertia weight dynamically in the particle swarm strategy.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.5 + np.random.rand() * 0.5  # Dynamic inertia weight\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "7776e82e-25d7-43ea-a8a4-a226264e2c79", "fitness": 0.16429942994186375, "name": "EnhancedMultiStrategyOptimizer", "description": "Integrating dynamic subpopulation management with adaptive strategy scheduling to improve exploration-exploitation balance and convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.num_subpopulations = 5\n        self.subpopulation_size = self.population_size // self.num_subpopulations\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.subpopulation_assignments = np.random.choice(len(self.strategies), self.num_subpopulations)\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            for sp in range(self.num_subpopulations):\n                chosen_strategy_idx = self.subpopulation_assignments[sp]\n                chosen_strategy = self.strategies[chosen_strategy_idx]\n                subpop_indices = np.arange(sp * self.subpopulation_size, (sp + 1) * self.subpopulation_size)\n                subpop = population[subpop_indices]\n                subpop_fitness = fitness[subpop_indices]\n                new_subpop, new_subpop_fitness = chosen_strategy(subpop, subpop_fitness, func, bounds)\n                \n                excess = evals + len(new_subpop_fitness) - self.budget\n                if excess > 0:\n                    new_subpop = new_subpop[:-excess]\n                    new_subpop_fitness = new_subpop_fitness[:-excess]\n\n                population[subpop_indices] = new_subpop\n                fitness[subpop_indices] = new_subpop_fitness\n                self.update_strategy_weights(new_subpop_fitness, chosen_strategy_idx)\n                evals += len(new_subpop_fitness)\n                if evals >= self.budget:\n                    break\n\n            # Dynamic reassignment of strategies to subpopulations\n            if np.random.rand() < 0.1:\n                self.subpopulation_assignments = np.random.choice(len(self.strategies), self.num_subpopulations, p=self.strategy_weights)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(len(population))\n        for i in range(len(population)):\n            indices = np.random.choice(len(population), 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (len(population), self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(len(population))\n        for i in range(len(population)):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(len(population))\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16430 with standard deviation 0.20786.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.22928736667872185, 0.7190711151915079, 0.1608075821568533, 0.009886015223404154, 0.02999674139644304, 0.0050000000000000044, 0.10416316156774696, 0.10271793110835759, 0.11776495615373905]}}
{"id": "1b2ee02f-380e-44a2-925f-2bf4db2781a0", "fitness": 0.17414065246933905, "name": "DynamicAdaptiveOptimizer", "description": "Incorporating a dynamic self-adaptive strategy selection mechanism that leverages historical performance trends and introduces a chaotic local search to enhance convergence stability and solution quality.", "code": "import numpy as np\n\nclass DynamicAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_local_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.evals = self.population_size\n\n        while self.evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            self.evals += len(new_fitness)\n\n            if self.evals > self.budget:\n                excess = self.evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                self.evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        exponential_decay = np.exp(-np.arange(len(self.strategies)))\n        weighted_strategy = exponential_decay * self.strategy_weights\n        return np.argmax(weighted_strategy)\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-9)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / (self.performance_history.sum() + 1e-9)\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / (np.max(fitness) + 1e-9))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / (T + 1e-9)):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_local_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            chaotic_seq = np.sin(np.arange(self.dim) * np.random.rand())  # Example chaotic sequence\n            new_solution = x + 0.1 * chaotic_seq\n            new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n            new_fitness[i] = func(new_solution)\n            new_population[i] = new_solution if new_fitness[i] < fitness[i] else x\n        return new_population, new_fitness", "configspace": "", "generation": 23, "feedback": "The algorithm DynamicAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17414 with standard deviation 0.15964.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5220923110945481, 0.1831907556100265, 0.3651552288646226, 0.010141346144961871, 0.03862183330630187, 0.02267392599807183, 0.14728887553895065, 0.1262428508275688, 0.15185874483899897]}}
{"id": "847bbae7-ca3b-44f1-a798-cc1fe2208975", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Introducing chaotic factor in strategy selection for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (3.99 * c * (1 - c))  # Adjusted with Logistic Map\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "6a161d81-86dd-4ed3-8c42-977f785f1f9a", "fitness": 0.2204061569911218, "name": "EnhancedMultiStrategyOptimizer", "description": "Introducing dynamic adaptive local search based on fitness landscape ruggedness to refine solutions within the EnhancedMultiStrategyOptimizer framework, enhancing local exploitation capabilities.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.local_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Chaotic map for diversification\n        c = np.random.rand()\n        chaotic_factor = (2 ** c) % 1\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def local_search(self, population, fitness, func, bounds):\n        new_population = population.copy()\n        new_fitness = fitness.copy()\n        for i in range(self.population_size):\n            local_best = population[i]\n            local_best_fitness = fitness[i]\n            for _ in range(5):\n                perturbation = np.random.normal(0, 0.05, self.dim) * (np.abs(func(local_best) - np.mean(fitness)) / (np.max(fitness) - np.min(fitness) + 1e-9))\n                candidate = local_best + perturbation\n                candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n                candidate_fitness = func(candidate)\n                if candidate_fitness < local_best_fitness:\n                    local_best, local_best_fitness = candidate, candidate_fitness\n            new_population[i] = local_best\n            new_fitness[i] = local_best_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "901f1027-3194-46f1-96e5-d1af4f7e7ee0", "fitness": 0.08798751912803313, "name": "RefinedMultiStrategyOptimizer", "description": "Adaptive solution using self-organizing maps to guide exploration and exploitation in a multi-strategy optimization framework.", "code": "import numpy as np\nfrom sklearn.cluster import MiniBatchKMeans\n\nclass RefinedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n            \n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n            self.self_organizing_map_adjustment(population)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        # Roulette Wheel Selection based on normalized weights\n        total_weight = np.sum(self.strategy_weights)\n        pick = np.random.rand() * total_weight\n        current = 0\n        for i, weight in enumerate(self.strategy_weights):\n            current += weight\n            if current > pick:\n                return i\n        return len(self.strategy_weights) - 1\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / np.min(new_fitness)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum()\n\n    def self_organizing_map_adjustment(self, population):\n        som = MiniBatchKMeans(n_clusters=4, batch_size=len(population))\n        som.fit(population)\n        centers = som.cluster_centers_\n        for i, center in enumerate(centers):\n            population[np.random.choice(range(len(population)))] = center\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])  # Velocity clamping\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness", "configspace": "", "generation": 26, "feedback": "The algorithm RefinedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08799 with standard deviation 0.06709.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.15323401988003849, 0.14866314767914413, 0.056613759293813404, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.17643374318766025, 0.09938882820401629, 0.1425541739076256]}}
{"id": "0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4", "fitness": 0.24063841542616057, "name": "EnhancedMultiStrategyOptimizer", "description": "Integrate a novel chaotic search mechanism with adaptive strategy learning to improve convergence and exploration in high-dimensional and complex landscapes.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24064 with standard deviation 0.25554.", "error": "", "parent_ids": ["c3eca698-3499-4981-aef8-84b18407d585"], "operator": null, "metadata": {"aucs": [0.5517466191487708, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "efe8d665-ec85-4022-8e19-4c5cb03e62d2", "fitness": 0.24063841542616057, "name": "EnhancedMultiStrategyOptimizer", "description": "Integrate a novel chaotic search mechanism with adaptive strategy learning and enhanced elitism to improve convergence and exploration in high-dimensional and complex landscapes.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24064 with standard deviation 0.25554.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.5517466191487708, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "c44b0332-14b2-4461-b58c-f1776a596cac", "fitness": 0.24063841542616057, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce a temperature decay factor to the simulated annealing strategy for better convergence.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        decay_factor = 0.99  # Added line\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha * decay_factor  # Modified line\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24064 with standard deviation 0.25554.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.5517466191487708, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "7d966a1b-b612-4648-b24a-ef6d996d5141", "fitness": 0.1298575436090815, "name": "AdaptiveMultiStrategyOptimizer", "description": "Incorporate a learning-based adaptive strategy selection mechanism using a multi-armed bandit approach with non-stationary rewards to enhance the performance of metaheuristic strategy selection.", "code": "import numpy as np\n\nclass AdaptiveMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_rewards = np.zeros(len(self.strategies))\n        self.strategy_counts = np.ones(len(self.strategies))  # Avoid division by zero\n        self.total_counts = len(self.strategies)\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_rewards(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        ucb_values = self.strategy_rewards / self.strategy_counts + np.sqrt(2 * np.log(self.total_counts) / self.strategy_counts)\n        chosen_strategy = np.argmax(ucb_values)\n        self.strategy_counts[chosen_strategy] += 1\n        self.total_counts += 1\n        return chosen_strategy\n\n    def update_strategy_rewards(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.strategy_rewards)) / (np.min(new_fitness) + 1e-6)\n        self.strategy_rewards[strategy_idx] += improvement\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 30, "feedback": "The algorithm AdaptiveMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12986 with standard deviation 0.04708.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.1950356636344892, 0.18289580595521737, 0.07377447378829383, 0.09118555937908568, 0.10776947833183503, 0.05641786027012585, 0.16366809824611994, 0.13331181419611093, 0.16465913868045534]}}
{"id": "2b66e7c2-4435-473c-af66-4174b8371a20", "fitness": 0.16071374694353682, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance strategy selection using entropy-weighted strategy diversity to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.diversity_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx, population)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        entropy = -np.sum(self.strategy_weights * np.log(self.strategy_weights + 1e-6))\n        diversity_factor = 1 - (entropy / np.log(len(self.strategies)))\n        chaotic_sequence = np.random.rand(self.strategy_weights.shape[0])\n        chaos_strategy = np.argmax(chaotic_sequence * self.strategy_weights * (1 + diversity_factor * self.diversity_history))\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx, population):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n        diversity = np.mean(np.std(population, axis=0))\n        self.diversity_history[strategy_idx] = diversity\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16071 with standard deviation 0.21779.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.10010738020255305, 0.759266983419659, 0.1173819874412676, 0.0050000000000000044, 0.023887228548256534, 0.02575782074852062, 0.12420697844237472, 0.13895764287233547, 0.15185770081686445]}}
{"id": "c7956381-fe12-4604-b1ea-f56684bb0808", "fitness": 0.20547639409312632, "name": "EnhancedDynamicOptimizer", "description": "Leverage dynamic strategy adaptation based on population diversity and incorporate Levy flight for enhanced exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.levy_flight_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            diversity = self.calculate_diversity(population)\n            if diversity < self.diversity_threshold:\n                self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n            \n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))\n        return np.argmax(chaotic_factor * self.strategy_weights)\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        return np.mean(np.linalg.norm(population - centroid, axis=1))\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def levy_flight_search(self, population, fitness, func, bounds):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1/beta)\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            step = np.random.normal(0, sigma, self.dim) / np.abs(np.random.normal(0, 1, self.dim))**(1/beta)\n            trial = np.clip(population[i] + step, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedDynamicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.20548 with standard deviation 0.23536.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.5419077635328222, 0.7144785482163103, 0.11685933809059235, 0.012436982048293133, 0.03741070027222493, 0.0050000000000000044, 0.1385364694495037, 0.1411002065877185, 0.1415575386406719]}}
{"id": "34a614db-cabd-4192-a1a5-b775ac290292", "fitness": 0.24063841542616057, "name": "EnhancedDynamicChaoticOptimizer", "description": "Integrate a dynamically tuned chaotic search with enhanced learning mechanisms to better balance exploration and exploitation for diverse optimization challenges.", "code": "import numpy as np\n\nclass EnhancedDynamicChaoticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.dynamic_chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.dynamic_factor = 1.0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        self.dynamic_factor = chaotic_factor  # Update dynamic factor\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def dynamic_chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim) * self.dynamic_factor\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedDynamicChaoticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24064 with standard deviation 0.25554.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.5517466191487708, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "50549656-c28a-44fc-9817-b7330b9909a1", "fitness": 0.2204061569911218, "name": "AdaptiveMemeticOptimizer", "description": "Introduce an adaptive memetic framework that combines differential evolution and chaotic local search with a self-adjusting mechanism to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveMemeticOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.chaotic_local_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.5 + 0.5 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def chaotic_local_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(3):  # Reduced iterations for faster convergence\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.01\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 34, "feedback": "The algorithm AdaptiveMemeticOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.22041 with standard deviation 0.21266.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.5122892545716089, 0.6703246006483997, 0.25653185778522025, 0.049758280696817536, 0.029395271115437827, 0.05201378396970857, 0.11707678731898497, 0.10932918066567543, 0.18693639614824276]}}
{"id": "e1c979a5-97e1-4d30-bd60-686c5956ed17", "fitness": -Infinity, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance strategy selection with entropy-based diversity control for better exploration-exploitation balance in diverse landscapes.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy(population)\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self, population):\n        diversity = np.std(population, axis=0).sum()\n        entropy_factor = np.exp(-diversity / self.dim)\n        strategy_prob = self.strategy_weights * entropy_factor\n        strategy_prob /= strategy_prob.sum()\n        return np.random.choice(len(self.strategies), p=strategy_prob)\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 35, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {}}
{"id": "2c025ab6-b6a3-4d79-9443-0416e337aba5", "fitness": -Infinity, "name": "RLAdaptiveOptimizer", "description": "Introduce a novel adaptive strategy balancing mechanism based on reinforcement learning to dynamically adjust strategy contributions for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass RLAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.rl_parameter = 0.1\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        return np.random.choice(len(self.strategies), p=self.strategy_weights)\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        reward = improvement / (np.sum(self.performance_history) + 1e-6)\n        self.performance_history[strategy_idx] += reward\n        strategy_use = np.zeros(len(self.strategies))\n        strategy_use[strategy_idx] = 1\n        self.strategy_weights += self.rl_parameter * (reward * strategy_use - self.strategy_weights)\n        self.strategy_weights = np.clip(self.strategy_weights, 0.01, 1.0)\n        self.strategy_weights /= np.sum(self.strategy_weights) + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 36, "feedback": "An exception occurred: ValueError('probabilities do not sum to 1').", "error": "ValueError('probabilities do not sum to 1')", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {}}
{"id": "0b998a91-be02-4cd5-ae44-511a932e848f", "fitness": 0.15558116280055478, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce a self-adaptive and diversity-driven evolution mechanism with hybrid chaotic dynamics to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.hybrid_chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            diversity = np.mean(np.std(population, axis=0) / (bounds[:, 1] - bounds[:, 0]))\n            if diversity < self.diversity_threshold:\n                self.introduce_diversity(population, bounds)\n            \n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        probabilities = self.strategy_weights / self.strategy_weights.sum()\n        return np.random.choice(len(self.strategies), p=probabilities)\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = np.exp(self.performance_history) / np.sum(np.exp(self.performance_history))\n\n    def introduce_diversity(self, population, bounds):\n        num_to_replace = int(0.1 * self.population_size)\n        replacement_indices = np.random.choice(self.population_size, num_to_replace, replace=False)\n        population[replacement_indices] = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_to_replace, self.dim))\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def hybrid_chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.05\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15558 with standard deviation 0.19120.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.6713084067146775, 0.16351912034846938, 0.07030534428986224, 0.0050000000000000044, 0.0050000000000000044, 0.06428155285148207, 0.16535264665180127, 0.11393567882010658, 0.1415277155285939]}}
{"id": "f14d129c-2e93-4132-8a54-1610daa664cc", "fitness": 0.24063841542616057, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce adaptive local search with self-adaptive learning rates and enhanced chaotic perturbation to improve convergence and robustness across diverse optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24064 with standard deviation 0.25554.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.5517466191487708, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "d95cbf19-2ca5-45dd-b97e-1227df1ef012", "fitness": 0.24063841542616057, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance strategy selection by incorporating entropy-based diversity assessment for better exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy(population)\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self, population):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        entropy = -np.sum(population * np.log2(population + 1e-9), axis=1).mean()  # Entropy-based diversity\n        chaos_strategy = np.argmax((chaotic_factor + entropy) * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24064 with standard deviation 0.25554.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.5517466191487708, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "c58694ac-44e9-4304-9b17-932d69923e21", "fitness": 0.19386480127549083, "name": "EnhancedMultiStrategyOptimizer", "description": "Integrate a dynamic adaptation of chaos intensity and diversity-enhanced exploration with multi-strategy reinforcement to improve convergence in diverse landscapes.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_intensity = 0.5  # Initial chaos intensity\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        chaos_strategy = np.argmax(self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n        self.chaos_intensity = min(1.0, self.chaos_intensity + (improvement / 10))\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / (np.max(fitness) + 1e-6))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729 + self.chaos_intensity * 0.1  # Dynamic inertia weight adjustment\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000 * self.chaos_intensity  # Dynamic temperature scaling\n        alpha = 0.995 + 0.005 * (1 - self.chaos_intensity)  # Dynamic cooling rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * self.chaos_intensity\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19386 with standard deviation 0.20612.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.34736719421547946, 0.7139007974720784, 0.14158423868001568, 0.012662492812045678, 0.01058419113093545, 0.09014041866056188, 0.12459325935194288, 0.16016444150613873, 0.14378617765021928]}}
{"id": "8df7fa3a-d77b-45c8-b57a-9601e8440405", "fitness": -Infinity, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce a dynamic adaptive control for `self.population_size` based on search progress to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Initial population size\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n            # Dynamically adjust population size for better adaptive control\n            if np.std(fitness) < 0.1 * np.abs(np.mean(fitness)):  # Check if population converges\n                self.population_size = max(5 * self.dim, int(0.8 * self.population_size))\n            else:\n                self.population_size = min(20 * self.dim, int(1.2 * self.population_size))\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * 0.02\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 41, "feedback": "An exception occurred: IndexError('index 22 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 22 is out of bounds for axis 0 with size 20')", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {}}
{"id": "96be1f84-910a-4493-ac00-c1b722c26aad", "fitness": 0.24323092462829018, "name": "EnhancedMultiStrategyOptimizer", "description": "Refine chaotic search with adaptive perturbation scaling to enhance exploration without sacrificing convergence speed.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24323 with standard deviation 0.25878.", "error": "", "parent_ids": ["0bb1c152-11e3-4bf3-ad68-30e3b3ab7fc4"], "operator": null, "metadata": {"aucs": [0.5750792019679374, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "1f2ba8ba-21f1-41d8-8d8f-9f7a25d89dbe", "fitness": 0.08812788058142389, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance adaptive strategy selection with a dynamic feedback mechanism to improve convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.epsilon = 1e-6\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        scaled_weights = self.strategy_weights / self.strategy_weights.sum()\n        strategy_idx = np.random.choice(len(self.strategies), p=scaled_weights)\n        return strategy_idx\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + self.epsilon)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = np.exp(self.performance_history - np.max(self.performance_history))\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + self.epsilon))\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08813 with standard deviation 0.06492.", "error": "", "parent_ids": ["96be1f84-910a-4493-ac00-c1b722c26aad"], "operator": null, "metadata": {"aucs": [0.16550809710514747, 0.14827441022858945, 0.06757364720090042, 0.01013404842250798, 0.0050000000000000044, 0.0050000000000000044, 0.15599822079088466, 0.08927721123262078, 0.14638529025216418]}}
{"id": "552ea91e-5aaf-4e12-b456-2cf3aed6240b", "fitness": 0.24063841542616057, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce chaotic velocity perturbation in particle swarm strategy to enhance global exploration.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution,\n            self.particle_swarm,\n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            chaotic_factor = np.sin(np.pi * np.random.rand(self.dim))  # Chaotic perturbation\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i])) + chaotic_factor * v_max\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24064 with standard deviation 0.25554.", "error": "", "parent_ids": ["96be1f84-910a-4493-ac00-c1b722c26aad"], "operator": null, "metadata": {"aucs": [0.5517466191487708, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "9b77ed2b-78fb-400b-b579-de18566a0378", "fitness": 0.24063841542616057, "name": "EnhancedMultiStrategyOptimizer", "description": "Refine chaotic search with adaptive perturbation scaling and improved weight update to enhance exploration and convergence speed.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = (self.performance_history / self.performance_history.sum() + 1e-6) * 0.9 + 0.1 / len(self.strategy_weights) # slightly modified line\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24064 with standard deviation 0.25554.", "error": "", "parent_ids": ["96be1f84-910a-4493-ac00-c1b722c26aad"], "operator": null, "metadata": {"aucs": [0.5517466191487708, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "e15d2bdc-d14d-482e-9481-20e67dcc5c43", "fitness": 0.24063841542616057, "name": "EnhancedMultiStrategyOptimizer", "description": "Refine chaotic search with dynamic scaling based on fitness diversity to enhance exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (np.std(fitness) / max(np.mean(fitness), 1e-6))  # Dynamic scaling based on fitness diversity\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 46, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24064 with standard deviation 0.25554.", "error": "", "parent_ids": ["96be1f84-910a-4493-ac00-c1b722c26aad"], "operator": null, "metadata": {"aucs": [0.5517466191487708, 0.8304871820501913, 0.22431996111251518, 0.019024744030322838, 0.06433399225390479, 0.07441336378645702, 0.12696415257382654, 0.12005926114829679, 0.1543964627311598]}}
{"id": "3eee86b4-a7c5-4aa9-a34f-c23d9414831a", "fitness": 0.27214131006330156, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce diversity by modifying the crossover rate in differential evolution for enhanced adaptability.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (4 * c * (1 - c))  # Logistic map for stronger chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.strategy_weights = self.performance_history / self.performance_history.sum() + 1e-6\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["96be1f84-910a-4493-ac00-c1b722c26aad"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "2678cb07-3b96-4136-9f09-7831f605bd39", "fitness": -Infinity, "name": "EnhancedMultiStrategyOptimizer", "description": "Adaptively adjust strategy weights based on recent performance and incorporate elite solutions to guide the search.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.elite_fraction = 0.1\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        chaos = np.random.rand()\n        chaotic_factor = (4 * chaos * (1 - chaos))  # Logistic map for stronger chaos\n        strategy_probs = self.strategy_weights * chaotic_factor\n        strategy_probs /= strategy_probs.sum()\n        return np.random.choice(len(self.strategies), p=strategy_probs)\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        recent_improvement = np.minimum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.abs(np.min(new_fitness)) + 1e-6)\n        self.performance_history[strategy_idx] += recent_improvement\n        self.strategy_weights = self.performance_history / (self.performance_history.sum() + 1e-6)\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8\n        CR = 0.9\n        elite_count = max(1, int(self.elite_fraction * self.population_size))\n        elite_indices = np.argsort(fitness)[:elite_count]\n        elites = population[elite_indices]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(elite_count, 3, replace=False)\n            x1, x2, x3 = elites[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 48, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["3eee86b4-a7c5-4aa9-a34f-c23d9414831a"], "operator": null, "metadata": {}}
{"id": "71c9b18d-11b3-4bd9-995f-7a9157407c54", "fitness": 0.30547464339663494, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce adaptive chaos and dynamic strategy weights to enhance exploration and exploitation balance in metaheuristic optimization.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30547 with standard deviation 0.28375.", "error": "", "parent_ids": ["3eee86b4-a7c5-4aa9-a34f-c23d9414831a"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.30500000000000005, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "5dfa4232-fe93-4d76-bde4-051debe352b3", "fitness": 0.27214131006330156, "name": "EnhancedMultiStrategyOptimizer", "description": "Slightly adjust the scaling factor in chaotic search for enhanced adaptive performance.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.03 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adjusted adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "9625517a-0bb4-4a3f-940e-615901fd93e7", "fitness": 0.27214131006330156, "name": "EnhancedMultiStrategyOptimizer", "description": "Slightly adjusted chaotic factor to optimize strategy selection in the EnhancedMultiStrategyOptimizer.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c)) ** 1.1  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "28f3e63f-a47f-4c78-b254-f4d10ee0fc73", "fitness": 0.27214131006330156, "name": "AdvancedSynergisticLvyOptimizer", "description": "Integrate adaptive Lvy flight and synergy-based strategy selection to boost optimization efficiency in dynamic environments.", "code": "import numpy as np\n\nclass AdvancedSynergisticLvyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search,\n            self.levy_flight\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.synergy_factor = np.zeros((len(self.strategies), len(self.strategies)))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        synergy_threshold = np.random.rand()\n        chosen_idx = np.argmax(self.strategy_weights)\n        if synergy_threshold < 0.3:\n            synergy_idx = np.argmax(self.synergy_factor[chosen_idx])\n            if synergy_idx != chosen_idx:\n                return synergy_idx\n        return chosen_idx\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        for i in range(len(self.strategies)):\n            self.synergy_factor[strategy_idx, i] += improvement\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness\n\n    def levy_flight(self, population, fitness, func, bounds):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            u = np.random.normal(0, sigma, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            step = u / np.abs(v) ** (1 / beta)\n            step_size = 0.01 * step * (bounds[:, 1] - bounds[:, 0])\n            new_solution = population[i] + step_size\n            new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n            new_fitness[i] = func(new_solution)\n            if new_fitness[i] < fitness[i]:\n                new_population[i] = new_solution\n            else:\n                new_population[i] = population[i]\n        return new_population, new_fitness", "configspace": "", "generation": 52, "feedback": "The algorithm AdvancedSynergisticLvyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "7d78bf0f-833b-44d1-bb73-7db4a4a3814b", "fitness": 0.27214131006330156, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance exploration by increasing the chaos factor multiplier in the strategy selection process.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 8  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "a91c03e9-d840-48d7-ba76-f5f786eb1b49", "fitness": 0.27214131006330156, "name": "EnhancedMultiStrategyOptimizer", "description": "Fine-tune strategy selection by altering the chaos factor formula for better adaptability.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c) * 0.99)  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "32400a13-a931-440a-a468-49fd7d172952", "fitness": -Infinity, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance strategy selection by implementing a proportional selection mechanism based on strategy performance history.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return np.random.choice(len(self.strategies), p=self.strategy_weights)  # Proportional selection\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 55, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {}}
{"id": "669f9782-39d9-48e8-80a8-e05952658790", "fitness": 0.2725538900540166, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance diversity and convergence by introducing adaptive velocity control in particle swarm and enhancing chaotic perturbation scaling.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        w = w_max - ((w_max - w_min) / self.budget) * len(fitness)\n        v_max = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.05 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Enhanced scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 1.5 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27255 with standard deviation 0.29960.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.6678500092833701, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "52ee7009-bd55-48a6-a399-0dd602af6642", "fitness": 0.29228128325056446, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce diversity enhancement and adaptive learning rates to boost exploration in multi-strategy metaheuristic optimization.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.learning_rate = 0.1  # Adaptive learning rate for strategy weights\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum() * (1 - self.learning_rate) + self.learning_rate / len(self.strategy_weights)\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        diversity_factor = 0.5 + 0.5 * (1 - np.std(fitness) / np.mean(fitness))  # Diversity enhancement\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness)) * diversity_factor\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 57, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.29228 with standard deviation 0.30510.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.6611144591211321, 0.9092391340812157, 0.5283352251137907, 0.038490544437111085, 0.03963976586263529, 0.030798271042264624, 0.1419276675152068, 0.13420091535927492, 0.1467855667224488]}}
{"id": "e0eec7f9-d591-4761-bed7-f963aea188b3", "fitness": 0.27214131006330156, "name": "EnhancedMultiStrategyOptimizer", "description": "Slightly adjust the chaos factor multiplier to improve exploration capabilities.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 6  # Adjusted adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 58, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "ecd0e111-5d87-4187-9f60-9ac42679d1ba", "fitness": 0.27214131006330156, "name": "EnhancedLvyStrategyOptimizer", "description": "Integrate Lvy flights and dynamic neighborhood adaptation to improve exploration and convergence speed in metaheuristic optimization.", "code": "import numpy as np\n\nclass EnhancedLvyStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            levy_step = self.levy_flight()\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = levy_step * (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                (np.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / abs(v)**(1 / beta)\n        return step", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedLvyStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "53c640fe-abc1-4686-97a7-0ddc7b17f17c", "fitness": 0.27214131006330156, "name": "EnhancedMultiStrategyOptimizer", "description": "Incorporate a self-adaptive learning mechanism that dynamically adjusts strategy probabilities based on real-time performance metrics, enhancing convergence speed and solution precision.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "10670a65-e06a-4e98-b7f2-bd33e0b92ff7", "fitness": 0.27214131006330156, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce a momentum term in the particle swarm strategy to improve convergence stability.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.6 + 0.4 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.5 + 0.4 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        momentum = 0.9  # Added momentum term\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (momentum * velocities[i] +  # Introduce momentum\n                             w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            chaotic_sequence = np.random.rand(self.dim)\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            for j in range(5):\n                chaotic_sequence = np.sin(np.pi * chaotic_sequence)\n                perturbation = (chaotic_sequence - 0.5) * 2 * (bounds[:, 1] - bounds[:, 0]) * scale\n                trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n                trial_fitness = func(trial)\n                if trial_fitness < fitness[i]:\n                    new_population[i], new_fitness[i] = trial, trial_fitness\n                else:\n                    new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27214 with standard deviation 0.29906.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.664136789366935, 0.9092391340812157, 0.38077240883469965, 0.0050000000000000044, 0.021587214159010326, 0.07064910673505276, 0.12453437633597442, 0.12374667415817386, 0.14960608689865207]}}
{"id": "74b3b123-ab4d-450f-b277-6ba960681b9c", "fitness": 0.3285083263725351, "name": "AdvancedMultiStrategyOptimizer", "description": "Integrate a self-adaptive learning mechanism and chaotic Lvy flights to enhance strategy diversification and convergence in metaheuristic optimization.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5  # Lvy parameter\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 62, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["71c9b18d-11b3-4bd9-995f-7a9157407c54"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "fb8d02a9-fb69-4e76-a6f1-c2cfc48be739", "fitness": 0.3285083263725351, "name": "EnhancedMemoryStrategyOptimizer", "description": "Enhance strategy adaptation by introducing a historical memory of past performance to dynamically adjust the exploration-exploitation balance in metaheuristic optimization.", "code": "import numpy as np\n\nclass EnhancedMemoryStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.memory_factor = 0.9  # Memory factor for strategy performance\n        self.recent_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        chaos_strategy = np.argmax(chaotic_factor * self.strategy_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.recent_history[strategy_idx] = improvement\n        self.performance_history = self.memory_factor * self.performance_history + (1 - self.memory_factor) * self.recent_history\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = self.performance_history / total_improvement\n        self.strategy_weights /= self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedMemoryStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["74b3b123-ab4d-450f-b277-6ba960681b9c"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "28a2b89b-87bf-4a1a-a9dc-57562fd5ade5", "fitness": -Infinity, "name": "AdvancedMultiStrategyOptimizer", "description": "Fine-tune the adaptive strategy selection by incorporating a weighted random choice, balancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))  # Enhanced adaptive chaos\n        chaos_strategy = np.random.choice(len(self.strategies), p=self.strategy_weights)  # Weighted random choice\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)  # Dynamic scaling\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()  # Modified crossover rate\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5  # Lvy parameter\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Adaptive scaling\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 64, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["74b3b123-ab4d-450f-b277-6ba960681b9c"], "operator": null, "metadata": {}}
{"id": "9c586078-53cd-401f-aedf-b7032a5a27df", "fitness": 0.3288353707498226, "name": "AdvancedMultiStrategyOptimizer", "description": "Enhance dynamic strategy adaptation by introducing a novel energy-based mechanism to refine convergence and exploration in metaheuristic optimization.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 65, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32884 with standard deviation 0.35068.", "error": "", "parent_ids": ["74b3b123-ab4d-450f-b277-6ba960681b9c"], "operator": null, "metadata": {"aucs": [0.8391286434348643, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "247f7cfb-9836-4801-8875-4a3f8c8b83d4", "fitness": -Infinity, "name": "EntropyDrivenMultiStrategyOptimizer", "description": "Introduce an adaptive, self-organizing strategy selection mechanism using entropy measurements to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass EntropyDrivenMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.entropy_threshold = 0.5  # Adaptive entropy threshold\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        entropy = -np.sum(self.strategy_weights * np.log(self.strategy_weights + 1e-6))\n        if entropy < self.entropy_threshold:\n            max_weight_idx = np.argmax(self.strategy_weights)\n            return max_weight_idx\n        return np.random.choice(len(self.strategies), p=self.strategy_weights)\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w = 0.729\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 66, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["9c586078-53cd-401f-aedf-b7032a5a27df"], "operator": null, "metadata": {}}
{"id": "55f931cb-e431-4b29-a78c-845d059e544a", "fitness": 0.3295925501981382, "name": "AdvancedMultiStrategyOptimizer", "description": "Introduce adaptive inertia weight in the particle swarm strategy to improve convergence speed and stability.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 67, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32959 with standard deviation 0.35178.", "error": "", "parent_ids": ["9c586078-53cd-401f-aedf-b7032a5a27df"], "operator": null, "metadata": {"aucs": [0.8459432584697051, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "f970edf5-85b8-43a3-8ab1-4478a8e54632", "fitness": 0.3285083263725351, "name": "AdvancedMultiStrategyOptimizer", "description": "Introduce a dynamic scaling factor for the chaotic Levy search to enhance exploration based on current best fitness.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0]) * (1 - np.min(fitness) / (np.max(fitness) + 1e-6)) # Updated line\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 68, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "f5323050-8aa5-4cce-a95c-76d1014f9f52", "fitness": 0.3285083263725351, "name": "RefinedAdvancedMultiStrategyOptimizer", "description": "Integrate adaptive learning rates and a novel chaotic attractor to enhance exploration and exploitation balance in multi-strategy optimization.", "code": "import numpy as np\n\nclass RefinedAdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.learning_rates = np.ones(len(self.strategies)) * 0.1\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n        self.learning_rates[strategy_idx] = min(1.0, self.learning_rates[strategy_idx] * (1 + improvement))\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 69, "feedback": "The algorithm RefinedAdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "8b4a6b89-68fe-465c-9e5f-84164a0572d6", "fitness": 0.3285083263725351, "name": "AdvancedMultiStrategyOptimizer", "description": "Improve the perturbation scale in chaotic_levy_search for better exploration.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.01 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))  # Modified scale\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 70, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "045847c2-66be-41c5-aa9f-ce7791501404", "fitness": 0.3285083263725351, "name": "AdvancedMultiStrategyOptimizer", "description": "Adjust the inertia weight update rule in the particle swarm strategy for better balance between exploration and exploitation.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (np.min(fitness) / (np.max(fitness) + 1e-6)))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 71, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "f0233196-c919-4460-9021-8c168e342c4b", "fitness": 0.3285083263725351, "name": "AdvancedMultiStrategyOptimizer", "description": "Fine-tune the inertia weight adaptation in the particle swarm strategy to enhance balance between exploration and exploitation.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            # Adjusted inertia weight line for fine-tuning\n            w = w_max - ((w_max - w_min) * (i / self.population_size)) * (1 - np.mean(fitness) / np.max(fitness))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 72, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "50e766f3-f4f5-4fc9-87d1-2a82f70929e6", "fitness": 0.3285083263725351, "name": "AdvancedMultiStrategyOptimizer", "description": "Enhanced inertia weight adaptation by incorporating global best fitness to improve convergence.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size)) * (fitness[i] / min(fitness))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 73, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "3e6a6b6b-b134-46b8-9d15-dc99c5e55919", "fitness": -Infinity, "name": "AdvancedMultiStrategyOptimizer", "description": "Introduce adaptive inertia weight in the particle swarm strategy to improve convergence speed and stability.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 68, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": "\n    Refine the strategy of the selected solution to improve it. Make sure you only change 0.7% of the code, which means if the code has 100 lines, you can only change 0.7092198581560284 lines, and the rest of the lines should remain unchanged. This input code has 141 lines, so you can only change 1 lines, the rest 140 lines should remain unchanged. This changing rate 0.7% is the mandatory requirement, you cannot change more or less than this rate.\n    ", "metadata": {"aucs": [0.8459432584697051, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "c82f1ffb-6a3f-4529-9d46-7b9188efad16", "fitness": 0.3285083263725351, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhanced multi-strategy optimizer with adaptive inertia weight and dynamic crossover, leveraging chaotic dynamics and energy-based strategy selection for improved convergence.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "4f4adfe4-e071-4fe2-b13c-a7b8db2ee6a9", "fitness": 0.3285083263725351, "name": "AdvancedMultiStrategyOptimizer", "description": "Introduce a dynamic strategy adaptation framework based on strategy performance variance to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n        self.variance_history = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.variance_history[strategy_idx] = np.var(self.performance_history)\n        variance_adjustment = 1 / (1 + self.variance_history[strategy_idx])  # Reduce weight for high variance\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement) * variance_adjustment\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 76, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "655857fb-b06d-4e5a-bf51-7be07cde422a", "fitness": 0.3285083263725351, "name": "AdvancedMultiStrategyOptimizer", "description": "Optimize energy-level adjustments by increasing decay rate for non-improving strategies.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.7  # Increased decay rate\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 77, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "ad323230-e437-49fe-a2c7-eae4d2d70eec", "fitness": -Infinity, "name": "EnhancedStrategyOptimizer", "description": "Enhance strategy selection and population diversity using entropy-based feedback and stochastic diffusion.", "code": "import numpy as np\n\nclass EnhancedStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.entropy_threshold = 0.9  # Control diversity\n        self.energy_levels = np.zeros(len(self.strategies))\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n            if self.calculate_entropy(population) < self.entropy_threshold:\n                self.stochastic_diffusion(population, bounds)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        weighted_random_choice = np.random.choice(len(self.strategies), p=self.strategy_weights)\n        return weighted_random_choice\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def calculate_entropy(self, population):\n        hist, _ = np.histogramdd(population, bins=self.dim)\n        prob = hist / np.sum(hist)\n        entropy = -np.sum(prob * np.log(prob + 1e-6))\n        return entropy\n\n    def stochastic_diffusion(self, population, bounds):\n        diffusion_scale = 0.1 * (bounds[:, 1] - bounds[:, 0])\n        perturbation = np.random.uniform(-diffusion_scale, diffusion_scale, population.shape)\n        population += perturbation\n        np.clip(population, bounds[:, 0], bounds[:, 1], out=population)\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand()\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 78, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {}}
{"id": "2518e46e-43ac-4a0c-b204-2a829cea3f3f", "fitness": 0.3291724478385707, "name": "EnhancedDynamicStrategyOptimizer", "description": "Introduce dynamic exploration-exploitation balance in the metaheuristic framework to adaptively switch between global and local search for improved convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution,\n            self.particle_swarm,\n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.exploration_exploitation_balance = 0.5  # Initial balance factor\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)\n        exploration_adjusted_weights = energy_adjusted_weights * (1 + self.exploration_exploitation_balance)\n        chaos_strategy = np.argmax(chaotic_factor * exploration_adjusted_weights)\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n        # Update exploration-exploitation balance\n        self.exploration_exploitation_balance = 0.9 * self.exploration_exploitation_balance + 0.1 * (1 if improvement > 0 else -1)\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 79, "feedback": "The algorithm EnhancedDynamicStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32917 with standard deviation 0.35117.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8421623372335972, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "219eefd7-bc4f-48e6-b219-9f3662ae3875", "fitness": 0.3285083263725351, "name": "AdvancedMultiStrategyOptimizer", "description": "Enhance adaptive chaos factor for improved exploration during strategy selection.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * np.sin(np.pi * c))  # Change: enhance chaos factor\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 80, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32851 with standard deviation 0.35020.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.8361852440392764, 0.9092391340812157, 0.6947560851157493, 0.007702495266335152, 0.035253688990963816, 0.03674620991580824, 0.13822007312485207, 0.15694429129002063, 0.1415277155285939]}}
{"id": "51a6f684-4980-4c4a-a8ee-5726fc0f306e", "fitness": 0.33692609841374455, "name": "AdvancedMultiStrategyOptimizer", "description": "Alter the differential evolution mutation strategy to include an adaptive scaling factor based on population diversity.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())  # Altered line\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 81, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33693 with standard deviation 0.39661.", "error": "", "parent_ids": ["55f931cb-e431-4b29-a78c-845d059e544a"], "operator": null, "metadata": {"aucs": [0.9084054225255108, 0.9092391340812157, 0.8637688202874791, 0.0050000000000000044, 0.008372370533435869, 0.0050000000000000044, 0.11298230881480731, 0.07558901771119753, 0.14397781177005498]}}
{"id": "78ebb9c5-1648-4270-9120-3b32b2d4dd5d", "fitness": 0.33692609841374455, "name": "AdvancedMultiStrategyOptimizer", "description": "Introduce adaptive mutation in simulated annealing to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5  # Adaptive chaos\n        self.energy_levels = np.zeros(len(self.strategies))  # Added energy levels\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n            \n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        c = np.random.rand()\n        chaotic_factor = (self.chaos_factor_multiplier * c * (1 - c))\n        energy_adjusted_weights = self.strategy_weights * (1 + self.energy_levels)  # Adjust weights\n        chaos_strategy = np.argmax(chaotic_factor * energy_adjusted_weights)  # Use adjusted weights\n        return chaos_strategy\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        # Update energy levels based on improvement\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())  # Altered line\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        # Change: use adaptive inertia weight w\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))  # Line changed\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                mutation_strength = 0.1 * (T / T0)  # Adaptive mutation strength\n                new_solution = current_solution + np.random.normal(0, mutation_strength, self.dim)  # Line changed\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 82, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33693 with standard deviation 0.39661.", "error": "", "parent_ids": ["51a6f684-4980-4c4a-a8ee-5726fc0f306e"], "operator": null, "metadata": {"aucs": [0.9084054225255108, 0.9092391340812157, 0.8637688202874791, 0.0050000000000000044, 0.008372370533435869, 0.0050000000000000044, 0.11298230881480731, 0.07558901771119753, 0.14397781177005498]}}
{"id": "2f1824b8-9ebe-4f48-a22a-076fbab9f934", "fitness": 0.17080824780279277, "name": "AdvancedMultiStrategyOptimizer", "description": "Introduce learning automata to adaptively control strategy weights based on past strategy success for enhanced optimization.", "code": "import numpy as np\n\nclass AdvancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.reward_history = np.zeros(len(self.strategies))\n        self.learning_rate = 0.1  # Learning rate for automata\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy_idx = self.select_strategy()\n            chosen_strategy = self.strategies[chosen_strategy_idx]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        probabilities = np.exp(self.reward_history) / np.sum(np.exp(self.reward_history))\n        return np.random.choice(len(self.strategies), p=probabilities)\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        reward = 1.0 if improvement > 0 else 0.1\n        self.reward_history[strategy_idx] = (1 - self.learning_rate) * self.reward_history[strategy_idx] + self.learning_rate * reward\n        self.performance_history[strategy_idx] += improvement\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 83, "feedback": "The algorithm AdvancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17081 with standard deviation 0.22378.", "error": "", "parent_ids": ["51a6f684-4980-4c4a-a8ee-5726fc0f306e"], "operator": null, "metadata": {"aucs": [0.16719651616413989, 0.790835596457159, 0.05961267269403636, 0.04929830039917871, 0.049199894657177, 0.043126042947476906, 0.1468191628539024, 0.08965832852347089, 0.1415277155285939]}}
{"id": "6eeafe2b-5b7d-4d83-8306-494086f44118", "fitness": -Infinity, "name": "AdvancedDynamicStrategyOptimizer", "description": "Introduce a dynamic adaptive strategy selection that continuously evaluates and updates strategy effectiveness based on recent performance improvement trends.", "code": "import numpy as np\n\nclass AdvancedDynamicStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.recent_improvements = np.zeros(len(self.strategies))\n        self.strategy_select_interval = 5\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            for _ in range(self.strategy_select_interval):\n                if evals >= self.budget:\n                    break\n                chosen_strategy_idx = self.select_strategy()\n                chosen_strategy = self.strategies[chosen_strategy_idx]\n                new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n                evals += len(new_fitness)\n                \n                if evals > self.budget:\n                    excess = evals - self.budget\n                    new_population = new_population[:-excess]\n                    new_fitness = new_fitness[:-excess]\n                    evals = self.budget\n\n                combined_population = np.vstack((population, new_population))\n                combined_fitness = np.hstack((fitness, new_fitness))\n                best_indices = np.argsort(combined_fitness)[:self.population_size]\n                population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n                self.update_strategy_weights(new_fitness, chosen_strategy_idx)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        total_improvement = np.sum(self.recent_improvements) + 1e-6\n        dynamic_weights = self.strategy_weights * (1 + self.recent_improvements / total_improvement)\n        dynamic_weights /= np.sum(dynamic_weights)\n        return np.random.choice(len(self.strategies), p=dynamic_weights)\n\n    def update_strategy_weights(self, new_fitness, strategy_idx):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[strategy_idx] += improvement\n        self.recent_improvements[strategy_idx] = improvement\n        total_recent_improvement = np.sum(self.recent_improvements) + 1e-6\n        self.strategy_weights = self.recent_improvements / total_recent_improvement\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[strategy_idx] = 1 if improvement > 0 else self.energy_levels[strategy_idx] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 84, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["51a6f684-4980-4c4a-a8ee-5726fc0f306e"], "operator": null, "metadata": {}}
{"id": "85b41786-bf71-4b0a-8a3f-489d3e289684", "fitness": 0.35463752091768386, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce a dynamic strategy cycle mechanism with contextual feedback loops to better adapt strategy selection and parameter tuning based on real-time performance indicators.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0  # Track current strategy\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.cycle_strategy()\n\n        return population[np.argmin(fitness)]\n\n    def cycle_strategy(self):\n        self.current_strategy = (self.current_strategy + 1) % len(self.strategies)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35464 with standard deviation 0.37937.", "error": "", "parent_ids": ["51a6f684-4980-4c4a-a8ee-5726fc0f306e"], "operator": null, "metadata": {"aucs": [0.8875951849228993, 0.9092391340812157, 0.8638367861629741, 0.02497508550934513, 0.027289323376323282, 0.05210428358793184, 0.16366809824611994, 0.10897103303001476, 0.15405875934233093]}}
{"id": "2971a497-8d98-4287-a5d4-2386733f1d66", "fitness": 0.3480823536875821, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance the optimizer's exploration by increasing the population size and adapting the mutation factor in differential evolution.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 12 * dim  # Changed from 10 * dim to 12 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0  # Track current strategy\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.cycle_strategy()\n\n        return population[np.argmin(fitness)]\n\n    def cycle_strategy(self):\n        self.current_strategy = (self.current_strategy + 1) % len(self.strategies)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.9 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))  # Changed 0.8 to 0.9 for F\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34808 with standard deviation 0.36786.", "error": "", "parent_ids": ["85b41786-bf71-4b0a-8a3f-489d3e289684"], "operator": null, "metadata": {"aucs": [0.8680531886635783, 0.8448136125142718, 0.8808925386681105, 0.023091474931171252, 0.061071414958366765, 0.03164871156770699, 0.14567029152340216, 0.13003713311089837, 0.14746281725073274]}}
{"id": "ebb610a8-340a-4f15-b5a4-f328a9fb88eb", "fitness": -Infinity, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhanced adaptability and efficiency through improved strategy selection and parameter tuning using a hybrid dynamic weighting and adaptive history mechanism.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.adaptive_strategy_selection()\n\n        return population[np.argmin(fitness)]\n\n    def adaptive_strategy_selection(self):\n        self.current_strategy = np.random.choice(\n            len(self.strategies), p=self.strategy_weights)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.95\n        if np.max(self.energy_levels) < 0.1:\n            self.energy_levels = np.ones(len(self.strategies))\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 87, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["85b41786-bf71-4b0a-8a3f-489d3e289684"], "operator": null, "metadata": {}}
{"id": "0fb81702-f46e-415f-8f79-bba38b5f8aff", "fitness": 0.27419107767496426, "name": "EnhancedMultiStrategyOptimizer", "description": "Introduce adaptive energy-based strategy allocation to dynamically adjust strategy utilization based on historical performance and transition efficiency.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.energy_levels = np.ones(len(self.strategies))\n        self.current_strategy = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            self.current_strategy = np.argmax(self.energy_levels * self.strategy_weights)\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n\n        return population[np.argmin(fitness)]\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history))\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8\n        CR = 0.7\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27419 with standard deviation 0.36777.", "error": "", "parent_ids": ["85b41786-bf71-4b0a-8a3f-489d3e289684"], "operator": null, "metadata": {"aucs": [1.0, 0.9092391340812157, 0.13157490865307253, 0.01791510399968743, 0.018401761995024413, 0.018629304040725714, 0.11314465099511817, 0.093367229867649, 0.16544760544218517]}}
{"id": "c2711d9a-b08b-420d-b60a-ab4d1f326860", "fitness": -Infinity, "name": "EnhancedMultiStrategyOptimizer", "description": "Improved strategy selection by adding weighted random selection based on performance history to enhance exploration.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0  # Track current strategy\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[np.random.choice(len(self.strategies), p=self.strategy_weights)]  # Change: weighted random selection\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.cycle_strategy()\n\n        return population[np.argmin(fitness)]\n\n    def cycle_strategy(self):\n        self.current_strategy = (self.current_strategy + 1) % len(self.strategies)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 89, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["85b41786-bf71-4b0a-8a3f-489d3e289684"], "operator": null, "metadata": {}}
{"id": "c39b3d06-03ec-42d9-ab77-d4a6567149cd", "fitness": -Infinity, "name": "EnhancedAdaptiveOptimizer", "description": "Utilize adaptive strategy selection with a fitness trend analysis to dynamically update strategy preferences, enhancing solution convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = np.random.choice(self.strategies, p=self.strategy_weights)\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.adjust_strategy_selection()\n\n        return population[np.argmin(fitness)]\n\n    def adjust_strategy_selection(self):\n        self.current_strategy = np.argmax(self.strategy_weights)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 90, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["85b41786-bf71-4b0a-8a3f-489d3e289684"], "operator": null, "metadata": {}}
{"id": "954360bc-5f1a-4530-bec4-bd7a907c4aa5", "fitness": 0.3613076146000945, "name": "EnhancedMultiStrategyOptimizer", "description": "Integrate adaptive cooling in simulated annealing to enhance convergence speed.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0  # Track current strategy\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.cycle_strategy()\n\n        return population[np.argmin(fitness)]\n\n    def cycle_strategy(self):\n        self.current_strategy = (self.current_strategy + 1) % len(self.strategies)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995 + 0.004 * np.random.rand()  # Adaptive cooling\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36131 with standard deviation 0.37397.", "error": "", "parent_ids": ["85b41786-bf71-4b0a-8a3f-489d3e289684"], "operator": null, "metadata": {"aucs": [0.8875951849228993, 0.9092391340812157, 0.8638367861629741, 0.029057624597279008, 0.08715896520731559, 0.04746340309325969, 0.16333455992822, 0.11526833099120504, 0.14881454241648173]}}
{"id": "74bdc2ac-3dd9-4046-a74a-fc354004bcf8", "fitness": 0.2533742439226867, "name": "EnhancedDynamicOptimizer", "description": "Introduce a dynamic strategy switching mechanism based on real-time performance feedback to enhance exploration-exploitation balance.", "code": "# Description: Introduce a dynamic strategy switching mechanism based on real-time performance feedback to enhance exploration-exploitation balance.\n\n# Code:\nimport numpy as np\n\nclass EnhancedDynamicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.energy_levels = np.ones(len(self.strategies))\n        self.current_strategy = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            if np.random.rand() < 0.5:  # Add randomness to strategy selection\n                self.dynamic_strategy_switch()\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n\n        return population[np.argmin(fitness)]\n\n    def dynamic_strategy_switch(self):\n        # Switch strategies based on performance history\n        probabilities = self.strategy_weights * self.energy_levels\n        probabilities /= probabilities.sum()\n        self.current_strategy = np.random.choice(len(self.strategies), p=probabilities)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = (np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995 + 0.004 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedDynamicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25337 with standard deviation 0.30422.", "error": "", "parent_ids": ["954360bc-5f1a-4530-bec4-bd7a907c4aa5"], "operator": null, "metadata": {"aucs": [0.1996496679756652, 0.9092391340812157, 0.6947560851157493, 0.007453570235119278, 0.010188500089215902, 0.040544988199627485, 0.15723495609829863, 0.11044810825115192, 0.1508531852581375]}}
{"id": "026cb23e-ef48-4b3a-8b9a-cf98899af6b8", "fitness": 0.3560238644301938, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhanced adaptive cooling in simulated annealing for improved convergence speed.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0  # Track current strategy\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.cycle_strategy()\n\n        return population[np.argmin(fitness)]\n\n    def cycle_strategy(self):\n        self.current_strategy = (self.current_strategy + 1) % len(self.strategies)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995 + 0.004 * np.random.rand()  # Adaptive cooling\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.05, self.dim)  # Slightly more exploratory movements\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 93, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35602 with standard deviation 0.37647.", "error": "", "parent_ids": ["954360bc-5f1a-4530-bec4-bd7a907c4aa5"], "operator": null, "metadata": {"aucs": [0.881793200045588, 0.9092391340812157, 0.8638367861629741, 0.02380174922320588, 0.05779703756815335, 0.059034247229778125, 0.1622394703823551, 0.09329729193652991, 0.15317586324194377]}}
{"id": "a4921700-9265-4396-b55d-cfb82d4002f8", "fitness": -Infinity, "name": "AdaptiveMultiStrategyOptimizer", "description": "Introduce adaptive learning rates and dynamic strategy selection to boost convergence in multi-strategy optimization.", "code": "import numpy as np\n\nclass AdaptiveMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.learning_rate = 0.1\n        self.current_strategy = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.select_next_strategy()\n\n        return population[np.argmin(fitness)]\n\n    def select_next_strategy(self):\n        probabilities = self.strategy_weights ** (1 / self.learning_rate)\n        probabilities /= probabilities.sum()\n        self.current_strategy = np.random.choice(len(self.strategies), p=probabilities)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.learning_rate *= 0.99\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995 + 0.004 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 94, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["954360bc-5f1a-4530-bec4-bd7a907c4aa5"], "operator": null, "metadata": {}}
{"id": "53d38eba-a389-4907-b824-f89ee70a803d", "fitness": 0.3737970384975501, "name": "EnhancedMultiStrategyOptimizer", "description": "Enhance strategy selection by incorporating historical best strategies to exploit previous successes.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0  # Track current strategy\n        self.best_strategies = []  # Track best strategies\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.cycle_strategy()\n\n        return population[np.argmin(fitness)]\n\n    def cycle_strategy(self):\n        self.current_strategy = (self.current_strategy + 1) % len(self.strategies)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n        self.best_strategies.append(self.current_strategy)  # Track strategy successes\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995 + 0.004 * np.random.rand()  # Adaptive cooling\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 95, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37380 with standard deviation 0.39274.", "error": "", "parent_ids": ["954360bc-5f1a-4530-bec4-bd7a907c4aa5"], "operator": null, "metadata": {"aucs": [1.0, 0.9092391340812157, 0.8638367861629741, 0.029057624597279008, 0.08715896520731559, 0.04746340309325969, 0.16333455992822, 0.11526833099120504, 0.14881454241648173]}}
{"id": "36f1a264-7156-4d3b-a874-1ebcafc93fd8", "fitness": -Infinity, "name": "AdaptiveHistoricalStrategyOptimizer", "description": "AdaptiveHistoricalStrategyOptimizer leverages adaptive strategy selection based on historical performance and dynamic learning rates to improve convergence efficiency.", "code": "import numpy as np\n\nclass AdaptiveHistoricalStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution,\n            self.particle_swarm,\n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.current_strategy = 0\n        self.best_strategies = []\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.select_strategy()\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n\n        return population[np.argmin(fitness)]\n\n    def select_strategy(self):\n        probabilities = np.exp(self.strategy_weights) / np.sum(np.exp(self.strategy_weights))\n        self.current_strategy = np.random.choice(len(self.strategies), p=probabilities)\n        return self.strategies[self.current_strategy]\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995 + 0.004 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 96, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["53d38eba-a389-4907-b824-f89ee70a803d"], "operator": null, "metadata": {}}
{"id": "1777037c-7366-40b7-8528-f2fe4972306e", "fitness": 0.3511310423996033, "name": "EnhancedMultiStrategyOptimizer", "description": "Improve exploration by enhancing the diversity of the initial population with a small chaos factor.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution,\n            self.particle_swarm,\n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.chaos_factor_multiplier = 5\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0  # Track current strategy\n        self.best_strategies = []  # Track best strategies\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        population += 0.01 * np.random.uniform(-1, 1, population.shape)  # Add chaos factor\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n            self.cycle_strategy()\n\n        return population[np.argmin(fitness)]\n\n    def cycle_strategy(self):\n        self.current_strategy = (self.current_strategy + 1) % len(self.strategies)\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n        self.best_strategies.append(self.current_strategy)  # Track strategy successes\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        F_adaptive = F * (1 + np.var(population, axis=0).mean())\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F_adaptive * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            w = w_max - ((w_max - w_min) * (i / self.population_size))\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995 + 0.004 * np.random.rand()  # Adaptive cooling\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedMultiStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35113 with standard deviation 0.38215.", "error": "", "parent_ids": ["53d38eba-a389-4907-b824-f89ee70a803d"], "operator": null, "metadata": {"aucs": [0.8748170277906512, 0.8961556727891979, 0.8948420079147775, 0.0302030616597464, 0.021395867487112863, 0.067317274508503, 0.13738529880445938, 0.08960706070009761, 0.14845610994188396]}}
{"id": "e9c07420-c9e8-4fd9-b898-ebce1613fd3d", "fitness": 0.25519551406561114, "name": "EnhancedMultiStrategyOptimizerV2", "description": "Integrate adaptive learning rates and crowding distance for enhanced diversity and convergence.", "code": "import numpy as np\n\nclass EnhancedMultiStrategyOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0\n        self.best_strategies = []\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n        learning_rate = 0.5\n\n        while evals < self.budget:\n            chosen_strategy = self.strategies[self.current_strategy]\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            crowding_distances = self.calculate_crowding_distance(combined_population, combined_fitness)\n            best_indices = np.argsort(crowding_distances)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness, learning_rate)\n            self.cycle_strategy()\n\n        return population[np.argmin(fitness)]\n\n    def cycle_strategy(self):\n        self.current_strategy = (self.current_strategy + 1) % len(self.strategies)\n\n    def update_strategy_weights(self, new_fitness, learning_rate):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement * learning_rate\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n        self.energy_levels[self.current_strategy] = 1 if improvement > 0 else self.energy_levels[self.current_strategy] * 0.9\n        self.best_strategies.append(self.current_strategy)\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8 + 0.3 * np.random.rand() * (1 - np.mean(fitness) / np.max(fitness))\n        CR = 0.7 + 0.2 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 2.05, 2.05\n        w_min, w_max = 0.4, 0.9\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        for _ in range(5):  # Iterate multiple times for better swarm updates\n            new_population = np.empty_like(population)\n            new_fitness = np.empty(self.population_size)\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                w = w_max - ((w_max - w_min) * (i / self.population_size))\n                velocities[i] = (w * velocities[i] +\n                                 c1 * r1 * (personal_best[i] - population[i]) +\n                                 c2 * r2 * (global_best - population[i]))\n                velocities[i] = np.clip(velocities[i], -v_max, v_max)\n                new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n                fit = func(new_population[i])\n                new_fitness[i] = fit\n                if fit < personal_best_fitness[i]:\n                    personal_best[i], personal_best_fitness[i] = new_population[i], fit\n                if fit < func(global_best):\n                    global_best = new_population[i]\n            population, fitness = new_population, new_fitness\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.995 + 0.004 * np.random.rand()\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02 * (1 - np.min(fitness) / (np.max(fitness) + 1e-6))\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness\n\n    def calculate_crowding_distance(self, population, fitness):\n        size = len(population)\n        distances = np.zeros(size)\n        sorted_indices = np.argsort(fitness)\n        scale = np.max(fitness) - np.min(fitness)\n        for i in range(1, size - 1):\n            distances[sorted_indices[i]] += (fitness[sorted_indices[i + 1]] - fitness[sorted_indices[i - 1]]) / scale\n        distances[0] = distances[-1] = np.inf\n        return distances", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedMultiStrategyOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25520 with standard deviation 0.33201.", "error": "", "parent_ids": ["53d38eba-a389-4907-b824-f89ee70a803d"], "operator": null, "metadata": {"aucs": [0.8324553781180726, 0.9092391340812157, 0.10173933299840998, 0.009876650210813409, 0.05508026728149118, 0.028862095900375806, 0.12352872804462134, 0.0944503244269067, 0.1415277155285939]}}
{"id": "8b40dccc-9b32-46c9-b590-343300f90ef8", "fitness": -Infinity, "name": "EnhancedDynamicStrategyOptimizer", "description": "Enhance strategy selection by dynamically adjusting strategy coefficients based on performance feedback to optimize search efficiency.", "code": "import numpy as np\n\nclass EnhancedDynamicStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.strategies = [\n            self.differential_evolution, \n            self.particle_swarm, \n            self.simulated_annealing,\n            self.chaotic_levy_search\n        ]\n        self.strategy_weights = np.ones(len(self.strategies)) / len(self.strategies)\n        self.performance_history = np.zeros(len(self.strategies))\n        self.energy_levels = np.zeros(len(self.strategies))\n        self.current_strategy = 0\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evals = self.population_size\n\n        while evals < self.budget:\n            chosen_strategy = np.random.choice(self.strategies, p=self.strategy_weights)\n            new_population, new_fitness = chosen_strategy(population, fitness, func, bounds)\n            evals += len(new_fitness)\n\n            if evals > self.budget:\n                excess = evals - self.budget\n                new_population = new_population[:-excess]\n                new_fitness = new_fitness[:-excess]\n                evals = self.budget\n\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[best_indices], combined_fitness[best_indices]\n\n            self.update_strategy_weights(new_fitness)\n\n        return population[np.argmin(fitness)]\n\n    def update_strategy_weights(self, new_fitness):\n        improvement = np.maximum(0, np.min(new_fitness) - np.min(self.performance_history)) / (np.min(new_fitness) + 1e-6)\n        self.performance_history[self.current_strategy] += improvement\n        total_improvement = np.sum(self.performance_history) + 1e-6\n        self.strategy_weights = (self.performance_history / total_improvement)\n        self.strategy_weights = self.strategy_weights / self.strategy_weights.sum()\n\n    def differential_evolution(self, population, fitness, func, bounds):\n        F = 0.8\n        CR = 0.9\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            x1, x2, x3 = population[indices]\n            mutant = np.clip(x1 + F * (x2 - x3), bounds[:, 0], bounds[:, 1])\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n            new_fitness[i] = func(trial)\n        return new_population, new_fitness\n\n    def particle_swarm(self, population, fitness, func, bounds):\n        c1, c2 = 1.5, 1.5\n        w = 0.5\n        v_max = 0.2 * (bounds[:, 1] - bounds[:, 0])\n        velocities = np.random.uniform(-v_max, v_max, (self.population_size, self.dim))\n        personal_best = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best = population[np.argmin(fitness)]\n\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i in range(self.population_size):\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            velocities[i] = (w * velocities[i] +\n                             c1 * r1 * (personal_best[i] - population[i]) +\n                             c2 * r2 * (global_best - population[i]))\n            velocities[i] = np.clip(velocities[i], -v_max, v_max)\n            new_population[i] = np.clip(population[i] + velocities[i], bounds[:, 0], bounds[:, 1])\n            fit = func(new_population[i])\n            new_fitness[i] = fit\n            if fit < personal_best_fitness[i]:\n                personal_best[i], personal_best_fitness[i] = new_population[i], fit\n            if fit < func(global_best):\n                global_best = new_population[i]\n        return new_population, new_fitness\n\n    def simulated_annealing(self, population, fitness, func, bounds):\n        T0 = 1000\n        alpha = 0.99\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        for i, x in enumerate(population):\n            T = T0\n            current_solution = x\n            current_fitness = fitness[i]\n            for _ in range(10):\n                new_solution = current_solution + np.random.normal(0, 0.1, self.dim)\n                new_solution = np.clip(new_solution, bounds[:, 0], bounds[:, 1])\n                new_fitness_candidate = func(new_solution)\n                if new_fitness_candidate < current_fitness or np.random.rand() < np.exp((current_fitness - new_fitness_candidate) / T):\n                    current_solution, current_fitness = new_solution, new_fitness_candidate\n                T *= alpha\n            new_population[i] = current_solution\n            new_fitness[i] = current_fitness\n        return new_population, new_fitness\n\n    def chaotic_levy_search(self, population, fitness, func, bounds):\n        new_population = np.empty_like(population)\n        new_fitness = np.empty(self.population_size)\n        beta = 1.5\n        for i in range(self.population_size):\n            u = np.random.normal(0, 1, self.dim)\n            v = np.random.normal(0, 1, self.dim)\n            levy_step = u / (np.abs(v) ** (1 / beta))\n            scale = 0.02\n            perturbation = levy_step * scale * (bounds[:, 1] - bounds[:, 0])\n            trial = np.clip(population[i] + perturbation, bounds[:, 0], bounds[:, 1])\n            trial_fitness = func(trial)\n            if trial_fitness < fitness[i]:\n                new_population[i], new_fitness[i] = trial, trial_fitness\n            else:\n                new_population[i], new_fitness[i] = population[i], fitness[i]\n        return new_population, new_fitness", "configspace": "", "generation": 99, "feedback": "An exception occurred: ValueError('probabilities contain NaN').", "error": "ValueError('probabilities contain NaN')", "parent_ids": ["53d38eba-a389-4907-b824-f89ee70a803d"], "operator": null, "metadata": {}}
