{"id": "9dacdd1b-c132-4ba9-ac5a-7f1b2057275c", "fitness": 0.053002406288949824, "name": "AdaptivePSO", "description": "Adaptive Particle Swarm Optimization with Dynamic Search Radius, which adjusts the search radius based on swarm convergence and diversity for effective exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia = 0.5  # inertia weight\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize swarm particles and velocities\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        # Initialize global best\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            # Update velocities and positions\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                # Evaluate current position\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                # Update personal bests\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            # Update global best\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            # Adaptive inertia weight - increase if swarm is converged, decrease if diverse\n            diversity = np.mean(np.std(positions, axis=0))\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = min(self.inertia + 0.05, 1.0)\n            else:\n                self.inertia = max(self.inertia - 0.05, 0.1)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05300 with standard deviation 0.03108.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.009262857169226035, 0.009275867020423156, 0.009258960424860074, 0.07861799038327688, 0.07874662164629642, 0.07857564121452354, 0.0710717117894748, 0.07117218518078361, 0.07103982177168389]}}
{"id": "d63232b8-47c1-4b46-9602-98595aef0475", "fitness": 0.05300038539331214, "name": "AdaptivePSO", "description": "Adaptive Particle Swarm Optimization with Dynamic Search Radius, using stochastic inertia adjustment to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia = 0.5  # inertia weight\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize swarm particles and velocities\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        # Initialize global best\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            # Update velocities and positions\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                # Evaluate current position\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                # Update personal bests\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            # Update global best\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            # Adaptive inertia weight - introduce stochastic component\n            diversity = np.mean(np.std(positions, axis=0))\n            self.inertia = np.clip(self.inertia + np.random.uniform(-0.05, 0.05), 0.1, 1.0)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05300 with standard deviation 0.03108.", "error": "", "parent_ids": ["9dacdd1b-c132-4ba9-ac5a-7f1b2057275c"], "operator": null, "metadata": {"aucs": [0.009263332060683638, 0.009275867020423156, 0.009257526231243651, 0.07862283366158651, 0.07874662164629642, 0.07856109551912749, 0.07107547116418811, 0.07117218518078361, 0.07102853605547665]}}
{"id": "3b995755-9b6a-4429-9b0b-49e46d084716", "fitness": 0.05300087767944042, "name": "AdaptivePSO", "description": "Improved inertia adaptation by dynamically adjusting based on the current global best value trend.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia = 0.5  # inertia weight\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize swarm particles and velocities\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        # Initialize global best\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        last_global_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            # Update velocities and positions\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                # Evaluate current position\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                # Update personal bests\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            # Update global best\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            # Adaptive inertia weight - increase if swarm is converged, decrease if diverse\n            diversity = np.mean(np.std(positions, axis=0))\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = min(self.inertia + 0.05, 1.0)\n            else:\n                self.inertia = max(self.inertia - 0.05, 0.1)\n            \n            # Adjust inertia based on the trend of global best value\n            if global_best_value < last_global_best_value:\n                self.inertia = max(self.inertia - 0.02, 0.1)\n            last_global_best_value = global_best_value\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05300 with standard deviation 0.03108.", "error": "", "parent_ids": ["9dacdd1b-c132-4ba9-ac5a-7f1b2057275c"], "operator": null, "metadata": {"aucs": [0.009262148842726692, 0.009275867020423156, 0.009258949415551943, 0.07861075890549685, 0.07874662164629642, 0.07857552896674747, 0.07106610354799925, 0.07117218518078361, 0.07103973558893839]}}
{"id": "a62833fe-9788-443f-bfae-eded91790236", "fitness": 0.05300264137113089, "name": "AdaptivePSO", "description": "Enhanced Adaptive PSO with Inertia Adjusted by Current Best Value for Improved Convergence.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia = 0.5  # inertia weight\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize swarm particles and velocities\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        # Initialize global best\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            # Update velocities and positions\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                # Evaluate current position\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                # Update personal bests\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            # Update global best\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            # Adaptive inertia weight - increase if swarm is converged, decrease if diverse\n            diversity = np.mean(np.std(positions, axis=0))\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = min(self.inertia + 0.05 * (1 - current_best_value / global_best_value), 1.0)\n            else:\n                self.inertia = max(self.inertia - 0.05 * (1 - current_best_value / global_best_value), 0.1)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05300 with standard deviation 0.03108.", "error": "", "parent_ids": ["9dacdd1b-c132-4ba9-ac5a-7f1b2057275c"], "operator": null, "metadata": {"aucs": [0.009263221677792366, 0.009275867020423156, 0.009258704869886003, 0.07862170989177852, 0.07874662164629642, 0.0785730524373055, 0.07107459730066334, 0.07117218518078361, 0.07103781231524908]}}
{"id": "1e4f009c-7512-432b-9a30-57dd5448e47d", "fitness": 0.05300444813455813, "name": "AdaptivePSO", "description": "Enhanced Adaptive PSO with Dynamic Velocity Scaling for Improved Exploration and Exploitation Balance.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia = 0.5  # inertia weight\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize swarm particles and velocities\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        # Initialize global best\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            # Update velocities and positions\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i])) * (1.0 + 0.1 * self.evaluations/self.budget)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                # Evaluate current position\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                # Update personal bests\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            # Update global best\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            # Adaptive inertia weight - increase if swarm is converged, decrease if diverse\n            diversity = np.mean(np.std(positions, axis=0))\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = min(self.inertia + 0.05 * (1 - current_best_value / global_best_value), 1.0)\n            else:\n                self.inertia = max(self.inertia - 0.05 * (1 - current_best_value / global_best_value), 0.1)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05300 with standard deviation 0.03108.", "error": "", "parent_ids": ["a62833fe-9788-443f-bfae-eded91790236"], "operator": null, "metadata": {"aucs": [0.009263393965382338, 0.009275891130690717, 0.009259363617454053, 0.07862346775038276, 0.07874686751443005, 0.07857972069244579, 0.07107595855065096, 0.07117237607933857, 0.07104299391024793]}}
{"id": "c374da5f-33ad-4e17-9921-2fe7afb0a390", "fitness": 0.05302036479149614, "name": "EnhancedAdaptivePSO", "description": "Enhanced PSO with Adaptive Learning Factors and Dynamic Inertia for Improved Convergence and Diversity Management.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5  # initial cognitive coefficient\n        self.c2 = 2.5  # initial social coefficient\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.1, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["1e4f009c-7512-432b-9a30-57dd5448e47d"], "operator": null, "metadata": {"aucs": [0.009264422515641413, 0.009276138645640475, 0.009265636724492099, 0.07863396130094547, 0.07874939305704876, 0.07864321958658071, 0.07108409437624774, 0.07117433035718546, 0.07109208655968313]}}
{"id": "039a6f04-638d-40d4-9c14-f6a1250b3666", "fitness": 0.05301439653643246, "name": "HybridAdaptivePSO_DE", "description": "Hybrid PSO with Adaptive Differential Evolution for Enhanced Exploration and Exploitation Balance", "code": "import numpy as np\n\nclass HybridAdaptivePSO_DE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5  # initial cognitive coefficient\n        self.c2 = 2.5  # initial social coefficient\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.F = 0.8  # Differential Evolution scale factor\n        self.CR = 0.9  # Crossover probability\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            # Implement Differential Evolution component\n            if self.evaluations < self.budget:\n                for i in range(self.swarm_size):\n                    a, b, c = np.random.choice(self.swarm_size, 3, replace=False)\n                    mutant = personal_best_positions[a] + self.F * (personal_best_positions[b] - personal_best_positions[c])\n                    trial = np.where(np.random.rand(self.dim) < self.CR, mutant, personal_best_positions[i])\n                    trial = np.clip(trial, lb, ub)\n\n                    trial_value = func(trial)\n                    self.evaluations += 1\n                    if trial_value < personal_best_values[i]:\n                        personal_best_positions[i] = trial\n                        personal_best_values[i] = trial_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.1, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 6, "feedback": "The algorithm HybridAdaptivePSO_DE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05301 with standard deviation 0.03109.", "error": "", "parent_ids": ["c374da5f-33ad-4e17-9921-2fe7afb0a390"], "operator": null, "metadata": {"aucs": [0.009264882858197265, 0.009276086110400983, 0.009262413566725858, 0.07863865884002552, 0.07874885644712026, 0.0786103960150526, 0.07108773910795152, 0.07117391388869831, 0.07106662199371983]}}
{"id": "fcb6c00c-2d15-4f52-b2e1-3e7876ac44a1", "fitness": 0.05300504492746943, "name": "SelfAdaptivePSO", "description": "Enhanced PSO with Self-Adaptive Mechanism for Dynamic Parameter Tuning and Improved Diversity Management.", "code": "import numpy as np\n\nclass SelfAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.0  # initial cognitive coefficient\n        self.c2 = 2.0  # initial social coefficient\n        self.min_c1 = 0.5\n        self.max_c1 = 4.0\n        self.min_c2 = 0.5\n        self.max_c2 = 4.0\n        self.inertia = 0.5  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.2\n        self.evaluations = 0\n        self.learning_rate = 0.1  # learning rate for self-adaptation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n\n            # Self-adaptive mechanism for inertia, c1, and c2\n            self.inertia = np.clip(self.inertia + self.learning_rate * (0.9 - diversity / (ub - lb).mean()), self.min_inertia, self.max_inertia)\n            self.c1 = np.clip(self.c1 + self.learning_rate * (1.0 - diversity / (ub - lb).mean()), self.min_c1, self.max_c1)\n            self.c2 = np.clip(self.c2 + self.learning_rate * (1.0 - diversity / (ub - lb).mean()), self.min_c2, self.max_c2)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 7, "feedback": "The algorithm SelfAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05301 with standard deviation 0.03108.", "error": "", "parent_ids": ["c374da5f-33ad-4e17-9921-2fe7afb0a390"], "operator": null, "metadata": {"aucs": [0.009263748079101464, 0.009275867020423156, 0.009259314108847727, 0.07862708122135553, 0.07874662164629642, 0.0785792218524205, 0.0710787607691471, 0.07117218518078361, 0.07104260446884936]}}
{"id": "2decc9e9-23a7-421d-8f1f-34bd17aae8d4", "fitness": 0.05302036479149614, "name": "EnhancedAdaptivePSO", "description": "Enhanced PSO with Adaptive Learning Factors, Dynamic Inertia, and Improved Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5  # initial cognitive coefficient\n        self.c2 = 2.5  # initial social coefficient\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.1, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                # Improved exploration-exploitation balance by adjusting inertia weight\n                self.inertia = min(self.inertia + 0.06, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["c374da5f-33ad-4e17-9921-2fe7afb0a390"], "operator": null, "metadata": {"aucs": [0.009264422515641413, 0.009276138645640475, 0.009265636724492099, 0.07863396130094547, 0.07874939305704876, 0.07864321958658071, 0.07108409437624774, 0.07117433035718546, 0.07109208655968313]}}
{"id": "1ed965ea-8c36-4031-9302-99daae59fc22", "fitness": 0.05302036479149614, "name": "EnhancedAdaptivePSO", "description": "Enhanced PSO with Adaptive Learning Factors, Dynamic Inertia, and Memory-Based Strategy for Faster Convergence and Greater Robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.memory_size = 5  # for storing previous best positions\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        memory = [global_best_position] * self.memory_size\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n                memory.pop(0)\n                memory.append(global_best_position)\n\n            diversity = np.mean(np.std(positions, axis=0))\n\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.1, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            # Utilize memory-based enhancement\n            if self.evaluations % (self.swarm_size * self.memory_size) == 0:\n                global_best_position = np.mean(memory, axis=0)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["c374da5f-33ad-4e17-9921-2fe7afb0a390"], "operator": null, "metadata": {"aucs": [0.009264422515641413, 0.009276138645640475, 0.009265636724492099, 0.07863396130094547, 0.07874939305704876, 0.07864321958658071, 0.07108409437624774, 0.07117433035718546, 0.07109208655968313]}}
{"id": "08645220-7f16-4d29-9fb6-960615156119", "fitness": 0.05299686004510178, "name": "AdaptiveHarmonySearch", "description": "Dynamic Harmony Search with Adaptive Bandwidth Adjustment for Enhanced Exploration and Exploitation Balance.", "code": "import numpy as np\n\nclass AdaptiveHarmonySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.harmony_memory_size = 20\n        self.harmony_memory_consideration_rate = 0.9\n        self.adjusting_rate = 0.5\n        self.bandwidth = 0.05\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        harmony_memory = np.random.uniform(lb, ub, (self.harmony_memory_size, self.dim))\n        harmony_values = np.array([func(h) for h in harmony_memory])\n        self.evaluations += self.harmony_memory_size\n\n        best_harmony_index = np.argmin(harmony_values)\n        best_harmony = harmony_memory[best_harmony_index]\n        best_value = harmony_values[best_harmony_index]\n\n        while self.evaluations < self.budget:\n            new_harmony = self.create_new_harmony(harmony_memory, lb, ub)\n            new_value = func(new_harmony)\n            self.evaluations += 1\n\n            worst_index = np.argmax(harmony_values)\n            if new_value < harmony_values[worst_index]:\n                harmony_memory[worst_index] = new_harmony\n                harmony_values[worst_index] = new_value\n\n                if new_value < best_value:\n                    best_harmony = new_harmony\n                    best_value = new_value\n\n            self.adapt_bandwidth(harmony_values)\n\n        return best_harmony, best_value\n\n    def create_new_harmony(self, harmony_memory, lb, ub):\n        new_harmony = np.zeros(self.dim)\n        for i in range(self.dim):\n            if np.random.rand() < self.harmony_memory_consideration_rate:\n                new_harmony[i] = harmony_memory[np.random.randint(self.harmony_memory_size)][i]\n                if np.random.rand() < self.adjusting_rate:\n                    new_harmony[i] += self.bandwidth * np.random.uniform(-1, 1)\n            else:\n                new_harmony[i] = np.random.uniform(lb[i], ub[i])\n        return np.clip(new_harmony, lb, ub)\n\n    def adapt_bandwidth(self, harmony_values):\n        diversity = np.mean(np.std(harmony_values))\n        self.bandwidth = min(max(0.001, self.bandwidth * (1 + 0.2 * (diversity - 0.1))), 0.1)", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveHarmonySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05300 with standard deviation 0.03108.", "error": "", "parent_ids": ["c374da5f-33ad-4e17-9921-2fe7afb0a390"], "operator": null, "metadata": {"aucs": [0.009261296067263114, 0.009271215247968034, 0.009262118969471711, 0.0786021087404647, 0.0786991471417946, 0.07860736937769852, 0.07105937545444507, 0.0711353748735496, 0.07107373453326071]}}
{"id": "3b30c471-fad1-4710-a9e5-09c2e6db7bc1", "fitness": 0.05302025078636511, "name": "EnhancedAdaptivePSO", "description": "Enhanced PSO using Non-Linear Inertia Weight Adjustment for Better Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5  # initial cognitive coefficient\n        self.c2 = 2.5  # initial social coefficient\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.1, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            # New non-linear inertia weight adjustment\n            self.inertia = self.max_inertia - (self.max_inertia - self.min_inertia) * (self.evaluations / self.budget)**2\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["c374da5f-33ad-4e17-9921-2fe7afb0a390"], "operator": null, "metadata": {"aucs": [0.009264369059289401, 0.009276138645640475, 0.00926563653995105, 0.07863341543639346, 0.07874939305704876, 0.07864321770246774, 0.07108367069458921, 0.07117433035718546, 0.07109208558472047]}}
{"id": "11f9b11a-a374-4693-8b6e-904634741552", "fitness": 0.053020365053061146, "name": "EnhancedAdaptivePSO", "description": "Enhanced PSO with improved balance of exploration and exploitation by adjusting inertia based on diversity and learning factors.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5  # initial cognitive coefficient\n        self.c2 = 2.5  # initial social coefficient\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)  # Adjusted learning factor update\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["c374da5f-33ad-4e17-9921-2fe7afb0a390"], "operator": null, "metadata": {"aucs": [0.009264422515641413, 0.009276138645640475, 0.009265636847519576, 0.07863396130094547, 0.07874939305704876, 0.07864322084266917, 0.07108409437624774, 0.07117433035718546, 0.07109208753465224]}}
{"id": "4b360fab-161c-4ff4-a55c-aa7984681010", "fitness": 0.05301307012272708, "name": "DiversifiedAdaptivePSO", "description": "Diversified Adaptive PSO with dynamic inertia and learning factors based on swarm diversity and performance improvement.", "code": "import numpy as np\n\nclass DiversifiedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5  # initial cognitive coefficient\n        self.c2 = 2.5  # initial social coefficient\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.no_improvement_count = 0  # Track number of iterations without improvement\n        self.no_improvement_thresh = 5  # Threshold to trigger diversification\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.swarm_size, self.dim), np.random.rand(self.swarm_size, self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1[i] * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2[i] * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n                self.no_improvement_count = 0\n            else:\n                self.no_improvement_count += 1\n\n            diversity = np.mean(np.std(positions, axis=0))\n\n            if self.no_improvement_count >= self.no_improvement_thresh:\n                positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n                self.no_improvement_count = 0\n\n            if diversity < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 13, "feedback": "The algorithm DiversifiedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05301 with standard deviation 0.03109.", "error": "", "parent_ids": ["11f9b11a-a374-4693-8b6e-904634741552"], "operator": null, "metadata": {"aucs": [0.00926637450812906, 0.00927650905259425, 0.009259823857729166, 0.07865387702764248, 0.07875317213553001, 0.0785844244963595, 0.07109953895015897, 0.07117726662301604, 0.07104664445338427]}}
{"id": "abf25c6f-9bf6-4c0a-9667-c0dd308d6fb2", "fitness": 0.053016749474834306, "name": "RefinedAdaptivePSO", "description": "Incorporates a non-linear inertia weight decay and adaptive velocity guidance based on fitness convergence rate to enhance the balance of exploration and exploitation.", "code": "import numpy as np\n\nclass RefinedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5  # initial cognitive coefficient\n        self.c2 = 2.5  # initial social coefficient\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.max_c1 = 4.0\n        self.max_c2 = 4.0\n        self.inertia = 0.9  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            convergence_rate = np.max(personal_best_values) - np.min(personal_best_values)\n\n            # Adaptive inertia based on diversity and convergence rate\n            if diversity < (ub - lb).mean() / 10 or convergence_rate < 1e-5:\n                self.inertia = max(self.inertia * 0.95, self.min_inertia)\n                self.c1 = max(self.c1 - 0.1 * (convergence_rate / 10), self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n            else:\n                self.inertia = min(self.inertia * 1.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.1 * (convergence_rate / 10), self.min_c2)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 14, "feedback": "The algorithm RefinedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["11f9b11a-a374-4693-8b6e-904634741552"], "operator": null, "metadata": {"aucs": [0.009262854923734354, 0.009276138645640475, 0.009265501639565299, 0.0786179645540116, 0.07874939305704876, 0.078641846075303, 0.07107169424268389, 0.07117433035718546, 0.07109102177833593]}}
{"id": "ade6b6ba-c4bf-4a8e-87ff-794f522b5ecc", "fitness": 0.053020365630362686, "name": "EnhancedAdaptivePSO", "description": "Enhanced PSO with adaptive inertia adjustment based on the mean velocity magnitude.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5  # initial cognitive coefficient\n        self.c2 = 2.5  # initial social coefficient\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))  # New line added\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:  # Adjusted condition\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)  # Adjusted learning factor update\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["11f9b11a-a374-4693-8b6e-904634741552"], "operator": null, "metadata": {"aucs": [0.009264422515641413, 0.009276138645640475, 0.009265637093574863, 0.07863396130094547, 0.07874939305704876, 0.07864322335487761, 0.07108409437624774, 0.07117433035718546, 0.07109208997210237]}}
{"id": "83b2ecdd-aaa8-4854-b47e-4cd73f9f9ffe", "fitness": 0.053020365630362686, "name": "EnhancedAdaptivePSO", "description": "Enhanced PSO with refined adaptive inertia adjustment using velocity variance for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = 20\n        self.c1 = 2.5  # initial cognitive coefficient\n        self.c2 = 2.5  # initial social coefficient\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9  # initial inertia weight\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        positions = np.random.uniform(lb, ub, (self.swarm_size, self.dim))\n        velocities = np.zeros((self.swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += self.swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(self.swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            velocity_variance = np.var(np.linalg.norm(velocities, axis=1))  # Changed line\n\n            if velocity_variance < (ub - lb).mean() / 100:  # Adjusted condition\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)  # Adjusted learning factor update\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["ade6b6ba-c4bf-4a8e-87ff-794f522b5ecc"], "operator": null, "metadata": {"aucs": [0.009264422515641413, 0.009276138645640475, 0.009265637093574863, 0.07863396130094547, 0.07874939305704876, 0.07864322335487761, 0.07108409437624774, 0.07117433035718546, 0.07109208997210237]}}
{"id": "d19a7f31-b25c-4ffd-8259-d43b630222df", "fitness": 0.053020581920620695, "name": "EnhancedDynamicPSO", "description": "Enhanced PSO with dynamic swarm size adjustment based on convergence speed and diversity.", "code": "import numpy as np\n\nclass EnhancedDynamicPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            # Dynamic swarm size adjustment based on diversity\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedDynamicPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["ade6b6ba-c4bf-4a8e-87ff-794f522b5ecc"], "operator": null, "metadata": {"aucs": [0.009264531820525446, 0.009276131140606214, 0.009265637093574863, 0.07863507681129178, 0.07874931639848759, 0.07864322335487761, 0.07108495983243301, 0.07117427086168737, 0.07109208997210237]}}
{"id": "db75ecae-321d-4bdb-86d4-6a550e8b05f3", "fitness": 0.05302058238959795, "name": "EnhancedDynamicPSO", "description": "Enhanced PSO with adaptive learning rates for velocity components based on individual and global convergence trends.", "code": "import numpy as np\n\nclass EnhancedDynamicPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            # Adjust learning rates based on convergence trends\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = global_best_value - current_best_value\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            # Dynamic swarm size adjustment based on diversity\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedDynamicPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["d19a7f31-b25c-4ffd-8259-d43b630222df"], "operator": null, "metadata": {"aucs": [0.009264531820525446, 0.009276131140606214, 0.009265637339630595, 0.07863507681129178, 0.07874931639848759, 0.07864322586712802, 0.07108495983243301, 0.07117427086168737, 0.0710920914345915]}}
{"id": "3adb5c92-dc8f-4509-bed3-c3fa20697939", "fitness": 0.053020581920620695, "name": "ImprovedDynamicPSO", "description": "Improved PSO with adaptive inertia, learning rates, and dynamic swarm size control for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedDynamicPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.linalg.norm(positions - positions.mean(axis=0), axis=1))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 19, "feedback": "The algorithm ImprovedDynamicPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["db75ecae-321d-4bdb-86d4-6a550e8b05f3"], "operator": null, "metadata": {"aucs": [0.009264531820525446, 0.009276131140606214, 0.009265637093574863, 0.07863507681129178, 0.07874931639848759, 0.07864322335487761, 0.07108495983243301, 0.07117427086168737, 0.07109208997210237]}}
{"id": "3dd40243-b7da-450e-b510-0fee7655d90a", "fitness": 0.053020470120326464, "name": "EnhancedDynamicPSO", "description": "Enhanced PSO with adaptive inertia and learning rates, leveraging swarm diversity and velocity magnitude for dynamic parameter tuning and swarm size adjustment.", "code": "import numpy as np\n\nclass EnhancedDynamicPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            # Adaptive inertia and learning rates\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            # Adjust learning rates based on convergence trends\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = global_best_value - current_best_value\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n                self.c1 = max(self.c1 - 0.1, self.min_c1)\n\n            # Dynamic swarm size adjustment based on diversity\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedDynamicPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["db75ecae-321d-4bdb-86d4-6a550e8b05f3"], "operator": null, "metadata": {"aucs": [0.009264478806607634, 0.009276131140606214, 0.009265637462658516, 0.0786345365747313, 0.07874931639848759, 0.07864322712326899, 0.07108454030529843, 0.07117427086168737, 0.07109209240959213]}}
{"id": "7e7258e7-8302-4e7f-8495-ae067eb15691", "fitness": 0.05302043973105722, "name": "EnhancedDynamicPSO", "description": "Improved EnhancedDynamicPSO with adaptive inertia reduction based on swarm stagnation detection.", "code": "import numpy as np\n\nclass EnhancedDynamicPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            # Adjust learning rates based on convergence trends\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = global_best_value - current_best_value\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            # Dynamic swarm size adjustment based on diversity and stagnation detection\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n            # Detect stagnation and adapt inertia\n            if np.abs(global_best_value - current_best_value) < 1e-6:\n                self.inertia = max(self.inertia - 0.1, self.min_inertia)\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedDynamicPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["db75ecae-321d-4bdb-86d4-6a550e8b05f3"], "operator": null, "metadata": {"aucs": [0.009264464102257453, 0.009276131140606214, 0.009265637893257406, 0.0786343862053468, 0.07874931639848759, 0.07864323151984498, 0.07108442363588297, 0.07117427086168737, 0.07109209582214415]}}
{"id": "77a1a164-11a5-4a3e-b407-151af567acb6", "fitness": 0.0530206947463544, "name": "EnhancedDynamicPSO", "description": "Enhanced PSO with adaptive inertia and learning rates using historical best improvements for improved convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = historical_best_value - global_best_value  # Changed line\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            historical_best_value = global_best_value  # Added line\n\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedDynamicPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["db75ecae-321d-4bdb-86d4-6a550e8b05f3"], "operator": null, "metadata": {"aucs": [0.009264584833807321, 0.009276131140606214, 0.009265637339630595, 0.07863561698144084, 0.07874931639848759, 0.07864322586712802, 0.07108537785981017, 0.07117427086168737, 0.0710920914345915]}}
{"id": "3ee7a9ea-02f6-4718-8976-270a74e8aed8", "fitness": 0.0530206947463544, "name": "EnhancedDynamicPSO", "description": "EnhancedDynamicPSO with dynamically adjusted swarm size and learning rates based on historical trend analysis for improved convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = historical_best_value - global_best_value\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedDynamicPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["77a1a164-11a5-4a3e-b407-151af567acb6"], "operator": null, "metadata": {"aucs": [0.009264584833807321, 0.009276131140606214, 0.009265637339630595, 0.07863561698144084, 0.07874931639848759, 0.07864322586712802, 0.07108537785981017, 0.07117427086168737, 0.0710920914345915]}}
{"id": "48900246-7bb7-4fc9-b624-9fbcbdf0a993", "fitness": 0.0530206947463544, "name": "EnhancedDynamicPSO", "description": "Enhanced PSO with adaptive inertia, learning rates, and diversity control using historical improvements for better convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = historical_best_value - global_best_value  # Changed line\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            historical_best_value = global_best_value  # Added line\n\n            # Dynamically adjust swarm size based on diversity\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedDynamicPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["77a1a164-11a5-4a3e-b407-151af567acb6"], "operator": null, "metadata": {"aucs": [0.009264584833807321, 0.009276131140606214, 0.009265637339630595, 0.07863561698144084, 0.07874931639848759, 0.07864322586712802, 0.07108537785981017, 0.07117427086168737, 0.0710920914345915]}}
{"id": "b74f06e8-bf94-4612-a15d-627c678f3cc7", "fitness": 0.05303135668843473, "name": "QuantumAdaptivePSO", "description": "Adaptive Quantum-inspired PSO with stochastic velocity adjustments and swarm diversity control for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, 1, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = historical_best_value - global_best_value\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 25, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["77a1a164-11a5-4a3e-b407-151af567acb6"], "operator": null, "metadata": {"aucs": [0.00926535828617725, 0.009276421660656342, 0.009269622501885233, 0.07864350020635469, 0.07875228167380965, 0.0786836215290978, 0.07109148624574202, 0.07117657168970348, 0.07112334640248608]}}
{"id": "9d5efe98-4df8-43fc-80b8-f225e8922774", "fitness": 0.05303135668843473, "name": "QuantumAdaptivePSO", "description": "Introduced inertia reduction triggered by diversity below a threshold to enhance exploration.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, 1, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = historical_best_value - global_best_value\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                if diversity < (ub - lb).mean() / 25:  # Reduced inertia when diversity is low\n                    self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 26, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["b74f06e8-bf94-4612-a15d-627c678f3cc7"], "operator": null, "metadata": {"aucs": [0.00926535828617725, 0.009276421660656342, 0.009269622501885233, 0.07864350020635469, 0.07875228167380965, 0.0786836215290978, 0.07109148624574202, 0.07117657168970348, 0.07112334640248608]}}
{"id": "9566f090-726d-4387-9774-a63dc572ba5d", "fitness": 0.05303087511142382, "name": "EnhancedQuantumAdaptivePSO", "description": "Enhanced QuantumAdaptivePSO with adaptive local search and dynamic swarm contraction/expansion based on convergence speed and diversity metrics for improved solution accuracy.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.inertia = 0.9\n        self.inertia_decrement = 0.99\n        self.local_search_prob = 0.1\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, 1, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                if np.random.rand() < self.local_search_prob:\n                    local_search_step = np.random.normal(0, 0.1, self.dim)\n                    positions[i] = np.clip(positions[i] + local_search_step, lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            if current_best_value < historical_best_value:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n\n            diversity = np.mean(np.std(positions, axis=0))\n            if diversity < (ub - lb).mean() / 20 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n            self.inertia *= self.inertia_decrement\n            historical_best_value = global_best_value\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["b74f06e8-bf94-4612-a15d-627c678f3cc7"], "operator": null, "metadata": {"aucs": [0.00926544881034208, 0.009276106552943753, 0.00926961918098912, 0.07864443373092689, 0.07874906703862639, 0.07868358770298745, 0.07109221392045983, 0.07117407893403571, 0.07112332013150313]}}
{"id": "901fa883-e7d0-4de1-89df-4e374baa7185", "fitness": 0.05303135668843473, "name": "QuantumAdaptivePSO", "description": "Enhanced QuantumAdaptivePSO with dynamic velocity update using a non-linear inertia weight adjustment for improved convergence.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, 1, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = max(self.min_inertia, self.max_inertia - (self.max_inertia - self.min_inertia) * (self.evaluations / self.budget))\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = historical_best_value - global_best_value\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 28, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["b74f06e8-bf94-4612-a15d-627c678f3cc7"], "operator": null, "metadata": {"aucs": [0.00926535828617725, 0.009276421660656342, 0.009269622501885233, 0.07864350020635469, 0.07875228167380965, 0.0786836215290978, 0.07109148624574202, 0.07117657168970348, 0.07112334640248608]}}
{"id": "10a1658a-7eef-4df9-a615-d30695945347", "fitness": 0.053032516792799606, "name": "QuantumAdaptivePSO", "description": "Enhanced QuantumAdaptivePSO by adjusting stochastic velocity update based on swarm diversity for improved convergence.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, np.std(personal_best_positions)/2, self.dim) # Adjusted line\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            avg_personal_improvement = np.mean(personal_best_values - current_best_value)\n            avg_global_improvement = historical_best_value - global_best_value\n            if avg_personal_improvement > avg_global_improvement:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 29, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["b74f06e8-bf94-4612-a15d-627c678f3cc7"], "operator": null, "metadata": {"aucs": [0.009265794209533995, 0.009276617198233583, 0.009269535935967688, 0.07864794860269153, 0.07875427670255353, 0.07868274197045722, 0.07109495120997111, 0.07117811805065544, 0.07112266725513239]}}
{"id": "34b87f35-f2e9-4e35-a367-80167e126b82", "fitness": 0.053032516792799606, "name": "QuantumAdaptivePSO", "description": "Introduce dynamic parameter adaptation using local and global improvements to balance exploration and exploitation in QuantumAdaptivePSO for enhanced optimization performance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.5\n        self.c2 = 2.5\n        self.min_c1 = 0.5\n        self.min_c2 = 0.5\n        self.inertia = 0.9\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, np.std(personal_best_positions) / 2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 10:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, 4.0)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.c2 = max(self.c2 - 0.1, self.min_c2)\n\n            local_improvement = np.mean(personal_best_values - current_best_value)\n            global_improvement = historical_best_value - global_best_value\n            improvement_ratio = local_improvement / (global_improvement + 1e-9)\n\n            if improvement_ratio > 1:\n                self.c1 = min(self.c1 + 0.1, 4.0)\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            else:\n                self.c2 = min(self.c2 + 0.1, 4.0)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 20 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 15 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 30, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["10a1658a-7eef-4df9-a615-d30695945347"], "operator": null, "metadata": {"aucs": [0.009265794209533995, 0.009276617198233583, 0.009269535935967688, 0.07864794860269153, 0.07875427670255353, 0.07868274197045722, 0.07109495120997111, 0.07117811805065544, 0.07112266725513239]}}
{"id": "2903454e-fe47-4811-90a4-01194feeb809", "fitness": 0.05572147488956472, "name": "QuantumAdaptivePSO", "description": "Introduce dynamic adaptation strategies in QuantumAdaptivePSO with local search intensification for improved exploration-exploitation balance and enhanced convergence.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, np.std(personal_best_positions)/2, self.dim) \n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 31, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["10a1658a-7eef-4df9-a615-d30695945347"], "operator": null, "metadata": {"aucs": [0.010294578364406282, 0.009276617198233583, 0.009269457944391157, 0.09210061264975844, 0.07875427670255353, 0.07868194642933057, 0.08081561705045459, 0.07117811805065544, 0.07112204961629887]}}
{"id": "5c34618b-8d95-4bf2-82ae-a80a85d9f63f", "fitness": 0.052962229555201246, "name": "QuantumAdaptivePSO", "description": "Refine QuantumAdaptivePSO with adaptive velocity thresholding and stochastic swarm resizing for enhanced convergence under varying objective landscapes.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.velocity_threshold = 0.1\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, np.std(personal_best_positions)/2, self.dim) \n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                velocities[i] = np.clip(velocities[i], -self.velocity_threshold, self.velocity_threshold)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < self.velocity_threshold / 2:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if np.random.rand() < 0.05:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 32, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05296 with standard deviation 0.03106.", "error": "", "parent_ids": ["2903454e-fe47-4811-90a4-01194feeb809"], "operator": null, "metadata": {"aucs": [0.009261528515898787, 0.009268018326989536, 0.009249214745629675, 0.07860443486722413, 0.07866655431623826, 0.07847633001583876, 0.07106124808432379, 0.07111010193787937, 0.07096263518678891]}}
{"id": "ce34195d-c1cc-4c0a-bc63-4686cc5783f4", "fitness": 0.05303167866393976, "name": "QuantumAdaptivePSO", "description": "Introduce dynamical perturbation scaling in local search step of QuantumAdaptivePSO for enhanced local convergence.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, np.std(personal_best_positions)/2, self.dim) \n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        perturbation_factor = 0.1 * (1 - (self.evaluations / self.budget))  # Dynamically scaled perturbation\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, perturbation_factor, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 33, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["2903454e-fe47-4811-90a4-01194feeb809"], "operator": null, "metadata": {"aucs": [0.009265477609374129, 0.009276617198233583, 0.009269457944391157, 0.07864471721748867, 0.07875427670255353, 0.07868194642933057, 0.07109244720713193, 0.07117811805065544, 0.07112204961629887]}}
{"id": "38d568c6-76db-42fe-955a-744c9f787d1e", "fitness": 0.05572147488956472, "name": "QuantumAdaptivePSO", "description": "Fine-tuned the inertia adjustment mechanism for enhanced convergence speed.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        historical_best_value = global_best_value\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, np.std(personal_best_positions)/2, self.dim) \n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            # Adjusted inertia rate for better adaptation\n            if mean_velocity_magnitude < (ub - lb).mean() / 20: \n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            historical_best_value = global_best_value\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 34, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["2903454e-fe47-4811-90a4-01194feeb809"], "operator": null, "metadata": {"aucs": [0.010294578364406282, 0.009276617198233583, 0.009269457944391157, 0.09210061264975844, 0.07875427670255353, 0.07868194642933057, 0.08081561705045459, 0.07117811805065544, 0.07112204961629887]}}
{"id": "cacd03ba-5a61-4c78-b72e-993ce3c9ba59", "fitness": 0.055790869200128185, "name": "QuantumAdaptivePSO", "description": "Introduce adaptive quantum potential and diversity-driven swarm restructuring in QuantumAdaptivePSO for enhanced exploration and convergence efficiency.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 35, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05579 with standard deviation 0.03314.", "error": "", "parent_ids": ["2903454e-fe47-4811-90a4-01194feeb809"], "operator": null, "metadata": {"aucs": [0.010294512153283475, 0.009276488013958306, 0.009300447994416983, 0.09209994031908553, 0.07875295724716724, 0.079020661745419, 0.08081508814786931, 0.07117710081972817, 0.07138062636022569]}}
{"id": "89c99e1a-f406-4d2e-8345-b0496704b059", "fitness": 0.05574734623046077, "name": "QuantumAdaptivePSO", "description": "Enhance QuantumAdaptivePSO by integrating adaptive dynamic velocity scaling and strategic swarm resizing for improved convergence.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 60  # Changed from 50 to 60\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.velocity_scale_factor = 0.1  # New parameter\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                velocities[i] *= self.velocity_scale_factor  # Dynamic scaling of velocity\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 36, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05575 with standard deviation 0.03312.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.010294163882192175, 0.009273925347047074, 0.009284242134235843, 0.09209640363248839, 0.0787267924990025, 0.07883730657999133, 0.08081234804041004, 0.07115685044099918, 0.07124408351778044]}}
{"id": "82452072-86aa-45fe-93a9-3984e729a8fc", "fitness": 0.05302728670732578, "name": "QuantumAdaptivePSO", "description": "Enhanced QuantumAdaptivePSO by introducing a small perturbation in the quantum factor for improved exploration.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                q_factor += np.random.normal(0, 0.01, self.dim)  # Slight perturbation added\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 37, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03109.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.009263734820731284, 0.009276487643985365, 0.0092692630710971, 0.07862694276156235, 0.07875295338601873, 0.07867995124820837, 0.07107865876287545, 0.07117709101347824, 0.07112049765797512]}}
{"id": "a8145b80-cbf1-462c-af9f-aa703e463117", "fitness": 0.055790869200128185, "name": "QuantumAdaptivePSOEnhanced", "description": "Enhance QuantumAdaptivePSO with dynamic swarm interactions and historical best-driven topology adjustments for superior convergence dynamics.", "code": "import numpy as np\n\nclass QuantumAdaptivePSOEnhanced:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.memory_size = 5\n        self.memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if self.evaluations % (self.budget // 20) == 0:\n                self.memory.append(global_best_value)\n                if len(self.memory) > self.memory_size:\n                    self.memory.pop(0)\n                if len(self.memory) == self.memory_size and max(self.memory) - min(self.memory) < 1e-5:\n                    self.perturb_swarm(positions, func.bounds)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value\n\n    def perturb_swarm(self, positions, bounds):\n        for i in range(len(positions)):\n            perturbation = np.random.uniform(-0.1, 0.1, self.dim) * (bounds.ub - bounds.lb)\n            positions[i] = np.clip(positions[i] + perturbation, bounds.lb, bounds.ub)", "configspace": "", "generation": 38, "feedback": "The algorithm QuantumAdaptivePSOEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05579 with standard deviation 0.03314.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.010294512153283475, 0.009276488013958306, 0.009300447994416983, 0.09209994031908553, 0.07875295724716724, 0.079020661745419, 0.08081508814786931, 0.07117710081972817, 0.07138062636022569]}}
{"id": "3177e8ee-5930-4adf-a6d8-baefd7284e87", "fitness": 0.05303271983902692, "name": "ImprovedQuantumAdaptivePSO", "description": "Enhance QuantumAdaptivePSO by incorporating a dynamic self-adaptive mutation rate and hybrid local search strategy for robust exploitation and exploration balance.", "code": "import numpy as np\n\nclass ImprovedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.mutation_rate = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, 3.0)\n                self.c2 = max(self.c2 - 0.05, 0.5)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, 0.5)\n                self.c2 = min(self.c2 + 0.1, 3.0)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.hybrid_local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n            # Adaptive mutation rate adjustment\n            self.mutation_rate = max(0.05, self.mutation_rate * (1.0 - diversity / (ub - lb).mean()))\n\n        return global_best_position, global_best_value\n    \n    def hybrid_local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            if np.random.rand() < self.mutation_rate:\n                perturbation = np.random.normal(0, 0.1, self.dim)\n                candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n                candidate_value = func(candidate_position)\n                self.evaluations += 1\n                if candidate_value < personal_best_values[i]:\n                    personal_best_positions[i] = candidate_position\n                    personal_best_values[i] = candidate_value", "configspace": "", "generation": 39, "feedback": "The algorithm ImprovedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.009265936532537378, 0.009276488013958306, 0.00926961856614783, 0.07864940560280753, 0.07875295724716724, 0.07868358145344567, 0.07109606821526993, 0.07117710081972817, 0.07112332210018024]}}
{"id": "3bff74e3-d03d-4de1-b942-90d96abea882", "fitness": 0.053077953812526474, "name": "EnhancedQuantumAdaptivePSO", "description": "Enhance QuantumAdaptivePSO by introducing inertia and acceleration coefficient scheduling based on convergence trends for improved adaptability and faster convergence.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.convergence_threshold = 50  # Number of evaluations to check convergence trend\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        \n        prev_global_best_value = None\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions) / 2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            # Adaptive scheduling based on convergence trends\n            if prev_global_best_value is not None:\n                if abs(prev_global_best_value - global_best_value) < 1e-6 and self.evaluations % self.convergence_threshold == 0:\n                    self.inertia = max(self.inertia - 0.1, self.min_inertia)\n                    self.c1 = min(self.c1 + 0.1, 3.0)\n                    self.c2 = min(self.c2 + 0.1, 3.0)\n                else:\n                    self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                    self.c1 = max(self.c1 - 0.05, 0.5)\n                    self.c2 = max(self.c2 - 0.05, 0.5)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n            \n            prev_global_best_value = global_best_value\n\n        return global_best_position, global_best_value", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05308 with standard deviation 0.03113.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.009265285457383365, 0.009276488013958306, 0.009291333735573781, 0.07864275696499179, 0.07875295724716724, 0.07890677973440896, 0.07109090782526395, 0.07117710081972817, 0.07129797451426267]}}
{"id": "a640534f-738b-4ced-95c0-6498025cbfd8", "fitness": 0.05303191071136906, "name": "QuantumAdaptivePSO", "description": "Integrate Lvy flights with adaptive quantum potential in QuantumAdaptivePSO for improved exploration and convergence.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.levy_alpha = 1.5\n\n    def levy_flight(self, size):\n        u = np.random.normal(0, 1, size) * np.power(np.abs(np.random.normal(0, 1, size)), -1 / self.levy_alpha)\n        return u\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i] + self.levy_flight(self.dim), lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 41, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.009265657448271902, 0.00927654768426367, 0.009269457576773554, 0.07864655653926522, 0.07875356665498112, 0.07868193703964865, 0.07109385672330293, 0.07117756697774869, 0.0711220497580658]}}
{"id": "c2be8f0c-deea-4ede-872f-d31429408fae", "fitness": 0.055720974472165605, "name": "QuantumAdaptivePSO", "description": "Improve exploration and exploitation balance in QuantumAdaptivePSO by introducing fitness-based dynamic swarm reshaping and adaptive inertia for enhanced convergence.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n            \n            # Adjust inertia dynamically based on fitness\n            if global_best_value < np.median(personal_best_values):\n                self.inertia = max(self.inertia - 0.1, self.min_inertia)\n                self.c1 = min(self.c1 + 0.2, self.max_c1)\n            else:\n                self.inertia = min(self.inertia + 0.1, self.max_inertia)\n                self.c2 = min(self.c2 + 0.2, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            # Introduce fitness-based swarm reshaping\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 2, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (2, self.dim))\n                additional_velocities = np.zeros((2, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 2\n                swarm_size += 2\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 42, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.010294512153283475, 0.009276488013958306, 0.009269417531714175, 0.09209994031908553, 0.07875295724716724, 0.07868152822395102, 0.08081508814786931, 0.07117710081972817, 0.07112173779273323]}}
{"id": "b27d8dff-fc73-4dce-816b-53a80c0010c4", "fitness": 0.055720974472165605, "name": "ChaoticQuantumAdaptivePSO", "description": "Integrate chaos theory with adaptive quantum potential for enhanced exploration and convergence in ChaoticQuantumAdaptivePSO.", "code": "import numpy as np\n\nclass ChaoticQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.chaos_factor = 0.7\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                chaotic_r = self.chaos_factor * (1 - r1) * r1  # Logistic map for chaotic behavior\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * chaotic_r * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 43, "feedback": "The algorithm ChaoticQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.010294512153283475, 0.009276488013958306, 0.009269417531714175, 0.09209994031908553, 0.07875295724716724, 0.07868152822395102, 0.08081508814786931, 0.07117710081972817, 0.07112173779273323]}}
{"id": "d0da6203-fb07-4f37-ac40-231a0c9cf3b6", "fitness": 0.053031419349769356, "name": "QuantumAdaptivePSO", "description": "Enhance swarm convergence by introducing adaptive quantum potential scaling and local search intensification in QuantumAdaptivePSO.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.local_search_intensity = 0.2  # Added variable\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * \n                                            np.std(personal_best_positions) / 2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor * (1 - (self.evaluations / self.budget)))  # Modified line\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, \n                                  personal_best_values, intensity=self.local_search_intensity)  # Modified line\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values, intensity=0.1):  # Modified line\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, intensity, self.dim)  # Modified line\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 44, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.009265665005116719, 0.009276345187930857, 0.009269421037870318, 0.07864663281942286, 0.07875149985603669, 0.0786815640090921, 0.0710939160995353, 0.07117596456168218, 0.07112176557123717]}}
{"id": "bbad2ad3-9da6-4262-b0bc-fad5a70cda6c", "fitness": 0.053031335383140726, "name": "QuantumAdaptivePSO", "description": "Enhance dynamic parameter adaptation and local exploitation mechanisms in QuantumAdaptivePSO for improved convergence and solution precision.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 60  # Increased max swarm size for better exploration\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.3  # Reduced min inertia for more exploitation\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.05, self.max_c1)  # Reduced change rate for stability\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.05, self.max_c2)  # Reduced change rate for stability\n\n            if self.evaluations % (self.budget // 8) == 0:  # More frequent local search\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 30 and swarm_size > self.min_swarm_size:  # Adjusted thresholds\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 10 and swarm_size < self.max_swarm_size:  # Adjusted thresholds\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.05, self.dim)  # Reduced perturbation size for precision\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 45, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.009265285457383365, 0.009276488013958306, 0.00926961856614783, 0.07864275696499179, 0.07875295724716724, 0.07868358145344567, 0.07109090782526395, 0.07117710081972817, 0.07112332210018024]}}
{"id": "9e0ffcde-bdfc-4f95-ac03-14a4048b46ef", "fitness": 0.05303144238719218, "name": "QuantumAdaptivePSO", "description": "Enhance the adaptive quantum potential by increasing the influence of the diversity measure on `q_factor`.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            diversity = np.mean(np.std(positions, axis=0))  # Moved before q_factor calculation\n\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * diversity / 2, self.dim)  # Changed line\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 46, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.009265712618685695, 0.009276287005029693, 0.009269442319909782, 0.07864711912534805, 0.07875090667396467, 0.07868178113308688, 0.07109431010449685, 0.07117550445857501, 0.07112191804563295]}}
{"id": "f4ebb491-c834-4e6a-a274-ab256f9a4c05", "fitness": 0.055790869200128185, "name": "QuantumAdaptivePSO", "description": "Enhance the adaptive behavior by refining the adjustment of inertia in response to velocity magnitude for improved convergence.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.075, self.min_inertia)  # Adjusted from 0.05 to 0.075\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 47, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05579 with standard deviation 0.03314.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.010294512153283475, 0.009276488013958306, 0.009300447994416983, 0.09209994031908553, 0.07875295724716724, 0.079020661745419, 0.08081508814786931, 0.07117710081972817, 0.07138062636022569]}}
{"id": "c082fd48-a2bd-41f7-a0c7-d8d92ce559ae", "fitness": -Infinity, "name": "QuantumAdaptivePSO", "description": "Enhance diversity and convergence by introducing a Lvy flight mechanism and adaptive mutation in QuantumAdaptivePSO.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                # Lvy flight mechanism\n                if np.random.rand() < 0.1:\n                    positions[i] += self.levy_flight()\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value\n\n    def levy_flight(self):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / np.abs(v) ** (1 / beta)\n        return step", "configspace": "", "generation": 48, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {}}
{"id": "ed2d2a3e-6070-4d6f-94a9-2a9a9a52c406", "fitness": -Infinity, "name": "EnhancedQuantumAdaptivePSO", "description": "Improved QuantumAdaptivePSO by incorporating non-linear inertia adaptation and a dynamic restart mechanism for sustained exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.restart_threshold = 0.1 * budget\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = np.copy(personal_best_positions[global_best_index])\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = np.copy(positions[i])\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = np.copy(personal_best_positions[current_best_index])\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia * 0.98, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia * 1.02, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n            if self.evaluations > self.restart_threshold:\n                self.dynamic_restart(func, positions, personal_best_positions, personal_best_values, lb, ub)\n\n        return global_best_position, global_best_value\n    \n    def dynamic_restart(self, func, positions, personal_best_positions, personal_best_values, lb, ub):\n        worst_indices = np.argsort(personal_best_values)[-len(positions)//2:]\n        for i in worst_indices:\n            positions[i] = np.random.uniform(lb, ub, self.dim)\n            velocities[i] = np.zeros(self.dim)\n            personal_best_positions[i] = np.copy(positions[i])\n            personal_best_values[i] = func(positions[i])\n            self.evaluations += 1", "configspace": "", "generation": 49, "feedback": "An exception occurred: NameError(\"name 'velocities' is not defined\").", "error": "NameError(\"name 'velocities' is not defined\")", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {}}
{"id": "dbab904f-3245-4a57-a22a-6fefaca7a15b", "fitness": 0.055790869200128185, "name": "EnhancedQuantumAdaptivePSO", "description": "Enhance QuantumAdaptivePSO by introducing dynamic swarm restructuring based on performance trends and adaptive memory to effectively balance exploration and exploitation.  ", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 60\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.performance_memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            self.performance_memory.append(global_best_value)\n            if len(self.performance_memory) > 5:\n                self.performance_memory.pop(0)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n            if len(self.performance_memory) == 5 and np.std(self.performance_memory) < 1e-3:\n                self.c1, self.c2 = 2.5, 2.5\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05579 with standard deviation 0.03314.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.010294512153283475, 0.009276488013958306, 0.009300447994416983, 0.09209994031908553, 0.07875295724716724, 0.079020661745419, 0.08081508814786931, 0.07117710081972817, 0.07138062636022569]}}
{"id": "331a45ed-f352-4e23-b172-8543e46f46e3", "fitness": 0.0557168723344696, "name": "QuantumAdaptivePSO", "description": "Enhance QuantumAdaptivePSO with adaptive velocity scaling and dynamic random restart for improved robustness and convergence speed.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n        self.velocity_scaling_factor = 0.7  # New parameter for velocity scaling\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                velocities[i] *= self.velocity_scaling_factor  # Apply velocity scaling\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n            if self.evaluations % (self.budget // 5) == 0:  # New: dynamic random restart\n                restart_positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n                restart_values = np.array([func(p) for p in restart_positions])\n                self.evaluations += swarm_size\n                for j in range(swarm_size):\n                    if restart_values[j] < personal_best_values[j]:\n                        personal_best_positions[j] = restart_positions[j]\n                        personal_best_values[j] = restart_values[j]\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 51, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.010294502317021115, 0.009274839497006693, 0.009269144954450637, 0.09209984044870723, 0.07873613681164782, 0.07867874433542721, 0.08081501831495885, 0.07116405743688148, 0.07111956689412535]}}
{"id": "c758c5a8-bd25-4e89-b7f1-16152c4074bc", "fitness": 0.055790869200128185, "name": "QuantumAdaptivePSO", "description": "Improve QuantumAdaptivePSO by increasing local search efficiency and incorporating adaptive swarm size adjustments based on current performance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values, lb, ub)  # Added lb, ub\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.zeros((1, self.dim))\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values, lb, ub):  # Added lb, ub parameters\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, lb, ub)  # Used lb and ub\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 52, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05579 with standard deviation 0.03314.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.010294512153283475, 0.009276488013958306, 0.009300447994416983, 0.09209994031908553, 0.07875295724716724, 0.079020661745419, 0.08081508814786931, 0.07117710081972817, 0.07138062636022569]}}
{"id": "75054bee-10a4-47b3-a467-d323f1e69987", "fitness": 0.05302818895990567, "name": "EnhancedQuantumAdaptivePSO", "description": "Enhance QuantumAdaptivePSO by integrating self-learning behavior updates and dynamic velocity control to improve convergence and exploration balance.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (swarm_size, self.dim)) * (ub - lb) / 10\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                velocities[i] = np.clip(velocities[i], -(ub - lb) / 2, (ub - lb) / 2)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            if mean_velocity_magnitude < (ub - lb).mean() / 15:\n                self.inertia = max(self.inertia - 0.05, self.min_inertia)\n                self.c1 = min(self.c1 + 0.1, self.max_c1)\n                self.c2 = max(self.c2 - 0.05, self.min_c2)\n            else:\n                self.inertia = min(self.inertia + 0.05, self.max_inertia)\n                self.c1 = max(self.c1 - 0.05, self.min_c1)\n                self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            if diversity < (ub - lb).mean() / 25 and swarm_size > self.min_swarm_size:\n                swarm_size = max(swarm_size - 1, self.min_swarm_size)\n                positions = positions[:swarm_size]\n                velocities = velocities[:swarm_size]\n                personal_best_positions = personal_best_positions[:swarm_size]\n                personal_best_values = personal_best_values[:swarm_size]\n            elif diversity > (ub - lb).mean() / 12 and swarm_size < self.max_swarm_size:\n                additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n                additional_velocities = np.random.uniform(-1, 1, (1, self.dim)) * (ub - lb) / 10\n                positions = np.vstack((positions, additional_positions))\n                velocities = np.vstack((velocities, additional_velocities))\n                personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n                additional_values = np.array([func(pos) for pos in additional_positions])\n                personal_best_values = np.append(personal_best_values, additional_values)\n                self.evaluations += 1\n                swarm_size += 1\n\n        return global_best_position, global_best_value\n    \n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.009265532207198834, 0.009274654617434486, 0.00926972153957184, 0.0786452951871951, 0.07873425211214313, 0.07868463273692439, 0.07109287966524525, 0.07116260128677865, 0.0711241312866594]}}
{"id": "1c2fd11f-6273-40aa-9490-d3f078c18737", "fitness": 0.0565101886180878, "name": "QuantumAdaptivePSO", "description": "Enhance QuantumAdaptivePSO with dynamic swarm size adjustment using entropy-based diversity metrics for improved convergence and exploration balance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 54, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["cacd03ba-5a61-4c78-b72e-993ce3c9ba59"], "operator": null, "metadata": {"aucs": [0.010294512153283475, 0.009276488013958306, 0.009596227966547133, 0.09209994031908553, 0.07875295724716724, 0.08256435203264234, 0.08081508814786931, 0.07117710081972817, 0.07401503086250871]}}
{"id": "820e3606-a4d0-4e8a-aa16-003b6e2fec0f", "fitness": 0.05651030457555457, "name": "QuantumAdaptivePSO", "description": "Enhance convergence by slightly increasing the adaptive_q_factor_weight for more effective exploration.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.12  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 55, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["1c2fd11f-6273-40aa-9490-d3f078c18737"], "operator": null, "metadata": {"aucs": [0.01029450956909883, 0.009276553466333048, 0.009596220646756692, 0.09209991387820338, 0.07875362566780841, 0.08256427733144522, 0.08081506763556745, 0.07117761278724088, 0.07401496019753717]}}
{"id": "f30c5f8b-b085-4037-82f7-6ff25e88dab6", "fitness": 0.05651030548784676, "name": "QuantumAdaptivePSO", "description": "Adjust the adaptive_q_factor_weight to enhance exploration and convergence balance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.15  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 56, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["820e3606-a4d0-4e8a-aa16-003b6e2fec0f"], "operator": null, "metadata": {"aucs": [0.010294505139425203, 0.009276570690980757, 0.009596207791074507, 0.09209986858809127, 0.07875380160327905, 0.082564146142312, 0.08081504174621967, 0.07117774933295462, 0.07401485835628374]}}
{"id": "11fb8f7c-220d-4c04-b199-038f13b03d7f", "fitness": -Infinity, "name": "QuantumEnhancedAdaptivePSO", "description": "Introduce entropy and diversity-driven adaptive parameter tuning in QuantumAdaptivePSO to refine convergence dynamics.", "code": "import numpy as np\n\nclass QuantumEnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            \n            self.adaptive_parameter_tuning(entropy, diversity, mean_velocity_magnitude, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def adaptive_parameter_tuning(self, entropy, diversity, mean_velocity_magnitude, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n        \n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 57, "feedback": "An exception occurred: NameError(\"name 'positions' is not defined\").", "error": "NameError(\"name 'positions' is not defined\")", "parent_ids": ["f30c5f8b-b085-4037-82f7-6ff25e88dab6"], "operator": null, "metadata": {}}
{"id": "c60ffa86-37d9-45e6-8388-ce74cb217897", "fitness": 0.05572104644635293, "name": "QuantumAdaptivePSO", "description": "Introduce a dynamic inertia weight adjustment based on convergence rate to enhance the exploration-exploitation trade-off.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.15\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n        self.inertia *= (1 - (self.evaluations / self.budget))  # Changed line\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 58, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["f30c5f8b-b085-4037-82f7-6ff25e88dab6"], "operator": null, "metadata": {"aucs": [0.010294505139425203, 0.009276570690980757, 0.009269376194722234, 0.09209986858809127, 0.07875380160327905, 0.07868110620420021, 0.08081504174621967, 0.07117774933295462, 0.07112139851730337]}}
{"id": "4c15baf2-27ff-45ba-b03f-1071f60781db", "fitness": 0.05651030548784676, "name": "QuantumAdaptivePSO", "description": "Refine velocity update using entropy-based scaling for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.15  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            velocities *= 1 + entropy * 0.05  # Adjusted line\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 59, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["f30c5f8b-b085-4037-82f7-6ff25e88dab6"], "operator": null, "metadata": {"aucs": [0.010294505139425203, 0.009276570690980757, 0.009596207791074507, 0.09209986858809127, 0.07875380160327905, 0.082564146142312, 0.08081504174621967, 0.07117774933295462, 0.07401485835628374]}}
{"id": "76b2759c-248b-445c-99fe-0d3c49f7f539", "fitness": 0.05651030641669849, "name": "QuantumAdaptivePSO", "description": "Fine-tune adaptive_q_factor_weight to further improve exploration-exploitation trade-off.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.17  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 60, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["f30c5f8b-b085-4037-82f7-6ff25e88dab6"], "operator": null, "metadata": {"aucs": [0.010294502555282414, 0.009276581887077251, 0.009596198872141692, 0.0920998421515844, 0.07875391596922543, 0.08256405513438281, 0.08081501831964688, 0.07117783809242129, 0.07401480476852429]}}
{"id": "422ac34c-a97a-49cb-b4ac-c046b0e2b143", "fitness": 0.05651030641669849, "name": "QuantumAdaptivePSO", "description": "Adjust the inertia decay rate in response to swarm diversity for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.17  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 61, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["76b2759c-248b-445c-99fe-0d3c49f7f539"], "operator": null, "metadata": {"aucs": [0.010294502555282414, 0.009276581887077251, 0.009596198872141692, 0.0920998421515844, 0.07875391596922543, 0.08256405513438281, 0.08081501831964688, 0.07117783809242129, 0.07401480476852429]}}
{"id": "8daef746-3974-465c-a344-c29374c77e55", "fitness": 0.05651030641669849, "name": "QuantumAdaptivePSO", "description": "Fine-tune inertia to adaptively enhance the balance between exploration and exploitation.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.85  # Adjusted line\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.17\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 62, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["76b2759c-248b-445c-99fe-0d3c49f7f539"], "operator": null, "metadata": {"aucs": [0.010294502555282414, 0.009276581887077251, 0.009596198872141692, 0.0920998421515844, 0.07875391596922543, 0.08256405513438281, 0.08081501831964688, 0.07117783809242129, 0.07401480476852429]}}
{"id": "ef047a5f-b033-425c-b936-e3628d6d808a", "fitness": 0.05572104737520467, "name": "DynamicQuantumPSO", "description": "Dynamic Quantum Particle Swarm Optimization (D-QPSO) dynamically adjusts swarm inertia and exploration-exploitation balance based on local search performance and entropy measures.", "code": "import numpy as np\n\nclass DynamicQuantumPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.base_inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.17  # Adjustable based on performance metrics\n        self.inertia = self.base_inertia\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.dynamic_parameter_update(mean_velocity_magnitude, diversity, lb, ub)\n            self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n\n    def dynamic_parameter_update(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 63, "feedback": "The algorithm DynamicQuantumPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["76b2759c-248b-445c-99fe-0d3c49f7f539"], "operator": null, "metadata": {"aucs": [0.010294502555282414, 0.009276581887077251, 0.00926936727578953, 0.0920998421515844, 0.07875391596922543, 0.07868101519627102, 0.08081501831964688, 0.07117783809242129, 0.0711213449295438]}}
{"id": "71c1e0bf-a1d5-4605-a68e-372fec9a530e", "fitness": 0.05302701197912981, "name": "QuantumAdaptivePSO", "description": "Introduce adaptive mutation and dynamic swarm size adjustment to improve convergence and exploration balance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 60  # Adjusted line\n        self.min_swarm_size = 8   # Adjusted line\n        self.c1 = 2.0\n        self.c2 = 2.5  # Adjusted line\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.7  # Adjusted line\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.15  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                mutation = np.random.normal(0, 0.05, self.dim)  # Added line\n                positions[i] = np.clip(positions[i] + mutation, lb, ub)  # Added line\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 20  # Adjusted line\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 64, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03109.", "error": "", "parent_ids": ["76b2759c-248b-445c-99fe-0d3c49f7f539"], "operator": null, "metadata": {"aucs": [0.009264074945404266, 0.009276617198233583, 0.00926866360648626, 0.07863041102407076, 0.07875427670255353, 0.07867384113007203, 0.07108134093198937, 0.07117811805065544, 0.07111576422270305]}}
{"id": "78fdb42c-8a1d-4eb5-bf22-f862b1df2fb9", "fitness": 0.05651030548784676, "name": "QuantumAdaptivePSO", "description": "Optimize the algorithm by adjusting `adaptive_q_factor_weight` for enhanced exploration-exploitation balance.  ", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.15  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 65, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["76b2759c-248b-445c-99fe-0d3c49f7f539"], "operator": null, "metadata": {"aucs": [0.010294505139425203, 0.009276570690980757, 0.009596207791074507, 0.09209986858809127, 0.07875380160327905, 0.082564146142312, 0.08081504174621967, 0.07117774933295462, 0.07401485835628374]}}
{"id": "7b486bfd-01d7-437a-a863-3181bbf95234", "fitness": 0.05651030641669849, "name": "QuantumAdaptivePSO", "description": "Fine-tune the balance between exploration and exploitation by adjusting inertia.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.85  # Adjusted line\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.17\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 66, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["76b2759c-248b-445c-99fe-0d3c49f7f539"], "operator": null, "metadata": {"aucs": [0.010294502555282414, 0.009276581887077251, 0.009596198872141692, 0.0920998421515844, 0.07875391596922543, 0.08256405513438281, 0.08081501831964688, 0.07117783809242129, 0.07401480476852429]}}
{"id": "4db7f762-131e-4d3c-83c9-b9dfcba2c708", "fitness": 0.055720960199785376, "name": "EnhancedQuantumAdaptivePSO", "description": "Enhance global search capability with adaptive inertia and stochastic restarts to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.17\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions) / 2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n            if np.random.rand() < 0.1:\n                self.stochastic_restart(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value\n\n    def stochastic_restart(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        restart_indices = np.random.choice(len(positions), len(positions) // 5, replace=False)\n        for idx in restart_indices:\n            new_position = np.random.uniform(lb, ub, self.dim)\n            positions[idx] = new_position\n            velocities[idx] = np.zeros(self.dim)\n            personal_best_positions[idx] = new_position\n            personal_best_values[idx] = func(new_position)\n            self.evaluations += 1", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["76b2759c-248b-445c-99fe-0d3c49f7f539"], "operator": null, "metadata": {"aucs": [0.010294502555282414, 0.009276581887077251, 0.00926932618302967, 0.0920998421515844, 0.07875391596922543, 0.0786805954987082, 0.08081501831964688, 0.07117783809242129, 0.07112102114109287]}}
{"id": "29e9f4d1-dfa3-411e-ba8f-e728d459291a", "fitness": 0.05651030641669849, "name": "QuantumAdaptivePSO", "description": "Enhance adaptive parameters by incorporating an entropy-weighted adaptive mechanism for improved convergence.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.17\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions) / 2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n\n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        entropy_weight = np.clip(diversity / (ub - lb).mean(), 0.0, 1.0)\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05 * entropy_weight, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1 * entropy_weight, self.max_c1)\n            self.c2 = max(self.c2 - 0.05 * entropy_weight, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05 * entropy_weight, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05 * entropy_weight, self.min_c1)\n            self.c2 = min(self.c2 + 0.1 * entropy_weight, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 68, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["76b2759c-248b-445c-99fe-0d3c49f7f539"], "operator": null, "metadata": {"aucs": [0.010294502555282414, 0.009276581887077251, 0.009596198872141692, 0.0920998421515844, 0.07875391596922543, 0.08256405513438281, 0.08081501831964688, 0.07117783809242129, 0.07401480476852429]}}
{"id": "9e3a2bc2-474d-48d9-9a1a-728653571ac3", "fitness": 0.05651030668885545, "name": "QuantumAdaptivePSO", "description": "Slightly increase `adaptive_q_factor_weight` to improve exploration.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.18  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 69, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["76b2759c-248b-445c-99fe-0d3c49f7f539"], "operator": null, "metadata": {"aucs": [0.010294501078740859, 0.009276587915769352, 0.009596195489205428, 0.09209982705655362, 0.0787539775534638, 0.08256402062526125, 0.08081500806425423, 0.07117788588753138, 0.07401475652891909]}}
{"id": "01d81837-3947-4147-9864-dc9702cc3098", "fitness": 0.0557210476473616, "name": "DynamicAdaptivePSO", "description": "Introduce dynamic learning factors and adaptive inertia for enhanced global and local search balance.", "code": "import numpy as np\n\nclass DynamicAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 4.0\n        self.min_c2 = 0.5\n        self.max_c2 = 4.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.18\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub, global_best_value, personal_best_values)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub, global_best_value, personal_best_values):\n        convergence_speed = 1.0 - diversity / ((ub - lb).mean())\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + convergence_speed, self.max_c1)\n            self.c2 = max(self.c2 - convergence_speed, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - convergence_speed, self.min_c1)\n            self.c2 = min(self.c2 + convergence_speed, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 70, "feedback": "The algorithm DynamicAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["9e3a2bc2-474d-48d9-9a1a-728653571ac3"], "operator": null, "metadata": {"aucs": [0.010294501078740859, 0.009276587915769352, 0.009269363892853155, 0.09209982705655362, 0.0787539775534638, 0.07868098068714946, 0.08081500806425423, 0.07117788588753138, 0.0711212966899385]}}
{"id": "996eb97a-5daa-4ebd-815a-4bfc85de0542", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Modify the adaptive_q_factor_weight and update_parameters to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:  # Adjusted line\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 71, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["9e3a2bc2-474d-48d9-9a1a-728653571ac3"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "a6bbf229-81a6-495e-9410-d572bcf7a84c", "fitness": 0.056510289318098206, "name": "QuantumAdaptivePSO", "description": "Slightly increase the adaptive_q_factor_weight for enhanced diversity and exploration in the PSO.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.25  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 72, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294492465104499, 0.009276617198233583, 0.009596166396164008, 0.09209973895280821, 0.07875427670255353, 0.08256372386906974, 0.08081493970918974, 0.07117811805065544, 0.07401453051910512]}}
{"id": "7a257a3a-e57a-4055-adef-3992c381c5a8", "fitness": -Infinity, "name": "RefinedAdaptivePSO", "description": "Introduce self-adaptive inertia and cognitive/social coefficients with dynamic entropy-based swarm size adjustment.", "code": "import numpy as np\n\nclass RefinedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n\n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = np.random.uniform(self.min_c1, self.max_c1)\n            self.c2 = np.random.uniform(self.min_c2, self.max_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = np.random.uniform(self.min_c1, self.max_c1)\n            self.c2 = np.random.uniform(self.min_c2, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 73, "feedback": "An exception occurred: AttributeError(\"'RefinedAdaptivePSO' object has no attribute 'c1'\").", "error": "AttributeError(\"'RefinedAdaptivePSO' object has no attribute 'c1'\")", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {}}
{"id": "7e6ee087-1505-4551-96e6-fe371bdbc88f", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Introduce a subtle conditional check to tweak inertia for better balance between exploration and exploitation.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if diversity > (ub - lb).mean() / 20:  # Adjusted line\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 74, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "ae039917-555e-4beb-a135-00b3d5543db3", "fitness": 0.05651030809115105, "name": "EnhancedQuantumAdaptivePSO", "description": "Enhance exploration-exploitation balance by introducing adaptive self-adjustment of swarm parameters and dynamic diversity control.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions) / 2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 5) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values) + 1e-10))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n\n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        velocity_threshold = (ub - lb).mean() / 10\n        if mean_velocity_magnitude < velocity_threshold: \n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        diversity_threshold = (ub - lb).mean() / 20\n        if diversity < diversity_threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size()\n        elif diversity > diversity_threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self):\n        positions, velocities, personal_best_positions, personal_best_values =  positions[:-1], velocities[:-1], personal_best_positions[:-1], personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_position = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocity = np.zeros((1, self.dim))\n        additional_value = np.array([func(pos) for pos in additional_position])\n        positions = np.vstack((positions, additional_position))\n        velocities = np.vstack((velocities, additional_velocity))\n        personal_best_positions = np.vstack((personal_best_positions, additional_position))\n        personal_best_values = np.append(personal_best_values, additional_value)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "414d4dfe-ecb6-4230-8453-cd92fd586cf7", "fitness": 0.053820950199447895, "name": "EnhancedQuantumAdaptivePSO", "description": "Introduce a dynamically adjusted local search intensity and integrate a memory-based velocity update to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n        self.memory_factor = 0.5  # New parameter for memory factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions) / 2, self.dim)\n                memory_velocity = self.memory_factor * velocities[i]  # New memory-based velocity component\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 memory_velocity + q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n\n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, positions, velocities, personal_best_positions, personal_best_values):\n        if len(positions) > self.min_swarm_size:\n            indices = np.random.choice(len(positions), len(positions) - 1, replace=False)\n            positions = positions[indices]\n            velocities = velocities[indices]\n            personal_best_positions = personal_best_positions[indices]\n            personal_best_values = personal_best_values[indices]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation_magnitude = 0.1 * (1 - self.evaluations / self.budget)  # Dynamic local search intensity\n            perturbation = np.random.normal(0, perturbation_magnitude, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05382 with standard deviation 0.03162.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.009265605396976184, 0.00927659911195744, 0.009596186016822772, 0.07864602410884092, 0.07875409192897553, 0.08256392398311163, 0.07109344364538983, 0.07117797465275955, 0.07401470295019719]}}
{"id": "3dd87dc0-41c0-416e-a24b-1ab568f82055", "fitness": 0.05651030809115105, "name": "EnhancedQuantumAdaptivePSO", "description": "Enhance QuantumAdaptivePSO by dynamically adjusting adaptive_q_factor_weight based on global best improvement to balance exploration-exploitation.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub, global_best_value)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub, global_best_value):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n        \n        # Adjust adaptive_q_factor_weight based on global best improvement\n        if global_best_value < 0.1:  # Assuming the cost function is minimization\n            self.adaptive_q_factor_weight = max(self.adaptive_q_factor_weight - 0.01, 0.05)\n        else:\n            self.adaptive_q_factor_weight = min(self.adaptive_q_factor_weight + 0.01, 0.4)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "5d2aaab9-6c2a-49dc-8ce2-65970443c9af", "fitness": 0.056510289318098206, "name": "QuantumAdaptivePSO", "description": "Slightly increase adaptive_q_factor_weight to enhance exploration at early stages.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.25  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:  # Adjusted line\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 78, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294492465104499, 0.009276617198233583, 0.009596166396164008, 0.09209973895280821, 0.07875427670255353, 0.08256372386906974, 0.08081493970918974, 0.07117811805065544, 0.07401453051910512]}}
{"id": "4683ec73-3b09-4963-bc8c-17f16a43b328", "fitness": 0.056510289318098206, "name": "QuantumAdaptivePSO", "description": "Slightly adjust the adaptive_q_factor_weight for better balance between exploration and exploitation.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.25  # Changed line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:  # Adjusted line\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 79, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294492465104499, 0.009276617198233583, 0.009596166396164008, 0.09209973895280821, 0.07875427670255353, 0.08256372386906974, 0.08081493970918974, 0.07117811805065544, 0.07401453051910512]}}
{"id": "e6747b3d-f939-4b79-b65e-9a20d8b8a358", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Introduce dynamic inertia and coefficients adjustment based on swarm diversity and convergence to improve exploration-exploitation balance in QuantumAdaptivePSO.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            convergence_rate = np.abs((global_best_value - current_best_value) / global_best_value)\n\n            self.update_parameters(convergence_rate, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, convergence_rate, diversity, lb, ub):\n        if convergence_rate < 0.01 and diversity < (ub - lb).mean() / 20:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 80, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "b8b93172-fafa-4813-a264-8b9f2748db38", "fitness": 0.05651030809115105, "name": "EnhancedQuantumAdaptivePSO", "description": "Introduce a dynamic adaptive mechanism for velocity and position update rules to improve convergence adaptability and performance.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia = 0.8\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n            self.dynamic_inertia_update(diversity)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, 0.4)\n            self.c1 = min(self.c1 + 0.1, 3.0)\n            self.c2 = max(self.c2 - 0.05, 0.5)\n        else:\n            self.inertia = min(self.inertia + 0.05, 0.9)\n            self.c1 = max(self.c1 - 0.05, 0.5)\n            self.c2 = min(self.c2 + 0.1, 3.0)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value\n\n    def dynamic_inertia_update(self, diversity):\n        if diversity > 0.1:\n            self.inertia = max(self.inertia - 0.02, 0.4)\n        else:\n            self.inertia = min(self.inertia + 0.02, 0.9)", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "c593820f-4a06-4896-925a-2d0c17aab8ca", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Introduce dynamic entropy-based adaptive strategies for parameter tuning and diversity management to improve convergence efficiency.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub, personal_best_values)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub, personal_best_values):\n        entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n        if mean_velocity_magnitude < (ub - lb).mean() / 12 or entropy < 0.5:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 82, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "a6e7981b-44dc-41f0-a4a9-f7c31328f63b", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Adjusted diversity threshold to refine exploration-exploitation trade-off.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 15  # Adjusted line\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 83, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "6023ce24-7087-45ee-b7ad-290ac22f439c", "fitness": 0.05651023678645348, "name": "QuantumAdaptivePSO", "description": "Fine-tune the adaptive_q_factor_weight to further enhance the algorithm's performance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.3  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 84, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294488710440253, 0.009276617198233583, 0.009596144623109537, 0.0920997004007248, 0.07875427670255353, 0.08256350183449224, 0.0808149083693559, 0.07117811805065544, 0.07401437518851606]}}
{"id": "d1c951b7-6d2f-4de6-bf5a-44bcec4c26b0", "fitness": 0.05572096292054071, "name": "EnhancedQuantumAdaptivePSO", "description": "Enhance adaptive parameters using entropy and diversity analysis to balance exploration-exploitation dynamically.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n\n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        if len(positions) > self.min_swarm_size:\n            indices = np.random.choice(len(positions), len(positions) - 1, replace=False)\n            positions = positions[indices]\n            velocities = velocities[indices]\n            personal_best_positions = personal_best_positions[indices]\n            personal_best_values = personal_best_values[indices]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_count = min(self.max_swarm_size - len(positions), 5)\n        additional_positions = np.random.uniform(lb, ub, (additional_count, self.dim))\n        additional_velocities = np.zeros((additional_count, self.dim))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += additional_count\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05572 with standard deviation 0.03311.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009269313819822989, 0.09209980878371982, 0.07875409192897553, 0.078680469372022, 0.0808149860985472, 0.07117797465275955, 0.07112092322279362]}}
{"id": "a6bf3f4d-1da8-41ab-a57a-09ee19e86d45", "fitness": -Infinity, "name": "QuantumAdaptivePSO", "description": "Integrate dynamic adaptive parameters with elite particle learning to enhance convergence speed and robustness.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n        \n        elite_particle = global_best_position  # Track elite particle\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values, elite_particle)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n\n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values, elite_particle):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value\n        # Elite learning step\n        elite_perturbation = np.random.normal(0, 0.05, self.dim)\n        elite_candidate_position = np.clip(elite_particle + elite_perturbation, func.bounds.lb, func.bounds.ub)\n        elite_candidate_value = func(elite_candidate_position)\n        self.evaluations += 1\n        if elite_candidate_value < global_best_value:\n            global_best_position = elite_candidate_position\n            global_best_value = elite_candidate_value", "configspace": "", "generation": 86, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'global_best_value' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'global_best_value' referenced before assignment\")", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {}}
{"id": "5bca8377-7b64-4086-b77d-a1ce9074e30e", "fitness": -Infinity, "name": "EnhancedQuantumAdaptivePSO", "description": "Introduce dynamic inertia weight and social factors based on swarm convergence speed to improve exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia_max = 0.9\n        self.inertia_min = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity):\n        inertia_delta = (self.inertia_max - self.inertia_min) / 10\n        if mean_velocity_magnitude < diversity / 2:\n            self.inertia = max(self.inertia - inertia_delta, self.inertia_min)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.1, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + inertia_delta, self.inertia_max)\n            self.c1 = max(self.c1 - 0.1, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 87, "feedback": "An exception occurred: AttributeError(\"'EnhancedQuantumAdaptivePSO' object has no attribute 'inertia'\").", "error": "AttributeError(\"'EnhancedQuantumAdaptivePSO' object has no attribute 'inertia'\")", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {}}
{"id": "ce1c5753-f15a-4439-9ef0-b2d57e70e20a", "fitness": 0.056510289318098206, "name": "QuantumAdaptivePSO", "description": "Enhance the exploration by slightly increasing q_factor's influence on the velocity update.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.25  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:  # Adjusted line\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 88, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294492465104499, 0.009276617198233583, 0.009596166396164008, 0.09209973895280821, 0.07875427670255353, 0.08256372386906974, 0.08081493970918974, 0.07117811805065544, 0.07401453051910512]}}
{"id": "03e3bdad-d6fe-4704-8692-8686707c823b", "fitness": -Infinity, "name": "QuantumAdaptivePSO", "description": "Enhance Quantum Adaptive PSO by introducing a dynamic velocity scaling mechanism based on fitness improvement to better balance exploration and exploitation.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n        self.velocity_scale_factor = 0.5  # New parameter for dynamic velocity scaling\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + self.velocity_scale_factor * velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n        self.velocity_scale_factor = 0.5 + 0.5 * (global_best_value / (global_best_value + current_best_value))  # Dynamic scaling\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 89, "feedback": "An exception occurred: NameError(\"name 'global_best_value' is not defined\").", "error": "NameError(\"name 'global_best_value' is not defined\")", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {}}
{"id": "be894740-00a7-4486-9109-dd9b755cc413", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Further adjust the inertia update logic to better adapt to varying mean velocity magnitudes and enhance optimization efficiency.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 15:  # Adjusted line\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 90, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "9f633479-6442-445a-a77f-69214562d5a5", "fitness": 0.053015491078132665, "name": "EnhancedAdaptivePSO", "description": "Enhance exploration-exploitation dynamics by adaptive parameter tuning and stochastic swarm diversification.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 30\n        self.max_swarm_size = 60\n        self.min_swarm_size = 12\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.7\n        self.max_inertia = 0.95\n        self.min_inertia = 0.3\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.25\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        velocity_threshold = (ub - lb).mean() / 10\n        if mean_velocity_magnitude < velocity_threshold: \n            self.inertia = max(self.inertia - 0.07, self.min_inertia)\n            self.c1 = min(self.c1 + 0.15, self.max_c1)\n            self.c2 = max(self.c2 - 0.07, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.07, self.max_inertia)\n            self.c1 = max(self.c1 - 0.07, self.min_c1)\n            self.c2 = min(self.c2 + 0.15, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 20\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 91, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05302 with standard deviation 0.03109.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.009263996459068391, 0.009273763937332924, 0.009266136639655143, 0.07862963314221427, 0.0787251620955215, 0.0786483395935278, 0.07108077157314452, 0.07115553668572394, 0.07109607957700548]}}
{"id": "82e00ec9-2932-4e50-8c50-832f07160610", "fitness": 0.05651030809115105, "name": "EnhancedQuantumAdaptivePSO", "description": "Introduce adaptive mutation based on swarm diversity to improve local search and prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values, diversity)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values, diversity):\n        mutation_strength = max(0.1, 0.2 * (diversity / (np.ptp(positions, axis=0).mean() + 1e-9)))\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, mutation_strength, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 92, "feedback": "The algorithm EnhancedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "1a09f1a2-83bb-41f9-a57f-7f2d9f936fa0", "fitness": 0.053217382555832825, "name": "ImprovedQuantumAdaptivePSO", "description": "Incorporate dynamic neighborhood learning and adaptive velocity scaling to enhance convergence speed and robustness.", "code": "import numpy as np\n\nclass ImprovedQuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n        self.velocity_scaling_factor = 0.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                local_best_position = self.get_local_best(i, personal_best_positions, personal_best_values)\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions) / 2, self.dim)\n                velocities[i] = self.velocity_scaling_factor * (\n                    self.inertia * velocities[i] +\n                    self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                    self.c2 * r2 * (local_best_position - positions[i]) +\n                    q_factor\n                )\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n\n    def get_local_best(self, index, personal_best_positions, personal_best_values):\n        neighborhood_size = 5\n        neighbors_indices = np.random.choice(len(personal_best_positions), neighborhood_size, replace=False)\n        local_best_index = neighbors_indices[np.argmin(personal_best_values[neighbors_indices])]\n        return personal_best_positions[local_best_index]\n\n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, 3.0)\n            self.c2 = max(self.c2 - 0.05, 0.5)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, 0.5)\n            self.c2 = min(self.c2 + 0.1, 3.0)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 93, "feedback": "The algorithm ImprovedQuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05322 with standard deviation 0.03121.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.00935172531583206, 0.009275302133036845, 0.009268971488278321, 0.07955960001263496, 0.0787408536284332, 0.07867697352271685, 0.07179711927692634, 0.07116770673611539, 0.07111819088852145]}}
{"id": "d53ae437-53b5-4703-ab11-79590187c4aa", "fitness": 0.0530319470998175, "name": "QuantumAdaptivePSO", "description": "Refined QuantumAdaptivePSO with enhanced adaptive coefficient adjustments and local search exploitation.  ", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.3  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 8) == 0:  # Adjusted line\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 10:  # Adjusted line\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.15, self.max_c1)  # Adjusted line\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.15, self.max_c2)  # Adjusted line\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.05, self.dim)  # Adjusted line\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 94, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05303 with standard deviation 0.03110.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.009265554216049021, 0.009276617198233583, 0.009269507418014467, 0.07864550142270488, 0.07875427670255353, 0.07868244732938257, 0.07109305502059682, 0.07117811805065544, 0.0711224465401672]}}
{"id": "34da0e6e-2a5a-4f55-b65f-c4815612ba79", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Introduce adaptive inertia updates based on diversity to improve exploration-exploitation balance in QuantumAdaptivePSO.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:  # Adjusted line\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05 + 0.1 * diversity, self.max_inertia)  # Adjusted line\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 95, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "b84a88b5-02e7-4815-b104-a5560883bae0", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Introduce a dynamic inertia factor adjustment based on swarm diversity to enhance convergence and exploration balance.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        diversity_threshold = (ub - lb).mean() / 10\n        if diversity > diversity_threshold:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n        else:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 96, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "98bc2769-e732-4294-839b-31d9c1a9a022", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Fine-tune the update strategy for inertia and acceleration coefficients to enhance convergence speed.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:  # Adjusted line\n            self.inertia = max(self.inertia - 0.04, self.min_inertia)  # Slightly adjusted line\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 97, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "7bc90320-5e64-4040-b860-471c7940c7a5", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Enhance exploration-exploitation balance by dynamically adjusting inertia and acceleration coefficients based on entropy and diversity.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 12:\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 98, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
{"id": "b275ff70-156e-4a20-bf9c-a9ad5786e29e", "fitness": 0.05651030809115105, "name": "QuantumAdaptivePSO", "description": "Enhance parameter update logic for improved balance between exploration and exploitation.", "code": "import numpy as np\n\nclass QuantumAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarm_size = 20\n        self.max_swarm_size = 50\n        self.min_swarm_size = 10\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.min_c1 = 0.5\n        self.max_c1 = 3.0\n        self.min_c2 = 0.5\n        self.max_c2 = 3.0\n        self.inertia = 0.8\n        self.max_inertia = 0.9\n        self.min_inertia = 0.4\n        self.evaluations = 0\n        self.adaptive_q_factor_weight = 0.2  # Adjusted line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm_size = self.initial_swarm_size\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.zeros((swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_values = np.array([func(p) for p in positions])\n        self.evaluations += swarm_size\n\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index]\n        global_best_value = personal_best_values[global_best_index]\n\n        while self.evaluations < self.budget:\n            r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n            for i in range(swarm_size):\n                q_factor = np.random.normal(0, self.adaptive_q_factor_weight * np.std(personal_best_positions)/2, self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - positions[i]) +\n                                 self.c2 * r2 * (global_best_position - positions[i]) +\n                                 q_factor)\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                current_value = func(positions[i])\n                self.evaluations += 1\n\n                if current_value < personal_best_values[i]:\n                    personal_best_positions[i] = positions[i]\n                    personal_best_values[i] = current_value\n\n            current_best_index = np.argmin(personal_best_values)\n            current_best_value = personal_best_values[current_best_index]\n            if current_best_value < global_best_value:\n                global_best_position = personal_best_positions[current_best_index]\n                global_best_value = current_best_value\n\n            diversity = np.mean(np.std(positions, axis=0))\n            mean_velocity_magnitude = np.mean(np.linalg.norm(velocities, axis=1))\n\n            self.update_parameters(mean_velocity_magnitude, diversity, lb, ub)\n\n            if self.evaluations % (self.budget // 10) == 0:\n                self.local_search(func, positions, personal_best_positions, personal_best_values)\n\n            entropy = -np.sum((personal_best_values / np.sum(personal_best_values)) * np.log(personal_best_values / np.sum(personal_best_values)))\n            self.adjust_swarm_size(entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values)\n\n        return global_best_position, global_best_value\n    \n    def update_parameters(self, mean_velocity_magnitude, diversity, lb, ub):\n        if mean_velocity_magnitude < (ub - lb).mean() / 10:  # Adjusted line\n            self.inertia = max(self.inertia - 0.05, self.min_inertia)\n            self.c1 = min(self.c1 + 0.1, self.max_c1)\n            self.c2 = max(self.c2 - 0.05, self.min_c2)\n        else:\n            self.inertia = min(self.inertia + 0.05, self.max_inertia)\n            self.c1 = max(self.c1 - 0.05, self.min_c1)\n            self.c2 = min(self.c2 + 0.1, self.max_c2)\n\n    def adjust_swarm_size(self, entropy, diversity, lb, ub, func, positions, velocities, personal_best_positions, personal_best_values):\n        threshold = (ub - lb).mean() / 25\n        if diversity < threshold and entropy < 0.5 and len(positions) > self.min_swarm_size:\n            self.decrease_swarm_size(func, positions, velocities, personal_best_positions, personal_best_values)\n        elif diversity > threshold and len(positions) < self.max_swarm_size:\n            self.increase_swarm_size(func, lb, ub, positions, velocities, personal_best_positions, personal_best_values)\n\n    def decrease_swarm_size(self, func, positions, velocities, personal_best_positions, personal_best_values):\n        positions = positions[:-1]\n        velocities = velocities[:-1]\n        personal_best_positions = personal_best_positions[:-1]\n        personal_best_values = personal_best_values[:-1]\n\n    def increase_swarm_size(self, func, lb, ub, positions, velocities, personal_best_positions, personal_best_values):\n        additional_positions = np.random.uniform(lb, ub, (1, self.dim))\n        additional_velocities = np.zeros((1, self.dim))\n        positions = np.vstack((positions, additional_positions))\n        velocities = np.vstack((velocities, additional_velocities))\n        personal_best_positions = np.vstack((personal_best_positions, additional_positions))\n        additional_values = np.array([func(pos) for pos in additional_positions])\n        personal_best_values = np.append(personal_best_values, additional_values)\n        self.evaluations += 1\n\n    def local_search(self, func, positions, personal_best_positions, personal_best_values):\n        for i, pos in enumerate(personal_best_positions):\n            perturbation = np.random.normal(0, 0.1, self.dim)\n            candidate_position = np.clip(pos + perturbation, func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_position)\n            self.evaluations += 1\n            if candidate_value < personal_best_values[i]:\n                personal_best_positions[i] = candidate_position\n                personal_best_values[i] = candidate_value", "configspace": "", "generation": 99, "feedback": "The algorithm QuantumAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.05651 with standard deviation 0.03353.", "error": "", "parent_ids": ["996eb97a-5daa-4ebd-815a-4bfc85de0542"], "operator": null, "metadata": {"aucs": [0.010294499294268289, 0.00927659911195744, 0.009596186016822772, 0.09209980878371982, 0.07875409192897553, 0.08256392398311163, 0.0808149860985472, 0.07117797465275955, 0.07401470295019719]}}
