{"id": "8e98ae40-7828-4512-973d-b4d7eacf803f", "fitness": 0.03153690759020575, "name": "APSO", "description": "An adaptive particle swarm optimization (APSO) that dynamically adjusts its parameters based on swarm behavior to efficiently explore and exploit the search space.", "code": "import numpy as np\n\nclass APSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n    \n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n    \n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n                \n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n                    \n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n            \n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 0, "feedback": "The algorithm APSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03154 with standard deviation 0.01910.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.04052858885326682, 0.040916711661338256, 0.040280951740921056, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05067746606598045, 0.049075479877744965, 0.0473529701126002]}}
{"id": "58292896-5ce2-4637-b929-3461a6643fa6", "fitness": 0.02574204828420422, "name": "EAPSO", "description": "Enhanced Adaptive Particle Swarm Optimization with Dynamic Neighborhood Topology and Constriction Coefficient for improved convergence rate and solution quality on varied black box optimization problems.", "code": "import numpy as np\n\nclass EAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.neighborhood_size = max(2, int(0.1 * self.num_particles))\n    \n    def constriction_coefficient(self, phi):\n        return 2 / abs(2 - phi - np.sqrt(phi**2 - 4*phi))\n    \n    def update_velocity(self, particle_idx, neighbors_best):\n        phi = self.c1 + self.c2\n        k = self.constriction_coefficient(phi)\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (neighbors_best - self.positions[particle_idx])\n        new_velocity = k * (inertia + cognitive + social)\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        neighbors = self.get_neighbors(particle_idx)\n        neighbors_best = min(neighbors, key=lambda x: self.personal_best_values[x])\n        neighbors_best_position = self.personal_best_positions[neighbors_best]\n        self.velocities[particle_idx] = self.update_velocity(particle_idx, neighbors_best_position)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n    \n    def get_neighbors(self, particle_idx):\n        # Use ring topology for neighborhood\n        neighbors = [(particle_idx + i) % self.num_particles for i in range(-self.neighborhood_size // 2, self.neighborhood_size // 2 + 1)]\n        return neighbors\n    \n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n                \n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n                    \n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n            \n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 1, "feedback": "The algorithm EAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02574 with standard deviation 0.01857.", "error": "", "parent_ids": ["8e98ae40-7828-4512-973d-b4d7eacf803f"], "operator": null, "metadata": {"aucs": [0.03447258879528048, 0.03503780958782243, 0.03495127292412048, 0.0, 0.0, 0.0, 0.045512088862554756, 0.04407588437647059, 0.03762879001158925]}}
{"id": "027de75f-54af-4387-bda8-e1496ce4caa4", "fitness": 0.03153240351019564, "name": "APSO", "description": "Improved APSO with dynamic adjustment of cognitive and social parameters based on success rate for enhanced convergence.", "code": "import numpy as np\n\nclass APSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n    \n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n    \n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n                \n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n                    \n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1, self.c2 = self.c1 * 0.95, self.c2 * 1.05\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1, self.c2 = self.c1 * 1.05, self.c2 * 0.95\n            \n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 2, "feedback": "The algorithm APSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03153 with standard deviation 0.01909.", "error": "", "parent_ids": ["8e98ae40-7828-4512-973d-b4d7eacf803f"], "operator": null, "metadata": {"aucs": [0.040515232002263035, 0.040916655181012285, 0.040280704781320065, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05067686910887559, 0.0490085937592849, 0.04739357675900491]}}
{"id": "cf4c6e4b-a5e6-49a8-8b9a-ad8f4ea4dfe5", "fitness": -Infinity, "name": "EAPSO", "description": "An enhanced adaptive particle swarm optimization (EAPSO) with adaptive learning rates and dynamic neighborhood topology to improve convergence speed and accuracy.", "code": "import numpy as np\n\nclass EAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.neighborhood_radius = max(1, self.num_particles // 10)\n    \n    def get_neighbors(self, index):\n        distances = np.linalg.norm(self.positions - self.positions[index], axis=1)\n        neighbor_indices = np.argsort(distances)[:self.neighborhood_radius]\n        return neighbor_indices\n    \n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social_positions = [self.personal_best_positions[i] for i in self.get_neighbors(particle_idx)]\n        social_best = min(social_positions, key=lambda pos: func(pos))\n        social = self.c2 * np.random.rand(self.dim) * (social_best - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n    \n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n                \n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n                    \n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            \n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 3, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["8e98ae40-7828-4512-973d-b4d7eacf803f"], "operator": null, "metadata": {}}
{"id": "d6522e14-f0f8-4272-a00e-be7129941e9a", "fitness": 0.03208664126825827, "name": "EnhancedAPSO", "description": "Enhanced APSO with dynamic velocity and adaptive learning rates based on convergence state and diversity metrics for improved exploration and exploitation balancing.", "code": "import numpy as np\n\nclass EnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1  # Initial threshold for diversity\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03209 with standard deviation 0.01942.", "error": "", "parent_ids": ["8e98ae40-7828-4512-973d-b4d7eacf803f"], "operator": null, "metadata": {"aucs": [0.04536245695392804, 0.040916858382068044, 0.04028104188027559, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05067686910887559, 0.049182276357119026, 0.04736026873205812]}}
{"id": "2e891eef-c38c-4181-a942-8f0ad76983df", "fitness": -Infinity, "name": "DAPSO", "description": "Dynamic Adaptive PSO (DAPSO) with time-varying parameters and opposition-based learning to enhance convergence speed and solution quality.", "code": "import numpy as np\n\nclass DAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1  # Initial threshold for diversity\n\n    def update_velocity(self, particle_idx, iter_fraction):\n        inertia = (0.5 + iter_fraction) * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx, self.iter_fraction)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def opposition_learning(self):\n        for i in range(self.num_particles):\n            opposite_position = func.bounds.lb + func.bounds.ub - self.positions[i]\n            opposite_position = np.clip(opposite_position, func.bounds.lb, func.bounds.ub)\n            opposite_value = func(opposite_position)\n            if opposite_value < self.personal_best_values[i]:\n                self.personal_best_values[i] = opposite_value\n                self.personal_best_positions[i] = opposite_position\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            self.iter_fraction = eval_count / self.budget\n            self.opposition_learning()\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism based on diversity\n            diversity = self.calculate_diversity()\n            if diversity < self.diversity_threshold:\n                self.inertia_weight = min(self.inertia_weight * 1.05, 0.9)\n                self.c1 = max(self.c1 * 0.95, 1.5)\n                self.c2 = min(self.c2 * 1.05, 2.0)\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 5, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["d6522e14-f0f8-4272-a00e-be7129941e9a"], "operator": null, "metadata": {}}
{"id": "4bf0d123-1e47-442e-acf4-6970c5c0b9bf", "fitness": 0.03155190727430005, "name": "EnhancedAPSO", "description": "Refined APSO with dynamic parameter adjustment based on convergence and diversity metrics for enhanced optimization performance.", "code": "import numpy as np\n\nclass EnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1  # Initial threshold for diversity\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03155 with standard deviation 0.01911.", "error": "", "parent_ids": ["d6522e14-f0f8-4272-a00e-be7129941e9a"], "operator": null, "metadata": {"aucs": [0.040549851008304105, 0.040916858382068044, 0.04028104188027559, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05067686910887559, 0.049182276357119026, 0.04736026873205812]}}
{"id": "ec87c00a-9817-4bf1-8354-480ad194962e", "fitness": 0.031559761874655826, "name": "HybridAPSOLevy", "description": "Hybrid Adaptive Particle Swarm Optimization with Lévy Flights for enhanced global exploration and robust local refinement. ", "code": "import numpy as np\n\nclass HybridAPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.random.rand(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) /\n                 (np.math.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / np.abs(v)**(1 / beta)\n        return step\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = np.std(self.positions)\n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < 0.1:\n                # Apply Lévy flight for global exploration\n                for j in range(self.num_particles):\n                    self.positions[j] += self.levy_flight()\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 7, "feedback": "The algorithm HybridAPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03156 with standard deviation 0.01913.", "error": "", "parent_ids": ["d6522e14-f0f8-4272-a00e-be7129941e9a"], "operator": null, "metadata": {"aucs": [0.04044242333613779, 0.0408980587457769, 0.04012880988713263, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05076383974134491, 0.049403672329148995, 0.04740105283236118]}}
{"id": "130e8d81-2830-489e-9f25-4ec8b664de5d", "fitness": 0.03150839622099798, "name": "AdaptiveAPSO", "description": "AdaptiveAPSO with multi-phase dynamic adaptation and diversity-driven restart mechanism for enhanced convergence through strategic exploration and exploitation balancing.", "code": "import numpy as np\n\nclass AdaptiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.phase = 1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def restart_population(self, func):\n        self.positions = np.random.rand(self.num_particles, self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.velocities = np.random.rand(self.num_particles, self.dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values.fill(float('inf'))\n\n    def adapt_parameters(self):\n        if self.phase == 1:\n            self.inertia_weight *= 0.9\n            self.c1 *= 1.1\n        elif self.phase == 2:\n            self.inertia_weight *= 0.95\n            self.c1 *= 0.95\n        else:\n            self.inertia_weight *= 1.05\n            self.c2 *= 1.05\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n\n            if success_rate > 0.6:\n                self.phase = 1\n            elif success_rate < 0.4:\n                self.phase = 2\n            else:\n                self.phase = 3\n\n            self.adapt_parameters()\n\n            if diversity < self.diversity_threshold:\n                self.restart_population(func)\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03151 with standard deviation 0.01909.", "error": "", "parent_ids": ["d6522e14-f0f8-4272-a00e-be7129941e9a"], "operator": null, "metadata": {"aucs": [0.040428226192257055, 0.040798206861272446, 0.04018786470881042, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05067686910887559, 0.049100243215202344, 0.04738415590256395]}}
{"id": "51546594-467e-4ae3-991a-c3722774d2f8", "fitness": 0.03155190727430005, "name": "EnhancedAPSO", "description": "Enhanced APSO with adaptive inertia weight and velocity scaling for improved convergence and exploration.", "code": "import numpy as np\n\nclass EnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1  # Initial threshold for diversity\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n            # New changes\n            if eval_count % (self.num_particles * 10) == 0:  # Apply every 10 iterations\n                self.inertia_weight *= 0.95  # Reduce inertia weight adaptively\n                self.velocity_clamp *= 1.05  # Scale up velocity clamp for exploration\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03155 with standard deviation 0.01911.", "error": "", "parent_ids": ["d6522e14-f0f8-4272-a00e-be7129941e9a"], "operator": null, "metadata": {"aucs": [0.040549851008304105, 0.040916858382068044, 0.04028104188027559, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05067686910887559, 0.049182276357119026, 0.04736026873205812]}}
{"id": "0fa951aa-d24d-4027-a75c-7f102de126c2", "fitness": 0.03162242141173125, "name": "QuantumAPSO", "description": "Adaptive Quantum-Inspired APSO with dynamic quantum rotations and adaptive parameters for enhanced exploration and exploitation in high-dimensional spaces.", "code": "import numpy as np\n\nclass QuantumAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.quantum_delta = 0.05  # Quantum rotation influence factor\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        quantum_rotation = self.quantum_delta * (np.random.rand(self.dim) - 0.5)\n        new_velocity = inertia + cognitive + social + quantum_rotation\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 10, "feedback": "The algorithm QuantumAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03162 with standard deviation 0.01917.", "error": "", "parent_ids": ["d6522e14-f0f8-4272-a00e-be7129941e9a"], "operator": null, "metadata": {"aucs": [0.0405683538366165, 0.041053743209094606, 0.04023786603010082, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05090771451323217, 0.049430448695073514, 0.04740366642146365]}}
{"id": "f96f2d03-0aca-460c-a822-29de546e1a64", "fitness": 0.03148818174365488, "name": "EnhancedAPSO", "description": "Enhanced APSO with adaptive inertia weight and velocity clamping based on dynamic success rate and diversity metrics for robust exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight = max(0.4, self.inertia_weight * 0.9)\n                self.c1 = min(2.0, self.c1 * 1.1)\n                self.c2 = max(1.0, self.c2 * 0.9)\n            elif success_rate < 0.4:\n                self.inertia_weight = min(1.2, self.inertia_weight * 1.1)\n                self.c1 = max(1.0, self.c1 * 0.9)\n                self.c2 = min(2.0, self.c2 * 1.1)\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight = min(1.2, self.inertia_weight * 1.05)\n                self.c1 = max(1.0, self.c1 * 0.95)\n                self.c2 = min(2.0, self.c2 * 1.05)\n\n            self.velocity_clamp = max(0.05, min(0.2, self.velocity_clamp * (1.1 if diversity < self.diversity_threshold else 0.9)))\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03149 with standard deviation 0.01906.", "error": "", "parent_ids": ["d6522e14-f0f8-4272-a00e-be7129941e9a"], "operator": null, "metadata": {"aucs": [0.040420389012319924, 0.04085561134882709, 0.040204715004968694, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05054711661275235, 0.04903409653175539, 0.04733170718227042]}}
{"id": "dbbd74b2-47e4-4b8a-a8aa-7c4c29087564", "fitness": 0.03212712423730985, "name": "EnhancedAPSO", "description": "Enhanced APSO with slightly increased velocity clamp for improved exploration.  ", "code": "import numpy as np\n\nclass EnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15  # Updated from 0.1\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1  # Initial threshold for diversity\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03213 with standard deviation 0.01960.", "error": "", "parent_ids": ["d6522e14-f0f8-4272-a00e-be7129941e9a"], "operator": null, "metadata": {"aucs": [0.04109553510303743, 0.04180983466194954, 0.040598502271265935, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.050691126326385394, 0.0532445981687204, 0.04670452160442995]}}
{"id": "79211657-5ba6-4062-909f-88460cca070d", "fitness": 0.03204061273751414, "name": "EnhancedAPSORefined", "description": "Introducing adaptive inertia and velocity scaling to further refine the Enhanced APSO's exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAPSORefined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n\n            # Adaptive adjustment of parameters based on success rate and diversity\n            if success_rate > 0.6:\n                self.inertia_weight = max(0.4, self.inertia_weight * 0.9)\n                self.velocity_clamp = min(0.3, self.velocity_clamp * 1.05)\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight = min(1.0, self.inertia_weight * 1.1)\n                self.velocity_clamp = max(0.05, self.velocity_clamp * 0.95)\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight = min(1.0, self.inertia_weight * 1.05)\n                self.c1 = max(0.5, self.c1 * 0.95)\n                self.c2 *= 1.05\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedAPSORefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03204 with standard deviation 0.01952.", "error": "", "parent_ids": ["dbbd74b2-47e4-4b8a-a8aa-7c4c29087564"], "operator": null, "metadata": {"aucs": [0.04098085574088739, 0.04172187809807537, 0.040556543559127745, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05060081268094618, 0.052772653790895574, 0.046732770767695]}}
{"id": "499092a4-b05c-49c4-955e-5dcb9d69bc75", "fitness": 0.032146213519717026, "name": "AdaptiveEnhancedAPSO", "description": "Adaptive Enhanced APSO with dynamic velocity scaling to improve convergence speed and diversity control.", "code": "import numpy as np\n\nclass AdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n            # Apply dynamic scaling to velocity clamp\n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 14, "feedback": "The algorithm AdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03215 with standard deviation 0.01961.", "error": "", "parent_ids": ["dbbd74b2-47e4-4b8a-a8aa-7c4c29087564"], "operator": null, "metadata": {"aucs": [0.04117757437900016, 0.041881594673602085, 0.040634307932491676, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.050515454475939525, 0.05340383767816448, 0.0467031525382553]}}
{"id": "7e36c4ac-c1f9-479f-a60e-a62bdac545ee", "fitness": 0.032146006897454504, "name": "AdaptiveEnhancedAPSO", "description": "Enhanced APSO with adaptive learning rates for improved global and personal exploration.", "code": "import numpy as np\n\nclass AdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        new_velocity = inertia + cognitive + social\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n            # Apply dynamic scaling to velocity clamp\n            self.velocity_clamp *= dynamic_scaling\n\n            # Adaptive learning rates based on evaluations\n            self.c1 += 0.001 if eval_count < self.budget // 2 else -0.001\n            self.c2 -= 0.001 if eval_count < self.budget // 2 else 0.001\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 15, "feedback": "The algorithm AdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03215 with standard deviation 0.01961.", "error": "", "parent_ids": ["499092a4-b05c-49c4-955e-5dcb9d69bc75"], "operator": null, "metadata": {"aucs": [0.04117762280883963, 0.04188154668453192, 0.040634306910048434, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05051460365760452, 0.05340253890145452, 0.0467034431146115]}}
{"id": "7ee03d1e-c570-43eb-9cc5-c6689752ff40", "fitness": 0.03313872074296208, "name": "AdaptiveEnhancedAPSO", "description": "Adaptive Enhanced APSO with a multi-phase velocity update mechanism to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = np.random.normal(0, self.scaling_factor, self.dim)\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n            # Apply dynamic scaling to velocity clamp\n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 16, "feedback": "The algorithm AdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03314 with standard deviation 0.02061.", "error": "", "parent_ids": ["499092a4-b05c-49c4-955e-5dcb9d69bc75"], "operator": null, "metadata": {"aucs": [0.041963757446112915, 0.041813677243560776, 0.04052847026479367, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.056259643826930605, 0.05599225905188954, 0.04669067885337119]}}
{"id": "0202109d-58c4-4143-a876-16875c2c13cc", "fitness": 0.03313872074296208, "name": "EnhancedAdaptiveAPSO", "description": "Enhanced Adaptive APSO with dynamic inertia and velocity scaling for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = np.random.normal(0, self.scaling_factor, self.dim)\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n            # Apply dynamic scaling to velocity clamp\n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedAdaptiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03314 with standard deviation 0.02061.", "error": "", "parent_ids": ["7ee03d1e-c570-43eb-9cc5-c6689752ff40"], "operator": null, "metadata": {"aucs": [0.041963757446112915, 0.041813677243560776, 0.04052847026479367, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.056259643826930605, 0.05599225905188954, 0.04669067885337119]}}
{"id": "d49c69a5-1f62-4a5f-aa2f-c63ce8d5ff63", "fitness": 0.03215700522701573, "name": "AdaptiveEnhancedAPSO", "description": "Enhanced Adaptive APSO with gaussian perturbation for increased exploration and convergence precision.", "code": "import numpy as np\n\nclass AdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = np.random.normal(0, self.scaling_factor, self.dim)\n        perturbation = np.random.normal(0, 0.05, self.dim)  # New line for gaussian perturbation\n        new_velocity = inertia + cognitive + social + exploration_component + perturbation  # Add perturbation\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n            # Apply dynamic scaling to velocity clamp\n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 18, "feedback": "The algorithm AdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03216 with standard deviation 0.01956.", "error": "", "parent_ids": ["7ee03d1e-c570-43eb-9cc5-c6689752ff40"], "operator": null, "metadata": {"aucs": [0.04171465052031764, 0.041682163625768265, 0.04110521306770076, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05221242206010901, 0.0511565773012681, 0.04654202046797773]}}
{"id": "3f8a94c1-e69a-4be4-b330-a15a19e7ee5c", "fitness": 0.03298781320053575, "name": "EnhancedAdaptiveAPSO", "description": "Enhanced Adaptive Enhanced APSO with Dynamic Neighborhood Strategy for improved convergence and diversity management.", "code": "import numpy as np\n\nclass EnhancedAdaptiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.neighborhood_size = min(5, self.num_particles // 2)\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        neighborhood_best_position = self.find_neighborhood_best(particle_idx)\n        social = self.c2 * np.random.rand(self.dim) * (neighborhood_best_position - self.positions[particle_idx])\n        exploration_component = np.random.normal(0, self.scaling_factor, self.dim)\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def find_neighborhood_best(self, particle_idx):\n        start_idx = max(0, particle_idx - self.neighborhood_size // 2)\n        end_idx = min(self.num_particles, particle_idx + self.neighborhood_size // 2)\n        neighborhood_positions = self.positions[start_idx:end_idx]\n        neighborhood_values = self.personal_best_values[start_idx:end_idx]\n        best_idx = np.argmin(neighborhood_values)\n        return neighborhood_positions[best_idx]\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n            # Apply dynamic scaling to velocity clamp\n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedAdaptiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03299 with standard deviation 0.02056.", "error": "", "parent_ids": ["7ee03d1e-c570-43eb-9cc5-c6689752ff40"], "operator": null, "metadata": {"aucs": [0.04148689458655275, 0.041726378021956556, 0.0404237245686222, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05335978211710768, 0.058627841609096176, 0.04626569790148638]}}
{"id": "92e45894-dfc4-4450-926f-e3080d990189", "fitness": 0.03279317945860741, "name": "AdaptiveEnhancedAPSO", "description": "Improved AdaptiveEnhancedAPSO with adaptive velocity scaling and enhanced diversity maintenance for balanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = np.random.normal(0, self.scaling_factor, self.dim)\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.cos(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n            \n            self.velocity_clamp *= (0.9 + 0.2 * success_rate) * dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 20, "feedback": "The algorithm AdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03279 with standard deviation 0.02034.", "error": "", "parent_ids": ["7ee03d1e-c570-43eb-9cc5-c6689752ff40"], "operator": null, "metadata": {"aucs": [0.04177118859934881, 0.04163640373901811, 0.04038341181911631, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05530468895820595, 0.05568002634102076, 0.04536289567075669]}}
{"id": "462c7709-4d71-475b-bc7c-ef7227f10e44", "fitness": 0.03302751063455283, "name": "DynamicAdaptiveAPSO", "description": "Dynamic Adaptive APSO integrates dynamic adjustments of cognitive and social coefficients based on success rate and diversity for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass DynamicAdaptiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = np.random.normal(0, self.scaling_factor, self.dim)\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        success_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n                    success_count += 1\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = success_count / (self.num_particles * (eval_count / self.budget))\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n            # Apply dynamic scaling to velocity clamp\n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 21, "feedback": "The algorithm DynamicAdaptiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03303 with standard deviation 0.02053.", "error": "", "parent_ids": ["7ee03d1e-c570-43eb-9cc5-c6689752ff40"], "operator": null, "metadata": {"aucs": [0.04186307657772592, 0.04175513405314146, 0.04046261686740382, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05420346175953128, 0.05748595308719573, 0.04647735336597725]}}
{"id": "d7287622-b7a3-41ed-aa9d-6fddc19be3e8", "fitness": 0.032032055229000767, "name": "EnhancedDynamicAPSO", "description": "Enhanced Dynamic APSO with adaptive velocity scaling and diversity-based exploration to improve convergence.", "code": "import numpy as np\n\nclass EnhancedDynamicAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.uniform(-0.05, 0.05, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = np.random.normal(0, self.scaling_factor, self.dim)\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n                self.scaling_factor *= 1.05  # Increase exploration when diversity is low\n\n            # Apply dynamic scaling to velocity clamp\n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedDynamicAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03203 with standard deviation 0.01951.", "error": "", "parent_ids": ["7ee03d1e-c570-43eb-9cc5-c6689752ff40"], "operator": null, "metadata": {"aucs": [0.04165520444760129, 0.04164090290339717, 0.04038302178235387, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05273332619474658, 0.050843266280165555, 0.046032775452742425]}}
{"id": "8a2293ea-78ed-42cb-8477-704601600ae2", "fitness": 0.03363695193907274, "name": "AdaptiveEnhancedAPSO", "description": "Introduced a neighborhood-based cooperative strategy and adaptive mutation to enhance exploration and exploitation synergy.", "code": "import numpy as np\n\nclass AdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05  # Added mutation rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = np.random.normal(0, self.scaling_factor, self.dim)\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        # Apply mutation with a certain probability\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            # Adaptation Mechanism\n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n\n            # Apply dynamic scaling to velocity clamp\n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 23, "feedback": "The algorithm AdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03364 with standard deviation 0.02108.", "error": "", "parent_ids": ["7ee03d1e-c570-43eb-9cc5-c6689752ff40"], "operator": null, "metadata": {"aucs": [0.042563328151403956, 0.04211371079129844, 0.040750938542606696, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05521799196162591, 0.0597564712419596, 0.04733012676276005]}}
{"id": "375f954e-221e-4bbb-82fc-30b578e3b9d7", "fitness": 0.03678869254380852, "name": "AdaptiveEnhancedAPSO", "description": "Enhanced Adaptive Particle Swarm Optimization with Levy flight for occasional long-distance exploration and adaptive mutation rate to balance exploration-exploitation.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05  # Adaptive mutation rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n            \n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 24, "feedback": "The algorithm AdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03679 with standard deviation 0.02409.", "error": "", "parent_ids": ["8a2293ea-78ed-42cb-8477-704601600ae2"], "operator": null, "metadata": {"aucs": [0.04267235901956257, 0.04451044460846487, 0.0422828673139406, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06292246147188063, 0.06982349119823728, 0.05388660928219069]}}
{"id": "3dfb7f2a-776d-4136-b2b7-b402e9d447fb", "fitness": 0.03678869254380852, "name": "AdaptiveEnhancedAPSO", "description": "Enhanced Adaptive Particle Swarm Optimization with strategic inertia weight adjustment, elitist learning, and dynamic mutation for improved convergence and robustness.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            elitist_position = self.positions[np.argmin(self.personal_best_values)]\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n                if np.random.rand() < 0.2:  # Introduce dynamic mutation\n                    self.positions += np.random.normal(0, self.scaling_factor, self.positions.shape)\n            \n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 25, "feedback": "The algorithm AdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03679 with standard deviation 0.02409.", "error": "", "parent_ids": ["375f954e-221e-4bbb-82fc-30b578e3b9d7"], "operator": null, "metadata": {"aucs": [0.04267235901956257, 0.04451044460846487, 0.0422828673139406, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06292246147188063, 0.06982349119823728, 0.05388660928219069]}}
{"id": "ea82b84a-95ca-49ec-9327-4fcf7d8a25b2", "fitness": 0.032658838583073245, "name": "QuantumInspiredAPSO", "description": "Quantum-Inspired Adaptive Particle Swarm Optimization (QI-APSO) integrates quantum behaviors for enhanced exploration, using adaptive quantum walks and dynamic parameter tuning to balance global and local search more effectively.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass QuantumInspiredAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(low=-1, high=1, size=(self.num_particles, dim))\n        self.velocities = np.random.uniform(low=-0.1, high=0.1, size=(self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def quantum_walk(self, particle_idx):\n        quantum_step = np.random.normal(0, self.scaling_factor, self.dim)\n        self.positions[particle_idx] += quantum_step\n        np.clip(self.positions[particle_idx], 0, 1, out=self.positions[particle_idx])\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n            np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < 0.1:  # 10% chance to perform a quantum walk\n            self.quantum_walk(particle_idx)\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6:\n                self.inertia_weight *= 0.9\n                self.c1 *= 1.1\n                self.c2 *= 0.9\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.1\n                self.c1 *= 0.9\n                self.c2 *= 1.1\n\n            if diversity < self.diversity_threshold:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n            \n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 26, "feedback": "The algorithm QuantumInspiredAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03266 with standard deviation 0.02048.", "error": "", "parent_ids": ["375f954e-221e-4bbb-82fc-30b578e3b9d7"], "operator": null, "metadata": {"aucs": [0.04078152314785699, 0.042319556094160715, 0.040752606560626514, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05906514956160125, 0.054530158041700694, 0.04148055384171301]}}
{"id": "804b6b1f-078c-4d8b-8726-af7ea559430d", "fitness": 0.036844545686232584, "name": "AdaptiveEnhancedAPSO", "description": "Enhanced Adaptive Particle Swarm Optimization with Levy flight incorporating adaptive scaling for diverse exploration and an improved convergence strategy.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            # Improved convergence strategy\n            if success_rate > 0.6 or diversity < self.diversity_threshold:\n                self.inertia_weight *= 0.95\n                self.c1 *= 1.05\n                self.c2 *= 0.95\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n            \n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 27, "feedback": "The algorithm AdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03684 with standard deviation 0.02417.", "error": "", "parent_ids": ["375f954e-221e-4bbb-82fc-30b578e3b9d7"], "operator": null, "metadata": {"aucs": [0.04266887083067661, 0.04451044460846487, 0.04227307882827802, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06281465550892829, 0.07044725211755476, 0.05388660928219069]}}
{"id": "f1cb235b-8dac-429e-a30c-f4a7caed34be", "fitness": 0.03625177462450968, "name": "RefinedAdaptiveEnhancedAPSO", "description": "Leveraged Adaptive Enhanced APSO utilizing dynamic topology adaptation and hybrid crossover to balance exploration and exploitation for improved convergence.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass RefinedAdaptiveEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n        self.dynamic_topology = True\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def crossover(self):\n        for i in range(self.num_particles - 1):\n            if np.random.rand() < 0.5:\n                crossover_point = np.random.randint(0, self.dim)\n                self.positions[i, :crossover_point], self.positions[i + 1, :crossover_point] = \\\n                self.positions[i + 1, :crossover_point], self.positions[i, :crossover_point]\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            if success_rate > 0.6 or diversity < self.diversity_threshold:\n                self.inertia_weight *= 0.95\n                self.c1 *= 1.05\n                self.c2 *= 0.95\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n            \n            self.velocity_clamp *= dynamic_scaling\n\n            if self.dynamic_topology:\n                self.crossover()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 28, "feedback": "The algorithm RefinedAdaptiveEnhancedAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03625 with standard deviation 0.02357.", "error": "", "parent_ids": ["804b6b1f-078c-4d8b-8726-af7ea559430d"], "operator": null, "metadata": {"aucs": [0.04257782854923975, 0.04420799509229323, 0.04221503042573116, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05836154422422257, 0.06978425644589348, 0.054119316883206925]}}
{"id": "150c9d4b-a55a-4495-9866-a8fb1fc81b77", "fitness": -Infinity, "name": "HybridEnhancedAPSO", "description": "Hybrid Enhanced Particle Swarm Optimization with Adaptive Hyperparameters and Dynamic Mutation for Improved Exploration-Exploitation Balance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass HybridEnhancedAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor * (1 - self.personal_best_values[particle_idx] / (self.best_global_value + 1e-9)), self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            dynamic_scaling = 1 + self.scaling_factor * np.sin(2 * np.pi * eval_count / self.budget)\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            success_rate = np.mean(self.personal_best_values < self.best_global_value)\n            diversity = self.calculate_diversity()\n            \n            # Improved convergence strategy with adaptive hyperparameters\n            if success_rate > 0.6 or diversity < self.diversity_threshold:\n                self.inertia_weight *= 0.95\n                self.c1 *= 1.05\n                self.c2 *= 0.95\n            elif success_rate < 0.4:\n                self.inertia_weight *= 1.05\n                self.c1 *= 0.95\n                self.c2 *= 1.05\n            \n            self.velocity_clamp *= dynamic_scaling\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 29, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["804b6b1f-078c-4d8b-8726-af7ea559430d"], "operator": null, "metadata": {}}
{"id": "81fd7446-5ed0-43d4-b333-3919a9287083", "fitness": 0.21874877250189342, "name": "AdaptiveCompetitiveAPSO", "description": "Introduced adaptive learning parameters and a competition-based local search strategy in Enhanced Adaptive Particle Swarm Optimization to boost convergence and balance exploration-exploitation trade-off.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveCompetitiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 2.0  # Increased cognitive coefficient\n        self.c2 = 1.0  # Decreased social coefficient\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            self.c1 *= 1.05\n            self.c2 *= 0.95\n        else:\n            self.c1 *= 0.95\n            self.c2 *= 1.05\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                self.positions[i] = self.personal_best_positions[competitor_idx] + levy.rvs(size=self.dim) * self.scaling_factor\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.inertia_weight *= 0.99  # Slightly decrease inertia weight over time\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 30, "feedback": "The algorithm AdaptiveCompetitiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21875 with standard deviation 0.28067.", "error": "", "parent_ids": ["804b6b1f-078c-4d8b-8726-af7ea559430d"], "operator": null, "metadata": {"aucs": [0.6375061153925912, 0.7827982935964289, 0.3253389518911983, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07236544369130682, 0.07451719395907275, 0.061212953986442664]}}
{"id": "62b51a6c-8382-4484-8c8f-9a62debfcbac", "fitness": 0.21874170671292584, "name": "AdaptiveCompetitiveAPSO", "description": "Improved adaptive strategy in APSO by slightly modifying the inertia weight adjustment to enhance convergence across different problem landscapes.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveCompetitiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 2.0  # Increased cognitive coefficient\n        self.c2 = 1.0  # Decreased social coefficient\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            self.c1 *= 1.05\n            self.c2 *= 0.95\n        else:\n            self.c1 *= 0.95\n            self.c2 *= 1.05\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                self.positions[i] = self.personal_best_positions[competitor_idx] + levy.rvs(size=self.dim) * self.scaling_factor\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.inertia_weight *= 0.995  # Slightly adjusted inertia weight decrement over time\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 31, "feedback": "The algorithm AdaptiveCompetitiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21874 with standard deviation 0.28067.", "error": "", "parent_ids": ["81fd7446-5ed0-43d4-b333-3919a9287083"], "operator": null, "metadata": {"aucs": [0.6374694711398396, 0.7827982935964289, 0.3253393332108323, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07234195247642905, 0.07451335600636, 0.061212953986442664]}}
{"id": "ec05f0b6-36f4-482f-be2b-56194c364369", "fitness": 0.1864271850950493, "name": "EnhancedAdaptiveCompetitiveAPSO", "description": "Enhanced Adaptive Competitive APSO with Dynamic Neighborhood Search, introducing a dynamic neighborhood strategy to further balance exploration and exploitation.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveCompetitiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.7\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n        self.neighborhood_radius = 3\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            self.c1 *= 1.05\n            self.c2 *= 0.95\n        else:\n            self.c1 *= 0.95\n            self.c2 *= 1.05\n\n    def dynamic_neighborhood_search(self, func):\n        for i in range(self.num_particles):\n            neighborhood = np.random.choice(self.num_particles, self.neighborhood_radius, replace=False)\n            best_neighbor = min(neighborhood, key=lambda idx: self.personal_best_values[idx])\n            if self.personal_best_values[best_neighbor] < self.personal_best_values[i]:\n                self.positions[i] = self.personal_best_positions[best_neighbor] + levy.rvs(size=self.dim) * self.scaling_factor\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            self.adapt_learning_parameters()\n            self.dynamic_neighborhood_search(func)\n            self.inertia_weight *= 0.99\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedAdaptiveCompetitiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18643 with standard deviation 0.21981.", "error": "", "parent_ids": ["81fd7446-5ed0-43d4-b333-3919a9287083"], "operator": null, "metadata": {"aucs": [0.5870248118415271, 0.3067497674888462, 0.5396020634415293, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07381002064860609, 0.06959020217360556, 0.08606780026132921]}}
{"id": "25a2de3e-dd07-4e9d-8763-59a8c9cf15b7", "fitness": 0.21869375239573519, "name": "AdaptiveCompetitiveAPSO", "description": "Introduced dynamic adjustment of inertia weight and velocity clamp in Adaptive Competitive APSO to enhance convergence speed and stability.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveCompetitiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = min(40, budget // dim)\n        self.inertia_weight = 0.9  # Modified initial inertia weight\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.3  # Modified initial velocity clamp\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            self.c1 *= 1.05\n            self.c2 *= 0.95\n        else:\n            self.c1 *= 0.95\n            self.c2 *= 1.05\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                self.positions[i] = self.personal_best_positions[competitor_idx] + levy.rvs(size=self.dim) * self.scaling_factor\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.inertia_weight *= 0.98  # Slightly decrease inertia weight more over time\n            self.velocity_clamp *= 0.99  # Gradually decrease velocity clamp for stability\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 33, "feedback": "The algorithm AdaptiveCompetitiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.21869 with standard deviation 0.28074.", "error": "", "parent_ids": ["81fd7446-5ed0-43d4-b333-3919a9287083"], "operator": null, "metadata": {"aucs": [0.6374985129220528, 0.7827982935964289, 0.32587324338504275, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07321754828043925, 0.06993338356636203, 0.06392278981129085]}}
{"id": "ad4b0399-f97b-4efc-a086-386d7eb8845d", "fitness": 0.24062080992303056, "name": "DynamicAdaptiveCompetitiveAPSO", "description": "Introduced dynamic population resizing and enhanced diversity mechanisms in Adaptive Competitive Particle Swarm Optimization to improve convergence speed and robustness on varied landscapes.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass DynamicAdaptiveCompetitiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.7\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.mutation_rate = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n    \n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            self.c1 *= 1.05\n            self.c2 *= 0.95\n        else:\n            self.c1 *= 0.95\n            self.c2 *= 1.05\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                self.positions[i] = self.personal_best_positions[competitor_idx] + levy.rvs(size=self.dim) * self.scaling_factor\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                self.positions = np.vstack((self.positions, np.random.rand(1, self.dim)))\n                self.velocities = np.vstack((self.velocities, np.random.rand(1, self.dim) * 0.1))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, np.random.rand(1, self.dim)))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                \n                if eval_count >= self.budget:\n                    break\n                \n                self.update_position(i, func)\n            \n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n            self.inertia_weight *= 0.99\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 34, "feedback": "The algorithm DynamicAdaptiveCompetitiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24062 with standard deviation 0.29614.", "error": "", "parent_ids": ["81fd7446-5ed0-43d4-b333-3919a9287083"], "operator": null, "metadata": {"aucs": [0.6374694711398396, 0.7827982935964289, 0.5265727883922856, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06461695093429609, 0.07445783722047494, 0.06467194802394971]}}
{"id": "b5906321-7d85-4d32-9f8d-ed592389a7d4", "fitness": 0.24688006045452582, "name": "EnhancedDynamicAdaptiveCompetitiveAPSO", "description": "Enhanced DynamicAdaptiveCompetitiveAPSO with an adaptive mutation rate and improved diversity measures to achieve faster convergence and better exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedDynamicAdaptiveCompetitiveAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.7\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.rand(self.num_particles, dim)\n        self.velocities = np.random.rand(self.num_particles, dim) * 0.1\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 *= 1.05\n            self.c2 *= 0.95\n        else:\n            self.c1 *= 0.95\n            self.c2 *= 1.05\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                self.positions[i] = self.personal_best_positions[competitor_idx] + levy.rvs(size=self.dim) * self.scaling_factor\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                self.positions = np.vstack((self.positions, np.random.rand(1, self.dim)))\n                self.velocities = np.vstack((self.velocities, np.random.rand(1, self.dim) * 0.1))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, np.random.rand(1, self.dim)))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n            self.inertia_weight *= 0.99\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedDynamicAdaptiveCompetitiveAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24688 with standard deviation 0.30308.", "error": "", "parent_ids": ["ad4b0399-f97b-4efc-a086-386d7eb8845d"], "operator": null, "metadata": {"aucs": [0.6374694711398396, 0.7827982935964289, 0.5852461873410219, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06835678611839979, 0.07090212652188843, 0.06214767937315391]}}
{"id": "05154fd6-1437-49f6-acfe-0ccdd4580eaa", "fitness": 0.24768107470337192, "name": "HyperAdaptiveDynamicCompetitivePSO", "description": "DynamicAdaptiveCompetitivePSO with hyperadaptive inertia weight, learning rate, and mutation rate to improve convergence speed and solution accuracy by enhancing exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass HyperAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 36, "feedback": "The algorithm HyperAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24768 with standard deviation 0.30261.", "error": "", "parent_ids": ["b5906321-7d85-4d32-9f8d-ed592389a7d4"], "operator": null, "metadata": {"aucs": [0.6374694711398396, 0.7827982935964289, 0.5852461873410219, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06862065559955499, 0.07470115426830004, 0.06529391038520205]}}
{"id": "3bb3a080-f9ae-4378-9494-0b77e1f5d333", "fitness": 0.25244975922655427, "name": "HyperAdaptiveDynamicCompetitivePSO", "description": "Enhanced exploration by adjusting the scaling factor for Levy flight based on diversity to improve performance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass HyperAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)  # Adjust scaling factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 37, "feedback": "The algorithm HyperAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25245 with standard deviation 0.30741.", "error": "", "parent_ids": ["05154fd6-1437-49f6-acfe-0ccdd4580eaa"], "operator": null, "metadata": {"aucs": [0.6453095943767637, 0.7827982935964289, 0.6161766615108892, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06932364546281489, 0.06821952530411535, 0.07522011278797658]}}
{"id": "97581786-7e06-413c-85f2-a344260dc387", "fitness": 0.25244975922655427, "name": "EnhancedAdaptiveDynamicCompetitivePSO", "description": "Enhanced Particle Swarm Optimization with adaptive diversity-driven scaling factors and dynamic population control to improve convergence speed and accuracy.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n                if eval_count >= self.budget:\n                    break\n                self.update_position(i, func)\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25245 with standard deviation 0.30741.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.6453095943767637, 0.7827982935964289, 0.6161766615108892, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06932364546281489, 0.06821952530411535, 0.07522011278797658]}}
{"id": "530c9c34-9189-4cff-9123-dc49bb89ce98", "fitness": 0.2489053866166346, "name": "HyperAdaptiveDynamicCompetitivePSO", "description": "Enhanced exploration by adjusting mutation strategy to use Cauchy distribution for improved diversity control.", "code": "import numpy as np\nfrom scipy.stats import levy, cauchy\n\nclass HyperAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = cauchy.rvs(size=self.dim) * self.scaling_factor\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)  # Adjust scaling factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 39, "feedback": "The algorithm HyperAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24891 with standard deviation 0.30781.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.6458095289339172, 0.7827982935964289, 0.6069453869760096, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06697104072846016, 0.06234507024898295, 0.06027915906591286]}}
{"id": "07c4140c-44dc-41bf-8b22-dbb24131f294", "fitness": 0.25244975922655427, "name": "HyperAdaptiveDynamicCompetitivePSO", "description": "Improved global exploration by increasing the mutation rate when diversity is low.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass HyperAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold / 2:  # Increased mutation rate when diversity is low\n            self.mutation_rate *= 1.5\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)  # Adjust scaling factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 40, "feedback": "The algorithm HyperAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25245 with standard deviation 0.30741.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.6453095943767637, 0.7827982935964289, 0.6161766615108892, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06932364546281489, 0.06821952530411535, 0.07522011278797658]}}
{"id": "603b75c8-4f41-48d9-aec9-cf0fd51a8d1a", "fitness": 0.2520509283607788, "name": "HyperAdaptiveDynamicCompetitivePSO", "description": "Refined local search by modifying the competition step to increase exploration.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass HyperAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)  # Adjust scaling factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor * 1.2  # Slight increase in exploration\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 41, "feedback": "The algorithm HyperAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25205 with standard deviation 0.30815.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.6474514544333778, 0.7827982935964289, 0.61650386776686, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.0669747316699536, 0.07277291510807349, 0.06695709267231531]}}
{"id": "866630d9-8d55-4ce2-89d1-9a21483fb7c5", "fitness": 0.25244887953659273, "name": "HyperAdaptiveDynamicCompetitivePSO", "description": "Introduced a slight increase in cognitive component factor for adaptive learning.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass HyperAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.05  # Changed from 2.0 to 2.05\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)  # Adjust scaling factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 42, "feedback": "The algorithm HyperAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25245 with standard deviation 0.30741.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.6453095943767637, 0.7827982935964289, 0.6161766615108892, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06931582821267201, 0.06821924953057268, 0.0752202886020078]}}
{"id": "fa3658af-5785-4e23-8779-3d4b48472a4e", "fitness": 0.25181539108246315, "name": "HyperAdaptiveDynamicCompetitivePSO", "description": "Slightly increase the velocity clamp to improve exploration capabilities.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass HyperAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.2  # Adjusted from 0.15 to 0.2\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)  # Adjust scaling factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 43, "feedback": "The algorithm HyperAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25182 with standard deviation 0.30779.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.6453498164161136, 0.7827982935964289, 0.6161721437814988, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06962806456841042, 0.07372750797777627, 0.06366269340194008]}}
{"id": "acdaeba3-ab00-422a-a19e-abdc21c4238b", "fitness": 0.25239459269272424, "name": "HyperAdaptiveDynamicCompetitivePSO", "description": "Introduced a decay factor to the mutation rate for enhanced exploration and convergence balance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass HyperAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.mutation_decay = 0.99  # Added decay factor\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)  # Adjust scaling factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n            self.mutation_rate *= self.mutation_decay  # Apply decay to mutation rate\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 44, "feedback": "The algorithm HyperAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25239 with standard deviation 0.30744.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.6453095943767637, 0.7827982935964289, 0.6161766615108892, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06932364546281489, 0.06821952530411535, 0.07472361398350591]}}
{"id": "09f3492a-f329-43c9-82c7-50606e140f76", "fitness": 0.03718481960249996, "name": "ChaosAdaptivePSO", "description": "Introduce a chaos-based learning mechanism with adaptive variance reduction to enhance convergence stability and exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass ChaosAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.lambda_chaos = 3.8  # Logistic map parameter for chaos\n        self.chaotic_seq = np.random.rand(self.num_particles)\n        self.variance_reduction_factor = 0.99\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        chaotic_mutation = self.chaotic_seq[particle_idx] * self.scaling_factor\n        self.positions[particle_idx] += chaotic_mutation\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.scaling_factor *= self.variance_reduction_factor\n        self.chaotic_seq = self.lambda_chaos * self.chaotic_seq * (1 - self.chaotic_seq)  # Update chaotic sequence\n        self.inertia_weight = max(0.4, self.inertia_weight * 0.98 if diversity < self.diversity_threshold else self.inertia_weight * 1.02)\n        self.c1, self.c2 = (min(3.0, self.c1 * 1.1), max(0.5, self.c2 * 0.9)) if diversity < self.diversity_threshold else (max(0.5, self.c1 * 0.9), min(3.0, self.c2 * 1.1))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 45, "feedback": "The algorithm ChaosAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03718 with standard deviation 0.02421.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.04367044483202409, 0.043858745044193626, 0.04290406274793268, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06612905507122624, 0.0648356376657161, 0.058265431061406914]}}
{"id": "167b3183-e188-4513-bc23-e782322b1822", "fitness": 0.16283103938044047, "name": "HyperAdaptiveDynamicCompetitivePSO", "description": "Improved local search by including a perturbation step to the competition mechanism for better exploration in low diversity situations.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass HyperAdaptiveDynamicCompetitivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def competition_local_search(self, func):\n        for i in range(self.num_particles):\n            competitor_idx = np.random.randint(self.num_particles)\n            if self.personal_best_values[competitor_idx] < self.personal_best_values[i]:\n                offset = levy.rvs(size=self.dim) * self.scaling_factor\n                self.positions[i] = self.personal_best_positions[competitor_idx] + offset\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                perturbation = np.random.normal(0, self.scaling_factor, self.dim)  # Added perturbation step\n                self.positions[i] += perturbation\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.competition_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 46, "feedback": "The algorithm HyperAdaptiveDynamicCompetitivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16283 with standard deviation 0.17667.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.38460027679338327, 0.41949114404373766, 0.4227055790563632, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06466478463794856, 0.07318791838023087, 0.08582965151230038]}}
{"id": "6edac0e0-f711-4e49-b697-db1e7eb4701b", "fitness": 0.2711362296285762, "name": "SynergisticAdaptiveCooperativePSO", "description": "Introduce adaptive cooperative particle interactions and synergy-based velocity adjustments to enhance convergence and robustness.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass SynergisticAdaptiveCooperativePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 47, "feedback": "The algorithm SynergisticAdaptiveCooperativePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27114 with standard deviation 0.33015.", "error": "", "parent_ids": ["3bb3a080-f9ae-4378-9494-0b77e1f5d333"], "operator": null, "metadata": {"aucs": [0.6371831237568719, 0.8069195995321905, 0.7537209454549035, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07434302379870172, 0.0823681405130291, 0.0706912336014891]}}
{"id": "f0614af8-be90-4c96-8f68-a246e8c44867", "fitness": 0.25223218836265526, "name": "EnhancedSynergisticAdaptiveCooperativePSO", "description": "Integrate chaotic maps for dynamic parameter tuning and expand exploration using differential mutation to enhance convergence and robustness over a diverse range of problems.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticAdaptiveCooperativePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.chaos_parameter = np.random.rand()\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        differential_mutation = self.differential_mutation(particle_idx)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + differential_mutation\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.chaotic_tuning()\n\n    def chaotic_tuning(self):\n        # Logistic map for chaotic sequences\n        self.chaos_parameter = 4 * self.chaos_parameter * (1 - self.chaos_parameter)\n        self.inertia_weight = max(0.4, min(0.9, self.inertia_weight * (0.9 + 0.1 * self.chaos_parameter)))\n        self.c1 = max(0.5, min(3.0, self.c1 * (0.9 + 0.1 * self.chaos_parameter)))\n        self.c2 = max(0.5, min(3.0, self.c2 * (0.9 + 0.1 * self.chaos_parameter)))\n\n    def differential_mutation(self, idx):\n        a, b, c = np.random.choice([i for i in range(self.num_particles) if i != idx], 3, replace=False)\n        F = 0.8\n        return F * (self.positions[a] - self.positions[b] + self.positions[c] - self.positions[idx])\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 48, "feedback": "The algorithm EnhancedSynergisticAdaptiveCooperativePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25223 with standard deviation 0.30891.", "error": "", "parent_ids": ["6edac0e0-f711-4e49-b697-db1e7eb4701b"], "operator": null, "metadata": {"aucs": [0.7567177442077485, 0.6780504243775063, 0.6209365447830774, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.05763108462044353, 0.07587510465799252, 0.06587879261712959]}}
{"id": "9724d839-ee4d-44f3-85fe-c381241ccb4d", "fitness": 0.24090174796065555, "name": "SynergisticAdaptiveCooperativePSO", "description": "Enhance exploration via adaptive neighborhood search and integrate chaos theory for velocity updates to intensify solution diversity. ", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass SynergisticAdaptiveCooperativePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.scaling_factor = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.chaotic_factor = 0.7 \n\n    def chaotic_map(self, x):\n        return 4 * x * (1 - x)\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        chaotic_component = self.chaotic_map(self.chaotic_factor) * self.scaling_factor\n        exploration_component = levy.rvs(size=self.dim) * self.scaling_factor\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + chaotic_component + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def neighborhood_search(self, func):\n        for i in range(self.num_particles):\n            neighborhood_size = int(self.num_particles * 0.1)\n            neighbors = np.random.choice(self.num_particles, neighborhood_size, replace=False)\n            for j in neighbors:\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.neighborhood_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 49, "feedback": "The algorithm SynergisticAdaptiveCooperativePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24090 with standard deviation 0.28991.", "error": "", "parent_ids": ["6edac0e0-f711-4e49-b697-db1e7eb4701b"], "operator": null, "metadata": {"aucs": [0.6407384401675249, 0.6589008353561723, 0.6478933657089763, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07572103731619761, 0.06940678444937387, 0.06045526864765538]}}
{"id": "84a2309d-1b85-4992-b6b2-9b901f45620a", "fitness": 0.27194688994300775, "name": "EnhancedSynergisticPSO", "description": "Refine SynergisticAdaptiveCooperativePSO by integrating adaptive velocity scaling and hierarchical swarm intelligence to boost convergence and exploration.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 50, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27195 with standard deviation 0.33036.", "error": "", "parent_ids": ["6edac0e0-f711-4e49-b697-db1e7eb4701b"], "operator": null, "metadata": {"aucs": [0.6408469610868448, 0.8069195995321905, 0.7537196227883319, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07702040537059329, 0.08279983180934891, 0.07121558889976032]}}
{"id": "ad3eaeab-3108-4fa9-b0f9-d9d41a46fca0", "fitness": 0.27022482389732283, "name": "EnhancedSynergisticPSO", "description": "Integrate dynamic subpopulation adaptation and strategic hybrid search to balance exploration and exploitation and enhance convergence on diverse landscapes.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.subpop_size = 5\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def strategic_hybrid_search(self, func):\n        num_subpopulations = self.num_particles // self.subpop_size\n        for sp in range(num_subpopulations):\n            subpop_start = sp * self.subpop_size\n            subpop_end = subpop_start + self.subpop_size\n            subpop_positions = self.positions[subpop_start:subpop_end]\n            subpop_best_idx = np.argmin(self.personal_best_values[subpop_start:subpop_end])\n            for i in range(subpop_start, subpop_end):\n                if i != (subpop_start + subpop_best_idx):\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[subpop_start + subpop_best_idx] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n            self.strategic_hybrid_search(func)\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27022 with standard deviation 0.33073.", "error": "", "parent_ids": ["84a2309d-1b85-4992-b6b2-9b901f45620a"], "operator": null, "metadata": {"aucs": [0.6354228779895008, 0.8069195995321905, 0.7552087998578892, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06540753272331656, 0.08724282719195797, 0.06682177778105003]}}
{"id": "25beef94-c3ff-4d91-a429-0fbea9e7b3fd", "fitness": -Infinity, "name": "EnhancedDynamicSynergisticPSO", "description": "Introduce a self-adaptive dynamic synergy mechanism and an elite-sharing strategy to further enhance convergence and diversity.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedDynamicSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.elite_positions = []\n        self.elite_sharing_factor = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def elite_sharing(self):\n        if len(self.elite_positions) > 0:\n            for i in range(self.num_particles):\n                random_elite = self.elite_positions[np.random.randint(len(self.elite_positions))]\n                self.positions[i] += self.elite_sharing_factor * (random_elite - self.positions[i])\n                np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n            if len(self.elite_positions) < self.initial_particles:\n                self.elite_positions.append(self.best_global_position.copy())\n            self.elite_sharing()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 52, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["84a2309d-1b85-4992-b6b2-9b901f45620a"], "operator": null, "metadata": {}}
{"id": "15244d16-deaa-4dab-9831-985fed608b54", "fitness": 0.2700946026146889, "name": "EnhancedSynergisticPSO", "description": "Enhance exploration by adjusting the Levy flight component for better diversity management.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim, scale=0.1) * self.velocity_scaling_factor  # Adjusted line\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27009 with standard deviation 0.33123.", "error": "", "parent_ids": ["84a2309d-1b85-4992-b6b2-9b901f45620a"], "operator": null, "metadata": {"aucs": [0.6396792762693939, 0.8069195995321905, 0.75370909152171, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06766601127245464, 0.07602012828152571, 0.07185731665492556]}}
{"id": "fcc49515-f747-4837-a6fb-c8c7a1678621", "fitness": 0.28248425363267227, "name": "EnhancedSynergisticPSO", "description": "Integrate quantum-inspired mechanisms into EnhancedSynergisticPSO for enhanced exploration and exploitation balance.  ", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=0.1, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 54, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28248 with standard deviation 0.34877.", "error": "", "parent_ids": ["84a2309d-1b85-4992-b6b2-9b901f45620a"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06157944515352409, 0.07100550186060217]}}
{"id": "0d2cb28d-3533-4787-bca9-977337723bf2", "fitness": 0.28220409963142473, "name": "AdaptiveQuantumPSO", "description": "Integrate adaptive quantum-enhanced swarm intelligence with a dynamic topology and adaptive learning for improved convergence.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveQuantumPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.topology_update_step = 5\n        self.topology_neighbors = 5\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=0.1, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def update_topology(self):\n        dist_matrix = np.linalg.norm(self.positions[:, np.newaxis] - self.positions, axis=2)\n        neighbors = np.argsort(dist_matrix, axis=1)[:, 1:self.topology_neighbors + 1]\n        for i in range(self.num_particles):\n            best_neighbor_idx = neighbors[i, np.argmin(self.personal_best_values[neighbors[i]])]\n            if self.personal_best_values[best_neighbor_idx] < self.personal_best_values[i]:\n                self.personal_best_positions[i] = self.personal_best_positions[best_neighbor_idx]\n                self.personal_best_values[i] = self.personal_best_values[best_neighbor_idx]\n\n    def __call__(self, func):\n        eval_count = 0\n        step = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n            if step % self.topology_update_step == 0:\n                self.update_topology()\n\n            step += 1\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 55, "feedback": "The algorithm AdaptiveQuantumPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28220 with standard deviation 0.34894.", "error": "", "parent_ids": ["fcc49515-f747-4837-a6fb-c8c7a1678621"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06351688274817735, 0.0665466782547206]}}
{"id": "94751253-b2a3-49b9-9c75-a4fd503b5cfd", "fitness": 0.2832849543297622, "name": "EnhancedSynergisticPSO", "description": "Utilize adaptive mechanisms to dynamically adjust quantum-inspired parameters for improved convergence in EnhancedSynergisticPSO.  ", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 56, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28328 with standard deviation 0.34827.", "error": "", "parent_ids": ["fcc49515-f747-4837-a6fb-c8c7a1678621"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.0680029797711128, 0.0717882735168226]}}
{"id": "0525531d-18ab-47ed-84a1-d956f5ed179b", "fitness": 0.2818425416163586, "name": "QuantumLifecyclePSO", "description": "Introduce adaptive particle lifecycles and a memory-based archive to enhance exploration-exploitation balance in Quantum-Lifecycle PSO.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass QuantumLifecyclePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.archive_positions = []\n        self.archive_values = []\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.lifecycle_threshold = 0.2\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def archive_memory(self, position, value):\n        if len(self.archive_values) < self.num_particles:\n            self.archive_positions.append(position)\n            self.archive_values.append(value)\n        else:\n            worst_idx = np.argmax(self.archive_values)\n            if value < self.archive_values[worst_idx]:\n                self.archive_positions[worst_idx] = position\n                self.archive_values[worst_idx] = value\n\n    def lifecycle_management(self):\n        for i in range(self.num_particles):\n            if np.random.rand() < self.lifecycle_threshold:\n                archived_idx = np.random.randint(0, len(self.archive_positions))\n                self.positions[i] = self.archive_positions[archived_idx]\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                self.archive_memory(self.positions[i], current_value)\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n            self.lifecycle_management()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 57, "feedback": "The algorithm QuantumLifecyclePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28184 with standard deviation 0.34925.", "error": "", "parent_ids": ["94751253-b2a3-49b9-9c75-a4fd503b5cfd"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7835019282874176, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.058123569763900096, 0.06835170898282927]}}
{"id": "481a8fcb-0a81-43e2-958d-68465b268005", "fitness": 0.17114309634745087, "name": "MultiSwarmEnhancedSynergisticPSO", "description": "Introduce a multi-swarm approach in EnhancedSynergisticPSO with adaptive interaction among swarms to enhance exploration and exploitation balance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass MultiSwarmEnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_swarms = min(5, budget // (dim * 10))\n        self.num_swarms = self.initial_swarms\n        self.particles_per_swarm = min(40, budget // (dim * self.num_swarms))\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.global_best_positions = [np.zeros(dim) for _ in range(self.num_swarms)]\n        self.global_best_values = [float('inf') for _ in range(self.num_swarms)]\n        self.positions = [np.random.uniform(0, 1, (self.particles_per_swarm, dim)) for _ in range(self.num_swarms)]\n        self.velocities = [np.random.uniform(-0.1, 0.1, (self.particles_per_swarm, dim)) for _ in range(self.num_swarms)]\n        self.personal_best_positions = [np.copy(pos) for pos in self.positions]\n        self.personal_best_values = [np.full(self.particles_per_swarm, float('inf')) for _ in range(self.num_swarms)]\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, swarm_idx, particle_idx):\n        inertia = self.inertia_weight * self.velocities[swarm_idx][particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[swarm_idx][particle_idx] - self.positions[swarm_idx][particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.global_best_positions[swarm_idx] - self.positions[swarm_idx][particle_idx])\n        synergy = np.mean(self.velocities[swarm_idx], axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, swarm_idx, particle_idx, func):\n        self.velocities[swarm_idx][particle_idx] = self.update_velocity(swarm_idx, particle_idx)\n        self.positions[swarm_idx][particle_idx] += self.velocities[swarm_idx][particle_idx]\n        np.clip(self.positions[swarm_idx][particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[swarm_idx][particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[swarm_idx][particle_idx] += mutation_vector\n\n    def calculate_diversity(self, swarm_idx):\n        mean_position = np.mean(self.positions[swarm_idx], axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions[swarm_idx] - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self, swarm_idx):\n        diversity = self.calculate_diversity(swarm_idx)\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, swarm_idx, func):\n        for i in range(self.particles_per_swarm):\n            for j in range(i + 1, self.particles_per_swarm):\n                if self.personal_best_values[swarm_idx][j] < self.personal_best_values[swarm_idx][i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[swarm_idx][i] = self.personal_best_positions[swarm_idx][j] + offset\n                    np.clip(self.positions[swarm_idx][i], func.bounds.lb, func.bounds.ub, out=self.positions[swarm_idx][i])\n                elif self.personal_best_values[swarm_idx][i] < self.personal_best_values[swarm_idx][j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[swarm_idx][j] = self.personal_best_positions[swarm_idx][i] + offset\n                    np.clip(self.positions[swarm_idx][j], func.bounds.lb, func.bounds.ub, out=self.positions[swarm_idx][j])\n\n    def adaptive_interaction_between_swarms(self, func):\n        for swarm_idx in range(self.num_swarms):\n            for other_swarm_idx in range(self.num_swarms):\n                if swarm_idx != other_swarm_idx:\n                    if self.global_best_values[other_swarm_idx] < self.global_best_values[swarm_idx]:\n                        self.global_best_positions[swarm_idx] = self.global_best_positions[other_swarm_idx]\n                        self.global_best_values[swarm_idx] = self.global_best_values[other_swarm_idx]\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for swarm_idx in range(self.num_swarms):\n                for i in range(self.particles_per_swarm):\n                    current_value = func(self.positions[swarm_idx][i])\n                    eval_count += 1\n\n                    if current_value < self.personal_best_values[swarm_idx][i]:\n                        self.personal_best_values[swarm_idx][i] = current_value\n                        self.personal_best_positions[swarm_idx][i] = self.positions[swarm_idx][i]\n\n                    if current_value < self.global_best_values[swarm_idx]:\n                        self.global_best_values[swarm_idx] = current_value\n                        self.global_best_positions[swarm_idx] = self.positions[swarm_idx][i]\n\n                    if eval_count >= self.budget:\n                        break\n\n                    self.update_position(swarm_idx, i, func)\n\n                self.adapt_learning_parameters(swarm_idx)\n                self.cooperative_local_search(swarm_idx, func)\n\n            self.adaptive_interaction_between_swarms(func)\n\n        best_overall_value = min(self.global_best_values)\n        best_overall_position = self.global_best_positions[self.global_best_values.index(best_overall_value)]\n        return best_overall_position, best_overall_value", "configspace": "", "generation": 58, "feedback": "The algorithm MultiSwarmEnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17114 with standard deviation 0.19673.", "error": "", "parent_ids": ["94751253-b2a3-49b9-9c75-a4fd503b5cfd"], "operator": null, "metadata": {"aucs": [0.4693497669138438, 0.4473981910566036, 0.42422222674051235, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06925420613070965, 0.0605262034181816, 0.054537272867206776]}}
{"id": "b2ecdb6c-93ef-4cd3-8d5e-5b4f2690fdce", "fitness": 0.06467788222894263, "name": "EnhancedSynergisticPSO", "description": "Integrate chaotic maps and differential evolution-inspired operators to enhance exploration-exploitation balance in EnhancedSynergisticPSO.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.chaotic_map = np.random.rand(self.dim)\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        chaotic_factor = 0.5 * (1 - np.cos(2 * np.pi * self.chaotic_map))\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost + chaotic_factor\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        self.chaotic_map = (self.chaotic_map + np.random.rand(self.dim)) % 1.0\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def differential_evolution_operator(self):\n        for i in range(self.num_particles):\n            indices = np.random.choice(self.num_particles, 3, replace=False)\n            while i in indices:\n                indices = np.random.choice(self.num_particles, 3, replace=False)\n            a, b, c = indices\n            mutant_vector = self.positions[a] + 0.8 * (self.positions[b] - self.positions[c])\n            np.clip(mutant_vector, 0, 1, out=mutant_vector)\n            trial_vector = np.copy(self.positions[i])\n            crossover = np.random.rand(self.dim) < 0.9\n            trial_vector[crossover] = mutant_vector[crossover]\n            self.positions[i] = trial_vector\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n            self.differential_evolution_operator()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06468 with standard deviation 0.05243.", "error": "", "parent_ids": ["94751253-b2a3-49b9-9c75-a4fd503b5cfd"], "operator": null, "metadata": {"aucs": [0.16005595434076414, 0.11295092555948283, 0.11450254332109266, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.0670682281465913, 0.056165583938842634, 0.05635770475371016]}}
{"id": "2e715c5a-dc28-4e64-bed6-07027503dcd8", "fitness": 0.28281857019445233, "name": "EnhancedSynergisticPSO", "description": "Introduced neighborhood-based adaptive mutation to enhance exploration in EnhancedSynergisticPSO.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            # Change 1: Introduced neighborhood-based adaptive mutation\n            neighborhood_best = np.mean(self.positions[max(0, particle_idx-2):min(self.num_particles, particle_idx+3)], axis=0)\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim) + 0.1 * (neighborhood_best - self.positions[particle_idx])\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 60, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28282 with standard deviation 0.34855.", "error": "", "parent_ids": ["94751253-b2a3-49b9-9c75-a4fd503b5cfd"], "operator": null, "metadata": {"aucs": [0.7769888590777299, 0.7831655918309347, 0.7627526388942496, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07190840932471343, 0.06755584264072911, 0.0679957899817144]}}
{"id": "81334c7e-ba54-4169-9c23-5571b6124270", "fitness": 0.2761281437964386, "name": "RefinedEnhancedSynergisticPSO", "description": "Introduce multilevel learning adaptation with hybrid velocity strategies for superior convergence in EnhancedSynergisticPSO.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass RefinedEnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.learning_strategy_switch = 10\n\n    def update_velocity(self, particle_idx, iteration):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n\n        if iteration % self.learning_strategy_switch > self.learning_strategy_switch // 2:\n            quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        else:\n            quantum_boost = np.zeros(self.dim)\n\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx, particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        iteration = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n            iteration += 1\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 61, "feedback": "The algorithm RefinedEnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.27613 with standard deviation 0.33820.", "error": "", "parent_ids": ["94751253-b2a3-49b9-9c75-a4fd503b5cfd"], "operator": null, "metadata": {"aucs": [0.7691269631209097, 0.7453294084390606, 0.744018657856858, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06928988872009523, 0.06962970243882782, 0.0727586735921959]}}
{"id": "6f7735d0-b409-4449-8af8-2611d289751d", "fitness": 0.2832849543297622, "name": "EnhancedSynergisticPSO", "description": "Introduce an adaptive lower bound for inertia weight to maintain exploration potential.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.5, self.inertia_weight * 0.98)  # Modified line\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28328 with standard deviation 0.34827.", "error": "", "parent_ids": ["94751253-b2a3-49b9-9c75-a4fd503b5cfd"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.0680029797711128, 0.0717882735168226]}}
{"id": "7d059eb6-b1f5-47d2-9834-dd5dd4831445", "fitness": -Infinity, "name": "EnhancedSynergisticPSO", "description": "Introduce a dynamic quantum scale factor for improved exploration in EnhancedSynergisticPSO.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale * (self.best_global_value / self.budget), size=self.dim)  # Changed quantum scale factor\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 63, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["94751253-b2a3-49b9-9c75-a4fd503b5cfd"], "operator": null, "metadata": {}}
{"id": "ba4409d4-e3c8-4a75-bbf7-a9d8cb323152", "fitness": 0.2858571509673777, "name": "EnhancedSynergisticPSO", "description": "Improve convergence by enhancing personal best update strategy using a weighted average.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28586 with standard deviation 0.35153.", "error": "", "parent_ids": ["94751253-b2a3-49b9-9c75-a4fd503b5cfd"], "operator": null, "metadata": {"aucs": [0.7981283953968188, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06223470914960705, 0.07956684110499235]}}
{"id": "752a6cad-368b-4bbc-8a03-a13c604e36f7", "fitness": 0.2835083206593916, "name": "EnhancedSynergisticPSO", "description": "Integrate adaptive quantum exploration and dynamic diversity control to enhance global search and local refinement.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28351 with standard deviation 0.34815.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06223470914960705, 0.07956684110499235]}}
{"id": "fbbb3252-1e72-4208-88ca-69a11201bfda", "fitness": 0.28377525670556275, "name": "EnhancedSynergisticPSO", "description": "Improve convergence by adjusting learning parameters based on performance history rather than just diversity.  ", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.previous_best_global_value = float('inf')  # New line\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if self.best_global_value < self.previous_best_global_value:  # Changed line\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n        self.previous_best_global_value = self.best_global_value  # New line\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28378 with standard deviation 0.34799.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06422233621300688, 0.07998163845713346]}}
{"id": "2cb1ebab-33c5-413a-870c-66087d2dcaa3", "fitness": 0.28274320651313695, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by refining social component weight and improving quantum boost adaptability.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.2  # Adjusted for stronger social influence\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.15  # Adjusted for enhanced adaptability\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28274 with standard deviation 0.34861.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769922702662811, 0.7831683100996643, 0.7627527832792494, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06875908282432552, 0.06239234161149387, 0.07562407053721831]}}
{"id": "41a9714e-247d-41fa-bdd5-1528622ef26a", "fitness": 0.2836343294242446, "name": "AdaptiveSynergisticPSO", "description": "Enhance convergence by introducing adaptive parameter tuning based on observed convergence trends.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self, eval_count, prev_best_global_value):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        \n        # Adaptive inertia weight based on improvement\n        if self.best_global_value < prev_best_global_value:\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.99)\n        else:\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.01)\n        \n        # Adaptive c1 and c2 based on diversity and convergence rate\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.05)\n            self.c2 = max(0.5, self.c2 * 0.95)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.95)\n            self.c2 = min(3.0, self.c2 * 1.05)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        prev_best_global_value = float('inf')\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    prev_best_global_value = self.best_global_value\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters(eval_count, prev_best_global_value)\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 68, "feedback": "The algorithm AdaptiveSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28363 with standard deviation 0.34808.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06325547412520349, 0.07968015501307302]}}
{"id": "fe72b017-7c14-4001-b7e2-5f8a63bde165", "fitness": 0.2840316959870447, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by refining the personal best update and dynamic inertia for better exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.6 * self.personal_best_positions[i] + 0.4 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n            self.inertia_weight *= 0.99  # Add inertia weight decay for improved balance\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28403 with standard deviation 0.34784.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06369146714323715, 0.08282046106024055]}}
{"id": "34cfe322-10ac-4df8-9841-27190fa5b256", "fitness": 0.2835354190085111, "name": "AdaptiveSynergisticPSO", "description": "Introduce adaptive synergy and dynamic adjustment of component weights to improve convergence by making the algorithm more responsive to the optimization landscape.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.synergy_weight = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + self.synergy_weight * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        self.synergy_weight = min(0.2, max(0.05, 0.1 * (1 - diversity / self.diversity_threshold)))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 70, "feedback": "The algorithm AdaptiveSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28354 with standard deviation 0.34814.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.062472216654157364, 0.07957321874251788]}}
{"id": "9b21060f-3f74-43cf-a0cf-1553a5b01522", "fitness": 0.03823127027423935, "name": "EnhancedSynergisticPSO", "description": "Integrate adaptive velocity scaling and dynamic cognition-social balance to enhance convergence in diverse landscapes.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def adaptive_velocity_scaling(self):\n        diversity = self.calculate_diversity()\n        scaling_factor = 1 + np.tanh((diversity - self.diversity_threshold) * 10)\n        self.velocity_clamp = 0.15 * scaling_factor\n\n    def dynamic_cognition_social_balance(self):\n        if self.best_global_value < np.mean(self.personal_best_values):\n            self.c1, self.c2 = 1.5, 2.5\n        else:\n            self.c1, self.c2 = 2.5, 1.5\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.adaptive_velocity_scaling()\n            self.dynamic_cognition_social_balance()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03823 with standard deviation 0.02528.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.043802759108441, 0.04489930786443308, 0.04285590289976715, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06937512922531108, 0.06826365739306639, 0.059884675977135426]}}
{"id": "cec5a169-78c5-4d14-8ac0-1b0acb185d81", "fitness": 0.28348814988624266, "name": "EnhancedSynergisticPSO", "description": "Refine the convergence strategy by slightly enhancing the synergy weight to strengthen the global information transfer.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.15 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28349 with standard deviation 0.34817.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769888929854305, 0.7831682099878904, 0.7627537405320306, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07182865268490002, 0.062035329822209384, 0.07961852296372318]}}
{"id": "1f07017e-f851-40f9-b679-36f46ce05d18", "fitness": 0.2834661600919155, "name": "AdaptiveSynergisticPSO", "description": "Introduce adaptive synergy and dynamic swarm synergy based on diversity for enhanced convergence.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.adaptive_synergy_weight = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        \n        # Adaptive synergy\n        synergy = np.mean(self.velocities, axis=0)\n        adaptive_synergy = self.adaptive_synergy_weight * synergy\n        \n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + adaptive_synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        self.adaptive_synergy_weight = min(0.2, 0.1 + 0.1 * (diversity / self.diversity_threshold))  # Adjust synergy weight\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 73, "feedback": "The algorithm AdaptiveSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28347 with standard deviation 0.34818.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.061868197224303034, 0.07955390792301187]}}
{"id": "1ce958d5-441c-490e-8c20-0d90302eb038", "fitness": 0.28330459575974865, "name": "EnhancedSynergisticPSO", "description": "Incorporate adaptive synergy scaling for enhanced convergence in EnhancedSynergisticPSO.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.synergy_scale = 0.1  # Add synergy scale\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + self.synergy_scale * synergy + exploration_component + quantum_boost  # Use adaptive synergy scale\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        self.synergy_scale = 0.1 + 0.1 * (diversity / self.diversity_threshold)  # Adapt synergy scale\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28330 with standard deviation 0.34828.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06051145536276836, 0.07945657079504509]}}
{"id": "da96a85f-5658-4caa-b9a6-cda5a9da7f05", "fitness": 0.25423534579794427, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by adding adaptive perturbations to avoid local optima.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        perturbation = np.random.uniform(-0.02, 0.02, self.dim)  # Adaptive perturbation addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost + perturbation\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 75, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25424 with standard deviation 0.31033.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.6572258436544816, 0.667728950998608, 0.7463140501754952, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06510774493596638, 0.06610563252086488, 0.07063588989608294]}}
{"id": "dba17d31-1474-4ab1-9a02-daf724602c05", "fitness": 0.2835444324907905, "name": "EnhancedSynergisticPSO", "description": "Introduce a nonlinear deceleration factor to enhance the weighted personal best update, dynamically balancing exploration and exploitation based on convergence speed.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.decay_factor = 0.98  # Introduced non-linear decay factor\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use nonlinear deceleration in weighted average for personal best position\n                    decay = np.power(self.decay_factor, eval_count / self.budget)\n                    self.personal_best_positions[i] = (0.7 + 0.3 * decay) * self.personal_best_positions[i] + (0.3 - 0.3 * decay) * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 76, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28354 with standard deviation 0.34812.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06624009595308433, 0.07588646078410521]}}
{"id": "c4fc0020-5ad8-4887-bd1e-31a1f6089079", "fitness": 0.2840314058932943, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by adding swarm diversity through dynamic learning rate adjustment based on current swarm performance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.6 * self.personal_best_positions[i] + 0.4 * self.positions[i]  # Adjusted weights\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28403 with standard deviation 0.34784.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06368906905918026, 0.08282024830054358]}}
{"id": "f6c4ff43-ce0a-4d50-ba3d-6eaa116b5e83", "fitness": 0.2834322171616122, "name": "EnhancedSynergisticPSO", "description": "Refine the personal best update with momentum for improved convergence stability.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.momentum_factor = 0.8  # Add momentum factor for personal best update\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = (  # Update personal best with momentum\n                        self.momentum_factor * self.personal_best_positions[i] +\n                        (1.0 - self.momentum_factor) * self.positions[i] )\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28343 with standard deviation 0.34819.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06438806634278482, 0.07672855243180021]}}
{"id": "97861d3a-66f0-4c7b-bd45-66c2aef6738a", "fitness": 0.2829004573780953, "name": "RefinedSynergisticPSO", "description": "Refine synergy and adaptive mutation strategies for enhanced convergence in high-dimensional spaces.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass RefinedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.2 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.mutation_rate * self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 79, "feedback": "The algorithm RefinedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28290 with standard deviation 0.34851.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769903455197978, 0.7831676647539517, 0.7627534966032699, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07031939322817604, 0.0631563276722078, 0.07471688862545478]}}
{"id": "eeee44ca-bab3-4c79-84df-6ddb181b2da6", "fitness": 0.28361547095391537, "name": "EnhancedSynergisticPSO", "description": "Integrate adaptive inertia weight adjustment to balance exploration and exploitation more effectively.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        self.inertia_weight = max(0.4, 0.9 - diversity)  # Adaptive inertia weight adjustment\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 80, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28362 with standard deviation 0.34809.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06322170974990593, 0.07954419315540806]}}
{"id": "92d15a5e-0e0f-4231-9a0e-5c976296792a", "fitness": 0.283943379255573, "name": "EnhancedSynergisticPSO", "description": "Improve convergence by dynamically adjusting the exploration-exploitation balance using diversity measurement to guide parameter adaptation.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = self.base_mutation_rate * min(1.0, diversity / self.diversity_threshold)\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28394 with standard deviation 0.34788.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.0747510626776483, 0.07714021138983995, 0.0656889711799481]}}
{"id": "1121da9d-ee1f-437f-b43f-c2779319086e", "fitness": 0.28518710254929686, "name": "ImprovedSynergisticPSO", "description": "Enhance convergence and exploration balance by incorporating adaptive velocity scaling and mutation strategies based on particle diversity.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass ImprovedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.dynamic_scaling_factor = np.ones(dim)\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor * self.dynamic_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        self.dynamic_scaling_factor = np.std(self.positions, axis=0)\n\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 82, "feedback": "The algorithm ImprovedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28519 with standard deviation 0.34712.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.07633248805144166, 0.08057809921230541]}}
{"id": "9a950736-6f0f-4bd7-8ee1-c23610a1d076", "fitness": 0.2836674402751011, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by introducing adaptive cognitive scaling and synergy utilizing diversity-aware quantum exploration.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim) \n        adaptive_cognitive = cognitive * (1 + np.exp(-0.5 * (self.personal_best_values[particle_idx] / self.best_global_value)))\n        new_velocity = inertia + adaptive_cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 83, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28367 with standard deviation 0.34806.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.0633673503465263, 0.07986627644945943]}}
{"id": "b5411110-550d-4c93-af5c-60736ccb66a3", "fitness": -Infinity, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by combining adaptive personal best update with chaotic learning and strategic migration.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.chaos_factor = 0.05\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  \n        chaos_component = self.chaos_factor * np.sin(np.pi * self.positions[particle_idx]) \n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost + chaos_component\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  \n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def strategic_migration(self):\n        if np.random.rand() < 0.1:\n            poorest_particle = np.argmax(self.personal_best_values)\n            best_particle = np.argmin(self.personal_best_values)\n            migration_vector = self.personal_best_positions[best_particle] - self.personal_best_positions[poorest_particle]\n            self.positions[poorest_particle] += migration_vector * np.random.rand()\n            np.clip(self.positions[poorest_particle], func.bounds.lb, func.bounds.ub, out=self.positions[poorest_particle])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.strategic_migration()\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 84, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {}}
{"id": "e6cedb2c-bd51-4f89-9bee-526841fc05c8", "fitness": 0.23221903309664224, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by dynamically adjusting parameters and employing differential evolution-inspired operations for diversity maintenance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def differential_evolution_crossover(self):\n        for i in range(self.num_particles):\n            target = self.positions[i]\n            donors = np.random.choice(self.num_particles, 3, replace=False)\n            if i in donors:\n                continue\n            donor = self.positions[donors[0]] + 0.5 * (self.positions[donors[1]] - self.positions[donors[2]])\n            np.clip(donor, 0, 1, out=donor)\n            cross_points = np.random.rand(self.dim) < 0.5\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, donor, target)\n            if np.linalg.norm(trial - self.best_global_position) < np.linalg.norm(target - self.best_global_position):\n                self.positions[i] = trial\n\n    def dynamic_population_resize(self, eval_count, func):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        elif eval_count < self.budget * 0.5 and diversity > self.diversity_threshold:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.differential_evolution_crossover()\n            self.dynamic_population_resize(eval_count, func)\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.23222 with standard deviation 0.32366.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [1.0, 0.5394884794518578, 0.35497206653570257, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.0694994411688955, 0.0567606333050954, 0.05425067740822864]}}
{"id": "5e6b7bad-fdc9-4883-82cd-036e57f87f10", "fitness": 0.283374034460399, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by adjusting the synergy component based on the diversity measure.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0) * (self.calculate_diversity() / self.diversity_threshold)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28337 with standard deviation 0.34824.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769887286243949, 0.7831703909943918, 0.7627543400116596, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.0716465589558517, 0.060542584247577524, 0.08026370730971533]}}
{"id": "dc25f3fd-aa3a-4128-8c33-8017005d2324", "fitness": 0.24784807822331137, "name": "AdaptiveSubswarmPSO", "description": "Introduce adaptive cognitive and social coefficients based on particle performance and dynamic sub-swarm strategy for enhanced exploitation and exploration balance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveSubswarmPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.velocity_clamp = 0.15\n        self.common_c1 = 1.5\n        self.common_c2 = 1.5\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.adaptive_c1(particle_idx) * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.adaptive_c2(particle_idx) * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def adaptive_c1(self, particle_idx):\n        local_improvement = (self.personal_best_values[particle_idx] - self.best_global_value) / (self.best_global_value + 1e-6)\n        return self.common_c1 * (1 + local_improvement)\n\n    def adaptive_c2(self, particle_idx):\n        global_influence = (self.best_global_value - self.personal_best_values[particle_idx]) / (self.personal_best_values[particle_idx] + 1e-6)\n        return self.common_c2 * (1 + global_influence)\n\n    def cooperative_local_search(self, func):\n        grouped_indices = np.arange(self.num_particles)\n        np.random.shuffle(grouped_indices)\n        for i in range(0, self.num_particles, 2):\n            if i + 1 < self.num_particles:\n                idx1, idx2 = grouped_indices[i], grouped_indices[i+1]\n                if self.personal_best_values[idx1] < self.personal_best_values[idx2]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[idx2] = self.personal_best_positions[idx1] + offset\n                    np.clip(self.positions[idx2], func.bounds.lb, func.bounds.ub, out=self.positions[idx2])\n                else:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[idx1] = self.personal_best_positions[idx2] + offset\n                    np.clip(self.positions[idx1], func.bounds.lb, func.bounds.ub, out=self.positions[idx1])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 87, "feedback": "The algorithm AdaptiveSubswarmPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.24785 with standard deviation 0.29834.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.6977690532058842, 0.6106965149095416, 0.6917318883541212, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.0756273078647971, 0.06871303212933311, 0.07109490754612524]}}
{"id": "23b42d6c-895e-42ec-a21e-8151f79e9cf4", "fitness": -Infinity, "name": "AdaptiveElitePSO", "description": "Introduce adaptive weight strategies and elite layer handling to improve convergence robustness through dynamic updates.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveElitePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(50, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.elite_fraction = 0.1\n        self.adaptive_weight = 0.5\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def handle_elites(self):\n        elite_count = int(self.elite_fraction * self.num_particles)\n        elite_indices = np.argsort(self.personal_best_values)[:elite_count]\n        for idx in elite_indices:\n            self.velocities[idx] *= self.adaptive_weight\n            offset = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[idx] += offset\n            np.clip(self.positions[idx], func.bounds.lb, func.bounds.ub, out=self.positions[idx])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = (1 - self.adaptive_weight) * self.personal_best_positions[i] + self.adaptive_weight * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.handle_elites()\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 88, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {}}
{"id": "dc8c6cb1-0c51-4421-b563-7036bbe6d4ab", "fitness": 0.28222383112151456, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by refining personal best update with a dynamic weighted average based on diversity.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            diversity = self.calculate_diversity()  # Added to compute dynamic weighting\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use dynamic weighted average for personal best position\n                    weight = 0.7 * (diversity / self.diversity_threshold)\n                    self.personal_best_positions[i] = weight * self.personal_best_positions[i] + (1 - weight) * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28222 with standard deviation 0.34894.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7770342035205084, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07506742993908733, 0.06246360499278314, 0.06452799621347527]}}
{"id": "c26018e5-8132-4f96-b185-962b9d1a6991", "fitness": 0.25717005471387916, "name": "EnhancedSynergisticPSO", "description": "Enhance convergence by introducing adaptive quantum jump strategy and diversified synergy updates.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = (np.mean(self.velocities, axis=0) + np.random.rand(self.dim)) / 2\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_jump = np.random.normal(scale=self.quantum_scale * max(self.best_global_value, 0.1), size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_jump\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25717 with standard deviation 0.30487.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.6417815189853041, 0.7169638110565018, 0.6964467828337412, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.1041142134807429, 0.06612487294667246, 0.07409929312195007]}}
{"id": "d6a18863-eec2-4969-b8e1-b4310846ac57", "fitness": 0.2819812486695142, "name": "AdaptiveSynergisticPSO", "description": "Enhance convergence by introducing adaptive synergy weighting and dynamic learning rate adjustment based on performance feedback.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass AdaptiveSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.synergy_weight = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = (inertia + cognitive + social +\n                        self.synergy_weight * synergy + exploration_component + quantum_boost)\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        self.synergy_weight = max(0.05, diversity / self.diversity_threshold)\n\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def adaptive_cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset * self.synergy_weight\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset * self.synergy_weight\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.adaptive_cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 91, "feedback": "The algorithm AdaptiveSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28198 with standard deviation 0.35200.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7779505722178089, 0.7939539330250931, 0.7634286223081858, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.0502821511952134, 0.05496937472536023, 0.08224658455396605]}}
{"id": "11d16811-978f-4d78-a07e-ba07e930b195", "fitness": 0.2544239619201311, "name": "ChaoticAdaptivePSO", "description": "Integrate an adaptive local search with chaos theory elements for improved exploration and convergence.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass ChaoticAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        chaos_factor = 0.5 * (1 - np.cos(np.pi * np.random.rand(self.dim)))  # Chaotic map influence\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost + chaos_factor\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 92, "feedback": "The algorithm ChaoticAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25442 with standard deviation 0.31046.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.658290896046742, 0.6678239417911862, 0.7463287561883561, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06593039441440629, 0.0699574942192247, 0.06648417462126455]}}
{"id": "cf2b0ccc-b72d-42fe-98d4-de5787dbff52", "fitness": 0.2835083206593916, "name": "RefinedSynergisticPSO", "description": "Further enhance convergence by incorporating adaptive exploration-exploitation control and dynamic parameter tuning based on particle diversity.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass RefinedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 93, "feedback": "The algorithm RefinedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28351 with standard deviation 0.34815.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06223470914960705, 0.07956684110499235]}}
{"id": "88e7e1c6-964d-4fb6-b8bc-16864ce812cb", "fitness": 0.2842396119873863, "name": "EnhancedAdaptivePSO", "description": "Integrate adaptive inertia weights and a diversity-driven mutation strategy to enhance convergence and exploration.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight_max = 0.9\n        self.inertia_weight_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.inertia_weight = self.inertia_weight_max\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        self.inertia_weight = self.inertia_weight_min + (self.inertia_weight_max - self.inertia_weight_min) * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedAdaptivePSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28424 with standard deviation 0.34769.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.07015272541998196, 0.07823044678657054]}}
{"id": "e6275d37-7b5a-4db9-988e-49c5b240459d", "fitness": 0.2845696330764631, "name": "DynamicSynergisticPSO", "description": "Enhance synergy by dynamically prioritizing velocity components based on particle performance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass DynamicSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.performance_weights = np.ones(self.num_particles)\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        \n        # Scale components by particle-specific performance weight\n        weight = self.performance_weights[particle_idx]\n        new_velocity = inertia + weight * cognitive + (1 - weight) * social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n                self.performance_weights = self.performance_weights[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n                self.performance_weights = np.append(self.performance_weights, 1.0)\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                # Update performance weights based on improvement\n                self.performance_weights[i] = 0.9 * self.performance_weights[i] + 0.1 * (self.best_global_value / current_value)\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 95, "feedback": "The algorithm DynamicSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28457 with standard deviation 0.34755.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7770063121860864, 0.7831927112785798, 0.7627636415232333, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.06255024382293894, 0.07257123031158641, 0.08804255856574317]}}
{"id": "b782d191-f3c1-45ce-9ecb-52ea5a451b37", "fitness": 0.28418633905809226, "name": "EnhancedSynergisticPSO", "description": "Improve convergence by refining adaptive learning parameters to balance exploration and exploitation.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.15 * (diversity / self.diversity_threshold))  # Adjusted quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.15)  # Adjusted cognitive component increase\n            self.c2 = max(0.5, self.c2 * 0.85)  # Adjusted social component decrease\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 96, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28419 with standard deviation 0.34773.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06780204516148569, 0.08010167068141993]}}
{"id": "44883188-0d37-49c1-aebf-877bfa62427d", "fitness": 0.2820273037633104, "name": "EnhancedSynergisticPSO", "description": "Introduce adaptive velocity scaling and multi-swarm synergy to enhance convergence and exploration adaptability.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n        self.multi_swarm_factor = 0.5\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx] * (1 + self.multi_swarm_factor)\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))\n        self.multi_swarm_factor = 0.5 + 0.3 * (diversity / self.diversity_threshold)\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 97, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28203 with standard deviation 0.34904.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769944972915942, 0.7831660508459503, 0.7627529042522329, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.0661887649610392, 0.07089939329791561, 0.06324412322106154]}}
{"id": "12009570-7d74-4678-9014-90f074c2f29b", "fitness": 0.28442524021776905, "name": "EnhancedSynergisticPSO", "description": "Improve the global best update by incorporating a momentum term for smoother convergence.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass EnhancedSynergisticPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        momentum = 0.3  # Add momentum term for smoother global best update\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = momentum * self.best_global_position + (1 - momentum) * self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 98, "feedback": "The algorithm EnhancedSynergisticPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28443 with standard deviation 0.34757.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769853672593963, 0.7831449836911218, 0.7627552013960134, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07787063640023495, 0.07075186826144442, 0.07331910495171057]}}
{"id": "cdb0f792-18fe-412d-9fa4-4df458ba650f", "fitness": 0.2835083206593916, "name": "QuantumEnhancedPSO", "description": "Enhance exploration and exploitation balance by integrating adaptive inertia weight and quantum leap mechanisms with diversity-sensitive mutation rate.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass QuantumEnhancedPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_particles = min(40, budget // dim)\n        self.num_particles = self.initial_particles\n        self.inertia_weight = 0.9\n        self.c1 = 2.0\n        self.c2 = 1.0\n        self.velocity_clamp = 0.15\n        self.best_global_position = np.zeros(dim)\n        self.best_global_value = float('inf')\n        self.positions = np.random.uniform(0, 1, (self.num_particles, dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, (self.num_particles, dim))\n        self.personal_best_positions = np.copy(self.positions)\n        self.personal_best_values = np.full(self.num_particles, float('inf'))\n        self.diversity_threshold = 0.1\n        self.base_mutation_rate = 0.05\n        self.mutation_rate = self.base_mutation_rate\n        self.velocity_scaling_factor = 0.05\n        self.quantum_scale = 0.1\n\n    def update_velocity(self, particle_idx):\n        inertia = self.inertia_weight * self.velocities[particle_idx]\n        cognitive = self.c1 * np.random.rand(self.dim) * (self.personal_best_positions[particle_idx] - self.positions[particle_idx])\n        social = self.c2 * np.random.rand(self.dim) * (self.best_global_position - self.positions[particle_idx])\n        synergy = np.mean(self.velocities, axis=0)\n        exploration_component = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n        quantum_boost = np.random.normal(scale=self.quantum_scale, size=self.dim)  # Quantum-inspired addition\n        new_velocity = inertia + cognitive + social + 0.1 * synergy + exploration_component + quantum_boost\n        np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp, out=new_velocity)\n        return new_velocity\n\n    def update_position(self, particle_idx, func):\n        self.velocities[particle_idx] = self.update_velocity(particle_idx)\n        self.positions[particle_idx] += self.velocities[particle_idx]\n        np.clip(self.positions[particle_idx], func.bounds.lb, func.bounds.ub, out=self.positions[particle_idx])\n\n        if np.random.rand() < self.mutation_rate:\n            mutation_vector = np.random.normal(0, self.velocity_scaling_factor, self.dim)\n            self.positions[particle_idx] += mutation_vector\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.positions, axis=0)\n        diversity = np.mean(np.linalg.norm(self.positions - mean_position, axis=1))\n        return diversity\n\n    def adapt_learning_parameters(self):\n        diversity = self.calculate_diversity()\n        self.mutation_rate = max(self.base_mutation_rate, self.base_mutation_rate * (diversity / self.diversity_threshold))\n        self.velocity_scaling_factor = 0.1 + 0.05 * (diversity / self.diversity_threshold)\n        self.quantum_scale = max(0.05, 0.1 * (diversity / self.diversity_threshold))  # Adjust quantum scale factor\n        if diversity < self.diversity_threshold:\n            self.c1 = min(3.0, self.c1 * 1.1)\n            self.c2 = max(0.5, self.c2 * 0.9)\n            self.inertia_weight = max(0.4, self.inertia_weight * 0.98)\n        else:\n            self.c1 = max(0.5, self.c1 * 0.9)\n            self.c2 = min(3.0, self.c2 * 1.1)\n            self.inertia_weight = min(0.9, self.inertia_weight * 1.02)\n\n    def cooperative_local_search(self, func):\n        for i in range(self.num_particles):\n            for j in range(i + 1, self.num_particles):\n                if self.personal_best_values[j] < self.personal_best_values[i]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[i] = self.personal_best_positions[j] + offset\n                    np.clip(self.positions[i], func.bounds.lb, func.bounds.ub, out=self.positions[i])\n                elif self.personal_best_values[i] < self.personal_best_values[j]:\n                    offset = levy.rvs(size=self.dim) * self.velocity_scaling_factor\n                    self.positions[j] = self.personal_best_positions[i] + offset\n                    np.clip(self.positions[j], func.bounds.lb, func.bounds.ub, out=self.positions[j])\n\n    def dynamic_population_resize(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold / 2:\n            if self.num_particles > self.initial_particles // 2:\n                self.num_particles -= 1\n                self.positions = self.positions[:self.num_particles]\n                self.velocities = self.velocities[:self.num_particles]\n                self.personal_best_positions = self.personal_best_positions[:self.num_particles]\n                self.personal_best_values = self.personal_best_values[:self.num_particles]\n        else:\n            if self.num_particles < self.initial_particles * 1.5:\n                self.num_particles += 1\n                new_position = np.random.uniform(0, 1, (1, self.dim))\n                new_velocity = np.random.uniform(-0.1, 0.1, (1, self.dim))\n                self.positions = np.vstack((self.positions, new_position))\n                self.velocities = np.vstack((self.velocities, new_velocity))\n                self.personal_best_positions = np.vstack((self.personal_best_positions, new_position))\n                self.personal_best_values = np.append(self.personal_best_values, float('inf'))\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.num_particles):\n                current_value = func(self.positions[i])\n                eval_count += 1\n\n                if current_value < self.personal_best_values[i]:\n                    self.personal_best_values[i] = current_value\n                    # Update to use weighted average for personal best position\n                    self.personal_best_positions[i] = 0.7 * self.personal_best_positions[i] + 0.3 * self.positions[i]\n\n                if current_value < self.best_global_value:\n                    self.best_global_value = current_value\n                    self.best_global_position = self.positions[i]\n\n                if eval_count >= self.budget:\n                    break\n\n                self.update_position(i, func)\n\n            self.adapt_learning_parameters()\n            self.cooperative_local_search(func)\n            self.dynamic_population_resize()\n\n        return self.best_global_position, self.best_global_value", "configspace": "", "generation": 99, "feedback": "The algorithm QuantumEnhancedPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.28351 with standard deviation 0.34815.", "error": "", "parent_ids": ["ba4409d4-e3c8-4a75-bbf7-a9d8cb323152"], "operator": null, "metadata": {"aucs": [0.7769889226249435, 0.7831676681668436, 0.762753577260933, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.07186316762720424, 0.06223470914960705, 0.07956684110499235]}}
