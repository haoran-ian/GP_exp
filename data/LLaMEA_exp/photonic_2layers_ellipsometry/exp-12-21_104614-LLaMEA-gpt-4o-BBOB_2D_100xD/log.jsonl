{"id": "3d715410-8284-4699-9640-30eb05984b83", "fitness": 0.33898990194483875, "name": "HybridDifferentialEvolutionPSO", "description": "A novel hybrid metaheuristic combining differential evolution and adaptive particle swarm optimization to efficiently explore and exploit the search space for black box optimization.", "code": "import numpy as np\n\nclass HybridDifferentialEvolutionPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 1.5  # Cognitive (personal) component for PSO\n        self.c2 = 1.5  # Social (global) component for PSO\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDifferentialEvolutionPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33899 with standard deviation 0.39505.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.8221603038196379, 0.9645111763367995, 0.8895316290216724, 0.005107314466535229, 0.01601208938756582, 0.0050000000000000044, 0.13385062976554185, 0.10546979046009142, 0.10926618424570433]}}
{"id": "7011c1c5-fec8-47f2-8522-0b48c4b34159", "fitness": 0.33898990194483875, "name": "RefinedHybridDifferentialEvolutionPSO", "description": "A refined hybrid metaheuristic combining differential evolution, adaptive particle swarm optimization, and dynamic parameter tuning to improve convergence speed and diversity maintenance in black box optimization.", "code": "import numpy as np\n\nclass RefinedHybridDifferentialEvolutionPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 1.5  # Cognitive (personal) component for PSO\n        self.c2 = 1.5  # Social (global) component for PSO\n        self.param_adapt_rate = 0.1  # Rate at which parameters are adapted\n\n    def adapt_parameters(self):\n        self.f = np.clip(self.f + self.param_adapt_rate * np.random.uniform(-0.1, 0.1), 0.1, 0.9)\n        self.cr = np.clip(self.cr + self.param_adapt_rate * np.random.uniform(-0.1, 0.1), 0.1, 1.0)\n        self.w = np.clip(self.w + self.param_adapt_rate * np.random.uniform(-0.1, 0.1), 0.1, 0.9)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            # Adapt parameters periodically\n            if evaluations % (self.population_size * 10) == 0:\n                self.adapt_parameters()\n\n        return self.global_best", "configspace": "", "generation": 1, "feedback": "The algorithm RefinedHybridDifferentialEvolutionPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33899 with standard deviation 0.39505.", "error": "", "parent_ids": ["3d715410-8284-4699-9640-30eb05984b83"], "operator": null, "metadata": {"aucs": [0.8221603038196379, 0.9645111763367995, 0.8895316290216724, 0.005107314466535229, 0.01601208938756582, 0.0050000000000000044, 0.13385062976554185, 0.10546979046009142, 0.10926618424570433]}}
{"id": "880c24e5-2471-49a7-bd4e-960bbb5a592e", "fitness": 0.35602066270605187, "name": "ImprovedHybridDifferentialEvolutionPSO", "description": "Improved hybrid metaheuristic algorithm enhancing the balance between exploration and exploitation by dynamically adjusting parameters based on a diversity measure of the population.", "code": "import numpy as np\n\nclass ImprovedHybridDifferentialEvolutionPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self.w = 0.1 + 0.4 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self.f = 0.4 + 0.1 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 2, "feedback": "The algorithm ImprovedHybridDifferentialEvolutionPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35602 with standard deviation 0.38826.", "error": "", "parent_ids": ["3d715410-8284-4699-9640-30eb05984b83"], "operator": null, "metadata": {"aucs": [0.8459687700204084, 0.9646995797950072, 0.8893064973038538, 0.0050000000000000044, 0.02372364890912837, 0.07100692647324636, 0.1436903483772507, 0.13013270053966186, 0.13065749293591022]}}
{"id": "b7415353-4d9b-4413-8e55-aa4df19adc5a", "fitness": 0.34771188823161037, "name": "RefinedHybridDifferentialEvolutionPSO", "description": "Introducing adaptive scaling for exploration and exploitation dynamics by adjusting parameters in response to population stagnation, promoting diverse search and convergence.", "code": "import numpy as np\n\nclass RefinedHybridDifferentialEvolutionPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 5\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, diversity):\n        if self.stagnation_counter >= self.stagnation_threshold:\n            self.f = np.clip(self.f * 1.1, 0.4, 0.9)\n            self.cr = np.clip(self.cr * 0.9, 0.1, 1.0)\n            self.w = np.clip(self.w * 1.1, 0.1, 0.9)\n            self.stagnation_counter = 0\n        else:\n            self.f = 0.4 + 0.1 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self.w = 0.1 + 0.4 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n        previous_global_best_score = np.inf\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(diversity)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n            \n            # Check for stagnation\n            if self.global_best_score >= previous_global_best_score:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n                previous_global_best_score = self.global_best_score\n\n        return self.global_best", "configspace": "", "generation": 3, "feedback": "The algorithm RefinedHybridDifferentialEvolutionPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34771 with standard deviation 0.39464.", "error": "", "parent_ids": ["880c24e5-2471-49a7-bd4e-960bbb5a592e"], "operator": null, "metadata": {"aucs": [0.8479394506794463, 0.9648727817967488, 0.8893195460221373, 0.0050000000000000044, 0.0050000000000000044, 0.044757946585784136, 0.14302021706220658, 0.10759203996357147, 0.12190501197459858]}}
{"id": "6d187d70-c173-43ec-939e-53413a4d39ef", "fitness": -Infinity, "name": "EnhancedAdaptiveHybridDEPSO", "description": "Enhanced adaptive hybrid metaheuristic dynamically adjusting exploration and exploitation balance based on population diversity and convergence trends using an adaptive inertia weight and scaling factor.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            convergence = np.std(self.personal_best_scores) / np.mean(self.personal_best_scores)\n            self.w = 0.1 + 0.4 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self.f = 0.4 + 0.1 * (1 - (diversity * convergence) / (np.linalg.norm(ub - lb) / 2))\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 4, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_ids": ["880c24e5-2471-49a7-bd4e-960bbb5a592e"], "operator": null, "metadata": {}}
{"id": "d77a1f6a-11c8-41ec-b830-8bdc6195623b", "fitness": 0.30193560299578714, "name": "EnhancedHybridDEPSO", "description": "Enhanced hybrid metaheuristic algorithm that adaptively adjusts exploration-exploitation balance using success-based parameter adaptation and neighborhood-based local search to improve convergence.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Initial crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.success_rate = 0.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adapt_parameters(self):\n        # Adjust parameters based on success rate\n        if self.success_rate > 0.2:\n            self.f *= 1.1\n            self.cr *= 0.9\n        else:\n            self.f *= 0.9\n            self.cr *= 1.1\n\n        # Ensure parameters are within bounds\n        self.f = np.clip(self.f, 0.1, 0.9)\n        self.cr = np.clip(self.cr, 0.1, 0.9)\n\n    def _local_search(self, individual, lb, ub):\n        # Perform a simple neighborhood-based local search\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        candidate = np.clip(individual + perturbation, lb, ub)\n        return candidate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n        self.success_rate = 0.0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self.w = 0.1 + 0.4 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self._adapt_parameters()\n\n            successes = 0\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n\n                # Local search\n                if evaluations < self.budget:\n                    candidate = self._local_search(trial, lb, ub)\n                    candidate_score = func(candidate)\n                    evaluations += 1\n                    if candidate_score < score:\n                        trial, score = candidate, candidate_score\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    successes += 1\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            self.success_rate = successes / self.population_size\n\n        return self.global_best", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.30194 with standard deviation 0.34432.", "error": "", "parent_ids": ["880c24e5-2471-49a7-bd4e-960bbb5a592e"], "operator": null, "metadata": {"aucs": [0.7678311071276971, 0.8034959500631481, 0.7853766490660318, 0.013519100093113301, 0.023287936035255585, 0.0050000000000000044, 0.10701341504896933, 0.08520093843746668, 0.12669533109040176]}}
{"id": "429510a0-06fc-47c4-840b-88bc32574ec7", "fitness": 0.25345053377270066, "name": "EnhancedHybridDifferentialEvolution", "description": "Enhanced Hybrid Differential Evolution with Adaptive Learning Rate integrates adaptive learning rates for each agent to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass EnhancedHybridDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.learning_rates = np.random.uniform(0.1, 0.5, self.population_size)\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self.w = 0.1 + 0.4 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self.f = 0.4 + 0.1 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.learning_rates[i] = 0.1 + 0.4 * np.random.rand()\n                self.velocities[i] = self.learning_rates[i] * (self.w * self.velocities[i] + cognitive + social)\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedHybridDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.25345 with standard deviation 0.31869.", "error": "", "parent_ids": ["880c24e5-2471-49a7-bd4e-960bbb5a592e"], "operator": null, "metadata": {"aucs": [0.16758980360229614, 1.0, 0.6440149479888735, 0.06114642033538775, 0.018300250929114803, 0.029543746963630046, 0.14591568186253634, 0.09959762387961557, 0.1149463283928518]}}
{"id": "65a2d032-b7a3-4f85-bd83-6da92b37f49f", "fitness": 0.3391218923378925, "name": "ImprovedHybridDifferentialEvolutionPSO", "description": "Enhanced hybrid metaheuristic algorithm that adapts dynamic parameters and integrates a new mutation strategy to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass ImprovedHybridDifferentialEvolutionPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self.w = 0.1 + 0.4 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self.f = 0.4 + 0.1 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2) + 0.001 * np.random.randn(self.dim), lb, ub)  # Added Gaussian noise\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 7, "feedback": "The algorithm ImprovedHybridDifferentialEvolutionPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.33912 with standard deviation 0.38630.", "error": "", "parent_ids": ["880c24e5-2471-49a7-bd4e-960bbb5a592e"], "operator": null, "metadata": {"aucs": [0.8862053985506735, 0.9546929467102914, 0.7978842032657467, 0.02001361889143738, 0.0050000000000000044, 0.030041289025728335, 0.12061938321439736, 0.13490364633443652, 0.10273654504832164]}}
{"id": "ad217d3c-e606-47d9-af5f-59bae16ef25c", "fitness": 0.3572438092753199, "name": "ImprovedHybridDifferentialEvolutionPSO", "description": "Improved parameter adaptation by modifying the formula for the scaling factor `f` based on population diversity.", "code": "import numpy as np\n\nclass ImprovedHybridDifferentialEvolutionPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self.w = 0.1 + 0.4 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self.f = 0.6 + 0.2 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))  # Modified line\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 8, "feedback": "The algorithm ImprovedHybridDifferentialEvolutionPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.35724 with standard deviation 0.38860.", "error": "", "parent_ids": ["880c24e5-2471-49a7-bd4e-960bbb5a592e"], "operator": null, "metadata": {"aucs": [0.8481827126678628, 0.9660119599811728, 0.8900092732536702, 0.0050000000000000044, 0.015362768051256803, 0.16369507144506656, 0.11152252561311338, 0.09527817594167487, 0.12013179652406147]}}
{"id": "357ae491-a0bd-4c12-83a1-d44ee7be5a31", "fitness": 0.3467254021166855, "name": "EnhancedHybridDifferentialEvolutionPSO", "description": "Enhanced hybrid algorithm by incorporating adaptive crossover probability and dynamic inertia weight for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDifferentialEvolutionPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Initial crossover probability for differential evolution\n        self.w = 0.5  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            \n            # Adaptive parameters based on diversity\n            self.w = 0.1 + 0.4 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self.f = 0.6 + 0.2 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self.cr = 0.7 + 0.2 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))  # Adaptive crossover probability\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedHybridDifferentialEvolutionPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34673 with standard deviation 0.40768.", "error": "", "parent_ids": ["ad217d3c-e606-47d9-af5f-59bae16ef25c"], "operator": null, "metadata": {"aucs": [0.9003835103984182, 0.9660119599811728, 0.8898363503132765, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.1361388456097543, 0.08647105677371991, 0.12668689597382798]}}
{"id": "39e2b765-399a-4d70-a1a7-48ba253b9465", "fitness": 0.31916775516450413, "name": "EnhancedHybridDifferentialEvolutionPSO", "description": "Enhanced diversity control and adaptive learning rate in hybrid Differential Evolution and PSO for improved convergence in black-box optimization tasks.", "code": "import numpy as np\n\nclass EnhancedHybridDifferentialEvolutionPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.learning_rate = 0.1  # Adaptive learning rate\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self.w = 0.1 + 0.5 * np.exp(-diversity / (np.linalg.norm(ub - lb) / 2))\n            self.f = 0.6 + 0.3 * np.exp(-diversity / (np.linalg.norm(ub - lb) / 2))  # Enhanced diversity control\n            self.learning_rate = 0.1 + 0.4 * (diversity / (np.linalg.norm(ub - lb) / 2))  # Adaptive learning rate\n            \n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + self.learning_rate * (cognitive + social)\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedHybridDifferentialEvolutionPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31917 with standard deviation 0.34447.", "error": "", "parent_ids": ["ad217d3c-e606-47d9-af5f-59bae16ef25c"], "operator": null, "metadata": {"aucs": [0.7358019411704824, 0.9649373913500978, 0.6726817696787089, 0.021087636094281992, 0.03829088902280331, 0.029593967347568695, 0.1612995269107712, 0.12208600225833832, 0.12673067264748417]}}
{"id": "1d09b879-1b75-498c-9b83-6126cef5dd67", "fitness": -Infinity, "name": "EnhancedHybridDEPSO", "description": "Enhanced diversity management by dynamically adjusting the population size based on convergence rate and diversity indicators.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.5  # Inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adjust_population_size(self, evaluations, max_evaluations):\n        convergence_rate = (self.global_best_score - np.min(self.personal_best_scores)) / self.global_best_score\n        diversity = self._calculate_diversity()\n        if convergence_rate < 0.1 and diversity < 0.1:\n            self.population_size = min(int(self.population_size * 1.1), 2 * self.initial_population_size)\n        elif evaluations > 0.5 * max_evaluations and diversity > 0.2:\n            self.population_size = max(int(self.population_size * 0.9), self.initial_population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adjust_population_size(evaluations, self.budget)\n            diversity = self._calculate_diversity()\n            self.w = 0.1 + 0.4 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n            self.f = 0.6 + 0.2 * (1 - diversity / (np.linalg.norm(ub - lb) / 2))\n\n            for i in range(self.population_size):\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 11, "feedback": "An exception occurred: IndexError('index 21 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 21 is out of bounds for axis 0 with size 20')", "parent_ids": ["ad217d3c-e606-47d9-af5f-59bae16ef25c"], "operator": null, "metadata": {}}
{"id": "bdfa4aa7-20b5-42e2-82c7-ea5a50d28060", "fitness": 0.36164375402514454, "name": "EnhancedHybridDEPSO", "description": "Adaptive method combining Differential Evolution and PSO with dynamic adjustment of scaling factor and inertia weight based on convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n                        self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36164 with standard deviation 0.41920.", "error": "", "parent_ids": ["ad217d3c-e606-47d9-af5f-59bae16ef25c"], "operator": null, "metadata": {"aucs": [1.0, 0.9648021064696232, 0.8893062163616224, 0.0050000000000000044, 0.03926761287170821, 0.037555517850746845, 0.11282600103130891, 0.10393733577690423, 0.1020989958643872]}}
{"id": "569ef702-99fd-4b97-a4b2-cd154be785f9", "fitness": 0.3423370908275236, "name": "EnhancedAdaptiveDEPSOPlus", "description": "A novel hybrid algorithm, EnhancedAdaptiveDEPSO+, dynamically adjusts DE/PSO parameters and integrates a diversity-based restart mechanism to enhance exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDEPSOPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n        self.restart_threshold = 1e-3  # Diversity threshold for restart\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n\n    def _restart_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._restart_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            if diversity < self.restart_threshold:\n                self._restart_population(lb, ub)\n\n            for i in range(self.population_size):\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n                        self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedAdaptiveDEPSOPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34234 with standard deviation 0.39251.", "error": "", "parent_ids": ["bdfa4aa7-20b5-42e2-82c7-ea5a50d28060"], "operator": null, "metadata": {"aucs": [0.8262400312214109, 0.9648021064696232, 0.8893062163616224, 0.0050000000000000044, 0.03926761287170821, 0.037555517850746845, 0.11282600103130891, 0.10393733577690423, 0.1020989958643872]}}
{"id": "1b554501-4d3d-404a-9408-2ce4812d2ade", "fitness": 0.34301594339965197, "name": "EnhancedHybridDEPSO", "description": "EnhancedHybridDEPSO with improved adaptive parameter adjustment based on both diversity and recent improvement trends.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            diversity = self._calculate_diversity()  # Line shifted for reuse\n            if avg_improvement < 1e-6 and diversity < 1:  # Added diversity condition\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n                        self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.34302 with standard deviation 0.39211.", "error": "", "parent_ids": ["bdfa4aa7-20b5-42e2-82c7-ea5a50d28060"], "operator": null, "metadata": {"aucs": [0.8262400312214109, 0.9648021064696232, 0.8893062163616224, 0.0050000000000000044, 0.03926761287170821, 0.037555517850746845, 0.11753483574731693, 0.10533817421005165, 0.1020989958643872]}}
{"id": "b34f7b7c-9453-4665-898c-aa533a4cf867", "fitness": -Infinity, "name": "EnhancedHybridDEPSO", "description": "Enhanced hybrid DE-PSO with adaptive crossover probability and dynamic mutation evaluation for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.cr = min(1.0, self.cr * 1.05)  # Adapt crossover probability\n                self.w = min(0.8, self.w * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.cr = max(0.7, self.cr * 0.95)  # Adapt crossover probability\n                self.w = max(0.4, self.w * 0.95)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                if np.random.rand() < 0.5:  # Dynamic mutation evaluation\n                    score = func(trial)\n                    evaluations += 1\n                else:\n                    score = self.personal_best_scores[i]\n\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n                        self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 15, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_ids": ["bdfa4aa7-20b5-42e2-82c7-ea5a50d28060"], "operator": null, "metadata": {}}
{"id": "7259c207-03ed-426c-920f-998b276005bf", "fitness": 0.31906720467684635, "name": "EnhancedHybridDEPSO2", "description": "EnhancedHybridDEPSO2: Integrating local search with adaptive parameter tuning for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridDEPSO2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n        \n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n\n    def _local_search(self, candidate, func, lb, ub):\n        perturb = np.random.uniform(-0.1, 0.1, self.dim)\n        new_candidate = np.clip(candidate + perturb, lb, ub)\n        score = func(new_candidate)\n        return new_candidate, score\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n                        self.prev_global_best_scores.append(self.global_best_score)\n\n                # Local search enhancement\n                if np.random.rand() < 0.1:  # 10% chance to perform local search\n                    new_candidate, new_score = self._local_search(self.population[i], func, lb, ub)\n                    if new_score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = new_score\n                        self.personal_best[i] = new_candidate\n                        if new_score < self.global_best_score:\n                            self.global_best_score = new_score\n                            self.global_best = new_candidate\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n        return self.global_best", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridDEPSO2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31907 with standard deviation 0.37561.", "error": "", "parent_ids": ["bdfa4aa7-20b5-42e2-82c7-ea5a50d28060"], "operator": null, "metadata": {"aucs": [0.8404365167028823, 0.8308852873294335, 0.8692640237755327, 0.0050000000000000044, 0.0050000000000000044, 0.0050000000000000044, 0.09291290450448253, 0.10671160178481798, 0.11639450799446804]}}
{"id": "eae0e508-79b8-4f72-bfcc-011e7b1cdbb0", "fitness": 0.39926342049552543, "name": "EnhancedHybridDEPSO", "description": "EnhancedHybridDEPSO with dynamic local search using Nelder-Mead for fine-tuning best solutions to balance exploration and exploitation.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n\n    def _local_search(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxiter': 50})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                # Apply local search\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39926 with standard deviation 0.35266.", "error": "", "parent_ids": ["bdfa4aa7-20b5-42e2-82c7-ea5a50d28060"], "operator": null, "metadata": {"aucs": [0.8262400312214109, 0.9648021064696232, 0.8893062163616224, 0.18204138310464413, 0.1393488374859264, 0.2308336484836272, 0.13657356698880574, 0.10893090522992299, 0.11529408911414596]}}
{"id": "ddb43d0c-3d43-4f0f-a995-934529af6f9c", "fitness": 0.3973663281728196, "name": "EnhancedHybridDEPSO", "description": "Improved adaptive parameter adjustment strategy for better convergence.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.c1 = min(2.5, self.c1 * 1.05)  # Added change\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n\n    def _local_search(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxiter': 50})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                # Apply local search\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39737 with standard deviation 0.35411.", "error": "", "parent_ids": ["eae0e508-79b8-4f72-bfcc-011e7b1cdbb0"], "operator": null, "metadata": {"aucs": [0.8262400312214109, 0.9648021064696232, 0.8893062163616224, 0.18204138310464413, 0.1393488374859264, 0.2308336484836272, 0.11949973608445263, 0.10893090522992299, 0.11529408911414596]}}
{"id": "00b96d61-6254-4715-b949-34ea6c033881", "fitness": 0.39926342049552543, "name": "EnhancedHybridDEPSO", "description": "Introduced a diversity-based restart mechanism to enhance exploration in stagnant phases.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n\n    def _local_search(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxiter': 50})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            if diversity < 1e-3:  # Implementing diversity-based restart\n                self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n                self.personal_best_scores.fill(np.inf)\n            self._adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                # Apply local search\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39926 with standard deviation 0.35266.", "error": "", "parent_ids": ["eae0e508-79b8-4f72-bfcc-011e7b1cdbb0"], "operator": null, "metadata": {"aucs": [0.8262400312214109, 0.9648021064696232, 0.8893062163616224, 0.18204138310464413, 0.1393488374859264, 0.2308336484836272, 0.13657356698880574, 0.10893090522992299, 0.11529408911414596]}}
{"id": "06327a8a-1948-4d08-a89b-640ac857af7e", "fitness": 0.4008707333833348, "name": "EnhancedHybridDEPSO", "description": "EnhancedHybridDEPSO with dynamic local search using Nelder-Mead for fine-tuning best solutions to balance exploration and exploitation, now with adaptive crossover probability to further improve convergence.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)  # Adaptive crossover probability\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)  # Adaptive crossover probability\n\n    def _local_search(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxiter': 50})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                # Apply local search\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40087 with standard deviation 0.35149.", "error": "", "parent_ids": ["eae0e508-79b8-4f72-bfcc-011e7b1cdbb0"], "operator": null, "metadata": {"aucs": [0.8262400312214109, 0.9648021064696232, 0.8893062163616224, 0.18204138310464413, 0.1393488374859264, 0.2308336484836272, 0.15103938297908992, 0.10893090522992299, 0.11529408911414596]}}
{"id": "a3e57c43-980f-4216-977a-edd1e2bbd436", "fitness": 0.4008707333833348, "name": "EnhancedHybridDEPSO", "description": "EnhancedHybridDEPSO with dynamic local search using Nelder-Mead for fine-tuning best solutions to balance exploration and exploitation, now with adaptive crossover probability and enhanced diversity control to further improve convergence. ", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)  # Adaptive crossover probability\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)  # Adaptive crossover probability\n\n    def _local_search(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxiter': 50})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            # Change: Enhanced diversity control by adjusting population spread\n            if diversity < 0.1 * (ub - lb).mean():\n                self.population += np.random.uniform(-0.1, 0.1, self.population.shape)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                # Apply local search\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40087 with standard deviation 0.35149.", "error": "", "parent_ids": ["06327a8a-1948-4d08-a89b-640ac857af7e"], "operator": null, "metadata": {"aucs": [0.8262400312214109, 0.9648021064696232, 0.8893062163616224, 0.18204138310464413, 0.1393488374859264, 0.2308336484836272, 0.15103938297908992, 0.10893090522992299, 0.11529408911414596]}}
{"id": "c562ec24-bb92-4a42-a48d-bb9cfca2783a", "fitness": 0.4008707333833348, "name": "EnhancedHybridDEPSOPlus", "description": "EnhancedHybridDEPSO+ with dynamic adaptation of population size and self-adaptive crossover and mutation rates to improve exploration and convergence in diverse search landscapes.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridDEPSOPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim\n        self.population_size = self.base_population_size\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Inertia weight for PSO\n        self.c1 = 2.0  # Cognitive component for PSO\n        self.c2 = 2.0  # Social component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            diversity = self._calculate_diversity()\n            \n            # Adapt mutation and crossover rates based on improvement and diversity\n            if avg_improvement < 1e-6 or diversity < 0.1:  # If improvement is slow or diversity is low\n                self.f = max(0.4, self.f * 0.95)\n                self.cr = min(0.95, self.cr * 1.05)\n                self.w = min(0.8, self.w * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.cr = max(0.7, self.cr * 0.95)\n                self.w = max(0.4, self.w * 0.95)\n\n            # Dynamic population resizing\n            if evaluations > self.budget / 2:\n                self.population_size = max(int(self.base_population_size / 2), self.dim)\n            else:\n                self.population_size = self.base_population_size\n\n    def _local_search(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxiter': 50})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            for i in range(self.population_size):\n                # Differential evolution mutation and crossover\n                while True:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    if i not in idxs:\n                        break\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                # Apply local search\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedHybridDEPSOPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40087 with standard deviation 0.35149.", "error": "", "parent_ids": ["06327a8a-1948-4d08-a89b-640ac857af7e"], "operator": null, "metadata": {"aucs": [0.8262400312214109, 0.9648021064696232, 0.8893062163616224, 0.18204138310464413, 0.1393488374859264, 0.2308336484836272, 0.15103938297908992, 0.10893090522992299, 0.11529408911414596]}}
{"id": "9fcd2675-e48a-43da-9254-2ed499deebd0", "fitness": 0.40981062153391384, "name": "EnhancedHybridDEPSO", "description": "EnhancedHybridDEPSO with dynamic local search using Nelder-Mead, adaptive mutation strategy with Gaussian noise, and population clustering to improve diversity and convergence.", "code": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.cluster import KMeans\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)  # Adaptive crossover probability\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)  # Adaptive crossover probability\n\n    def _local_search(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxiter': 50})\n        return result.x, result.fun\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                # Differential evolution mutation with clustering and Gaussian noise\n                cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                if len(cluster_idx) > 3:\n                    idxs = np.random.choice(cluster_idx, 3, replace=False)\n                else:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                # Apply local search\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40981 with standard deviation 0.34147.", "error": "", "parent_ids": ["06327a8a-1948-4d08-a89b-640ac857af7e"], "operator": null, "metadata": {"aucs": [0.8216159388089492, 0.9505617054137767, 0.8736483240198705, 0.18677790084605828, 0.14665093584432842, 0.34195403539165714, 0.12593252565683344, 0.10062005345055947, 0.1405341743731917]}}
{"id": "857e8223-d2d4-44e2-a0fc-030efb307774", "fitness": 0.4103752184056713, "name": "EnhancedHybridDEPSO", "description": "Enhance local search by using Basin-Hopping to improve convergence in difficult landscapes.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)  # Adaptive crossover probability\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)  # Adaptive crossover probability\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                # Differential evolution mutation with clustering and Gaussian noise\n                cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                if len(cluster_idx) > 3:\n                    idxs = np.random.choice(cluster_idx, 3, replace=False)\n                else:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                x0, x1, x2 = self.population[idxs]\n                mutant = np.clip(x0 + self.f * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                crossover = np.random.rand(self.dim) < self.cr\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluate trial vector\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                # Update particle velocity and position using PSO\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                # Check budget\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                # Apply local search\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41038 with standard deviation 0.34102.", "error": "", "parent_ids": ["9fcd2675-e48a-43da-9254-2ed499deebd0"], "operator": null, "metadata": {"aucs": [0.8216159388089492, 0.9505617054137767, 0.8736483240198705, 0.18757288902681368, 0.14678790171769118, 0.34195403539165714, 0.13008194344853263, 0.10062005345055947, 0.1405341743731917]}}
{"id": "27cf1d34-cd74-4bf8-a16e-726578075b1e", "fitness": 0.31761907805593625, "name": "EnhancedHybridDEPSO", "description": "Incorporate elitism to ensure the best solutions are retained across generations, enhancing convergence speed and solution quality.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\n\nclass EnhancedHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5  # Initial scaling factor for differential evolution\n        self.cr = 0.9  # Crossover probability for differential evolution\n        self.w = 0.9  # Initial inertia weight for PSO\n        self.c1 = 2.0  # Cognitive (personal) component for PSO\n        self.c2 = 2.0  # Social (global) component for PSO\n        self.prev_global_best_scores = []\n        self.elitism_rate = 0.1  # Elitism rate\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:  # If improvement is slow\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)  # Adaptive crossover probability\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)  # Adaptive crossover probability\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            elite_count = max(1, int(self.elitism_rate * self.population_size))\n            elite_indices = np.argsort(self.personal_best_scores)[:elite_count]\n\n            new_population = np.zeros_like(self.population)\n            new_velocities = np.zeros_like(self.velocities)\n            \n            for i in range(self.population_size):\n                if i < elite_count:\n                    new_population[i] = self.personal_best[elite_indices[i]]\n                    new_velocities[i] = self.velocities[elite_indices[i]]\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                    if evaluations < self.budget - 50:\n                        score = func(trial)\n                        evaluations += 1\n                        if score < self.personal_best_scores[i]:\n                            self.personal_best_scores[i] = score\n                            self.personal_best[i] = trial\n                            if score < self.global_best_score:\n                                self.global_best_score = score\n                                self.global_best = trial\n                                self.prev_global_best_scores.append(self.global_best_score)\n\n                    r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                    cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                    social = self.c2 * r2 * (self.global_best - self.population[i])\n                    new_velocities[i] = self.w * self.velocities[i] + cognitive + social\n                    new_population[i] = np.clip(self.population[i] + new_velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            self.population = new_population\n            self.velocities = new_velocities\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.31762 with standard deviation 0.33292.", "error": "", "parent_ids": ["857e8223-d2d4-44e2-a0fc-030efb307774"], "operator": null, "metadata": {"aucs": [0.8128872632605411, 0.8492791572302354, 0.6811604958037893, 0.028927093376308166, 0.05410907673858378, 0.033950738464018304, 0.1642018779465133, 0.10877053433683614, 0.12528546534660134]}}
{"id": "6112108c-9868-4653-8471-c632f85b42b9", "fitness": 0.4161125669676268, "name": "EnhancedHybridDEPSOLevy", "description": "Introduce dynamic subpopulation management and Lvy flight for exploration to improve convergence and diversity handling.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41611 with standard deviation 0.39008.", "error": "", "parent_ids": ["857e8223-d2d4-44e2-a0fc-030efb307774"], "operator": null, "metadata": {"aucs": [0.9743272562515504, 0.9912579908131751, 0.9347154979065688, 0.16350173943104163, 0.1298278709207945, 0.1686032477553332, 0.10440275862751958, 0.15550697516722078, 0.12286976583543707]}}
{"id": "fb1d157e-3e4a-4240-8946-d493f46634d2", "fitness": 0.40794539632303467, "name": "EnhancedHybridDEPSOLevy", "description": "Improve stability and exploration by refining parameter dynamics and enhancing clustering logic.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.9)  # Adjusted to 0.9\n                self.w = min(0.85, self.w * 1.05)  # Adjustment to 0.85\n                self.cr = min(0.95, self.cr * 1.1)  # Adjustment to 1.1\n            else:\n                self.f = min(0.9, self.f * 1.1)  # Adjustment to 1.1\n                self.w = max(0.4, self.w * 0.9)  # Adjustment to 0.9\n                self.cr = max(0.7, self.cr * 0.9)  # Adjustment to 0.9\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3, n_init=10)  # Increased n_init to 10\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 27, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40795 with standard deviation 0.39629.", "error": "", "parent_ids": ["6112108c-9868-4653-8471-c632f85b42b9"], "operator": null, "metadata": {"aucs": [0.9743272562515504, 0.9912579908131751, 0.9347154979065688, 0.16831504677840636, 0.1298278709207945, 0.1686032477553332, 0.07707281331387184, 0.1045190773321748, 0.12286976583543707]}}
{"id": "b45ade77-d552-4b46-a2b1-d32425277a0a", "fitness": 0.4125321565499945, "name": "EnhancedHybridDEPSOLevy", "description": "Introduce dynamic inertia weight adjustment based on population diversity to improve convergence speed and solution quality.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9  # Initial inertia weight\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            # Modify the inertia weight based on population diversity\n            diversity = self._calculate_diversity()\n            self.w = 0.4 + 0.5 * (diversity / 10)  # New line 1\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                # Removed previous inertia weight adjustment\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                # Kept minimum bounds for inertia weight\n                self.cr = max(0.7, self.cr * 0.95)\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41253 with standard deviation 0.39234.", "error": "", "parent_ids": ["6112108c-9868-4653-8471-c632f85b42b9"], "operator": null, "metadata": {"aucs": [0.9743272562515504, 0.9912579908131751, 0.9347154979065688, 0.1298278709207945, 0.1298278709207945, 0.1686032477553332, 0.1364049408213429, 0.12495496772495396, 0.12286976583543707]}}
{"id": "2c55d5d3-ad45-4ce0-8e4c-24f0db2642fc", "fitness": 0.3998311962342805, "name": "EnhancedHybridDEPSOLevy", "description": "Introduce adaptive clustering and levy flight adjustment for more efficient exploration and convergence.  ", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=int(self.population_size / 10) + 1)  # Adjusted line\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = (i + evaluations) % 3 == 0  # Adjusted line\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39983 with standard deviation 0.37032.", "error": "", "parent_ids": ["6112108c-9868-4653-8471-c632f85b42b9"], "operator": null, "metadata": {"aucs": [0.8959917557905231, 0.9475479448790507, 0.9226370997573131, 0.16283659930807337, 0.17210503415759038, 0.16911117350555227, 0.09374915490122016, 0.11170661843041274, 0.12279538537878909]}}
{"id": "9f29a59e-6050-41cc-8ed2-fe04c2fbccdd", "fitness": 0.4161125669676268, "name": "EnhancedHybridDEPSOLevy", "description": "Enhance local search by increasing basinhopping iterations for better local refinement.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=20)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41611 with standard deviation 0.39008.", "error": "", "parent_ids": ["6112108c-9868-4653-8471-c632f85b42b9"], "operator": null, "metadata": {"aucs": [0.9743272562515504, 0.9912579908131751, 0.9347154979065688, 0.16350173943104163, 0.1298278709207945, 0.1686032477553332, 0.10440275862751958, 0.15550697516722078, 0.12286976583543707]}}
{"id": "93d2a82f-86fe-4b43-afc0-22326d03dc85", "fitness": 0.4037384048692685, "name": "EnhancedHybridDEPSOLevy", "description": "Introduce adaptive cognitive and social coefficients and a restart mechanism to prevent stagnation and improve convergence.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n                self.c1 = max(1.0, self.c1 * 0.9)  # Adaptive c1\n                self.c2 = min(3.0, self.c2 * 1.1)  # Adaptive c2\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        restart_threshold = self.budget // 5  # Restart mechanism threshold\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n            if evaluations >= restart_threshold:  # Restart mechanism\n                self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n                self.personal_best = np.copy(self.population)\n                self.personal_best_scores = np.full(self.population_size, np.inf)\n                restart_threshold += self.budget // 5\n\n        return self.global_best", "configspace": "", "generation": 31, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40374 with standard deviation 0.39897.", "error": "", "parent_ids": ["6112108c-9868-4653-8471-c632f85b42b9"], "operator": null, "metadata": {"aucs": [0.9743272562515504, 0.9912579908131751, 0.9347154979065688, 0.1298278709207945, 0.07939803864954853, 0.16299279554667612, 0.12007357001609908, 0.0997483211708926, 0.14130430254811144]}}
{"id": "0684412f-353e-4f4d-abb3-c06b35fac45f", "fitness": 0.41865338372727984, "name": "EnhancedHybridDEPSOLevy", "description": "Introduce sigmoid-based inertia weight adaptation and augmented cluster-based mutation strategy to enhance convergence and diversity.  ", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n        self.w = 0.4 + (0.5 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2))))  # Sigmoid-based adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)  # Augmented mutation\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41865 with standard deviation 0.36836.", "error": "", "parent_ids": ["6112108c-9868-4653-8471-c632f85b42b9"], "operator": null, "metadata": {"aucs": [0.9154747820340299, 0.9322692666328758, 0.9630137507841795, 0.2320432935609389, 0.18563863627635357, 0.14600651253797925, 0.16084466945473452, 0.14327370731721123, 0.08931583494721562]}}
{"id": "54430684-e88f-407b-be36-ffb0b69d1301", "fitness": 0.4330854017297033, "name": "EnhancedHybridDEPSOLevy", "description": "Introduce dynamic adjustment to the mutation factor to enhance exploitation capabilities.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n        self.w = 0.4 + (0.5 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2))))  # Sigmoid-based adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    self.f = 0.5 + 0.5 * np.random.rand()  # Dynamic adjustment of mutation factor\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)  # Augmented mutation\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43309 with standard deviation 0.37571.", "error": "", "parent_ids": ["0684412f-353e-4f4d-abb3-c06b35fac45f"], "operator": null, "metadata": {"aucs": [0.9569731495668329, 0.9481238536618807, 0.961305258060176, 0.14359219088557496, 0.1298278709207945, 0.17106735126303574, 0.10160329186194172, 0.3540286474843467, 0.13124700186274607]}}
{"id": "96c16f70-5831-47f1-875a-a8e765211d22", "fitness": 0.3906876018973915, "name": "MultiStageHybridDEPSOLevy", "description": "Introduce multi-stage adaptive parameters with a feedback loop to enhance convergence and stability.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass MultiStageHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n        self.stage = 1\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                if self.stage == 1:\n                    self.f *= 0.90\n                    self.cr *= 1.05\n                    self.w *= 1.05\n                else:\n                    self.f *= 0.95\n                    self.cr *= 0.95\n                    self.w *= 0.95\n            else:\n                if self.stage == 1:\n                    self.f *= 1.05\n                    self.cr *= 0.95\n                    self.w *= 0.95\n                else:\n                    self.f *= 1.10\n                    self.cr *= 1.05\n                    self.w *= 1.05\n\n        if evaluations > self.budget * 0.5 and self.stage == 1:\n            self.stage = 2\n            self.c1 = 1.5\n            self.c2 = 2.5\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"L-BFGS-B\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    self.f = 0.5 + 0.5 * np.random.rand()\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 34, "feedback": "The algorithm MultiStageHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39069 with standard deviation 0.40016.", "error": "", "parent_ids": ["54430684-e88f-407b-be36-ffb0b69d1301"], "operator": null, "metadata": {"aucs": [0.9569731495668329, 0.9481238536618807, 0.961305258060176, 0.09065102238493439, 0.1154856549791522, 0.04805976015772695, 0.13660075704825236, 0.12832823977429808, 0.1306607214432698]}}
{"id": "ca91ea8f-515e-4ec4-83e7-6a1fcc0b8435", "fitness": 0.40393890837730506, "name": "RefinedHybridDEPSOLevy", "description": "Incorporate adaptive Lvy flight strategies and time-varying clustering for enhanced exploration and dynamic exploitation.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass RefinedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n        self.w = 0.4 + (0.5 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2))))  # Sigmoid-based adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        alpha = 1.5\n        step = levy.rvs(size=self.dim) * np.power(np.abs(levy.rvs(size=self.dim)), 1/alpha)\n        return step\n\n    def _apply_clustering(self):\n        cluster_count = max(2, int(np.log(self.population_size)))\n        kmeans = KMeans(n_clusters=cluster_count)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = evaluations % 3 == 0  # Introduce adaptive Lvy flights periodically\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    self.f = 0.5 + 0.5 * np.random.rand()  # Dynamic adjustment of mutation factor\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)  # Augmented mutation\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 35, "feedback": "The algorithm RefinedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40394 with standard deviation 0.39138.", "error": "", "parent_ids": ["54430684-e88f-407b-be36-ffb0b69d1301"], "operator": null, "metadata": {"aucs": [1.0, 0.9350772704516023, 0.9282111211940999, 0.1298278709207945, 0.17616262494730706, 0.03534398883635215, 0.1339076550674172, 0.14911943609372613, 0.14780020788444603]}}
{"id": "5f8b8c83-cb01-47f1-832d-a025ff390d27", "fitness": 0.4307442895750199, "name": "EnhancedHybridDEPSOLevy", "description": "Enhance global exploration by introducing a dynamic inertia weight adjustment.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n        # Changed the inertia weight adjustment to a dynamic function based on evaluations\n        self.w = 0.4 + (0.5 * np.sin(0.005 * evaluations))  # Dynamic inertia weight adjustment using sine\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    self.f = 0.5 + 0.5 * np.random.rand()  # Dynamic adjustment of mutation factor\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)  # Augmented mutation\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 36, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43074 with standard deviation 0.37282.", "error": "", "parent_ids": ["54430684-e88f-407b-be36-ffb0b69d1301"], "operator": null, "metadata": {"aucs": [0.9569731495668329, 0.9481238536618807, 0.961305258060176, 0.24725958478086452, 0.1305186369090755, 0.1298278709207945, 0.1457971544737502, 0.21008991019411993, 0.14680318760768463]}}
{"id": "3d3e1d2c-ed42-4373-a4a5-2909999a2ea9", "fitness": 0.4211418766974588, "name": "EnhancedLvyAdaptiveSearch", "description": "Enhance global search with adaptive Lvy flight based on diversity metrics to improve exploration capabilities.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedLvyAdaptiveSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n        self.w = 0.4 + (0.5 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2))))  # Sigmoid-based adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self, alpha=1.5):\n        u = np.random.normal(0, 1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v)**(1/alpha))\n        return step\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                levy_scale = np.clip(diversity / self.dim, 0.1, 2.0)\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = levy_scale * self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    self.f = 0.5 + 0.5 * np.random.rand()  # Dynamic adjustment of mutation factor\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)  # Augmented mutation\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 37, "feedback": "The algorithm EnhancedLvyAdaptiveSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42114 with standard deviation 0.32901.", "error": "", "parent_ids": ["54430684-e88f-407b-be36-ffb0b69d1301"], "operator": null, "metadata": {"aucs": [0.88849635409812, 0.9529433127380338, 0.7707299974947663, 0.11405053112601105, 0.2406431182954436, 0.17045881783087324, 0.1618782511287512, 0.11871203210306513, 0.37236447546206475]}}
{"id": "e9f86592-866a-44b8-bd05-bdb5bd437851", "fitness": 0.4212400516493428, "name": "AdaptiveClusterHybridDEPSOLevy", "description": "Incorporate adaptive cluster-based mutation and enhanced local search to improve convergence speed and solution accuracy in hybrid DE-PSO with Lvy flights.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveClusterHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n        self.w = 0.4 + (0.5 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2))))\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"L-BFGS-B\"}, niter=15)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=max(3, self.dim // 2))\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    self.f = 0.5 + 0.5 * np.random.rand()\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                score = func(trial)\n                evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n                        self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 38, "feedback": "The algorithm AdaptiveClusterHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42124 with standard deviation 0.39247.", "error": "", "parent_ids": ["54430684-e88f-407b-be36-ffb0b69d1301"], "operator": null, "metadata": {"aucs": [0.9569731495668329, 0.9481238536618807, 0.961305258060176, 0.06997954790717598, 0.09065102238493439, 0.08934470695572772, 0.10140002024586858, 0.4427221846182188, 0.1306607214432698]}}
{"id": "e2a86b79-4ab6-4f58-9961-a553ad346436", "fitness": 0.4330854017297033, "name": "EnhancedHybridDEPSOLevyOptimized", "description": "Integrate dynamic parameter tuning with exploration-exploitation balance using adaptive Lvy flights and clustering-informed crossover.", "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevyOptimized:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        if len(self.prev_global_best_scores) > 5:\n            recent_improvements = np.diff(self.prev_global_best_scores[-5:])\n            avg_improvement = np.mean(recent_improvements)\n            if avg_improvement < 1e-6:\n                self.f = max(0.4, self.f * 0.95)\n                self.w = min(0.8, self.w * 1.05)\n                self.cr = min(0.95, self.cr * 1.05)\n            else:\n                self.f = min(0.9, self.f * 1.05)\n                self.w = max(0.4, self.w * 0.95)\n                self.cr = max(0.7, self.cr * 0.95)\n        self.w = 0.4 + (0.5 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2))))\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=10)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            diversity = self._calculate_diversity()\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    self.f = 0.5 + 0.5 * np.random.rand()\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget - 50:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 50\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedHybridDEPSOLevyOptimized got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43309 with standard deviation 0.37571.", "error": "", "parent_ids": ["54430684-e88f-407b-be36-ffb0b69d1301"], "operator": null, "metadata": {"aucs": [0.9569731495668329, 0.9481238536618807, 0.961305258060176, 0.14359219088557496, 0.1298278709207945, 0.17106735126303574, 0.10160329186194172, 0.3540286474843467, 0.13124700186274607]}}
{"id": "f27955ce-9ba6-44c8-8fdb-c6717706216d", "fitness": 0.4615426174891786, "name": "RefinedHybridDEPSOLevy", "description": "Integrate self-adaptive parameter control and strategic diversity preservation to improve exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass RefinedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        # Adaptive mutation factor and inertia weight\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0  # Introduce Lvy flight on alternate indices\n                if has_levy_flight:\n                    step = self._levy_flight()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)  # Augmented mutation\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 40, "feedback": "The algorithm RefinedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.46154 with standard deviation 0.34659.", "error": "", "parent_ids": ["54430684-e88f-407b-be36-ffb0b69d1301"], "operator": null, "metadata": {"aucs": [0.9154747820340299, 0.9322692666328758, 0.9630250646350219, 0.14101419922123004, 0.17964619297798068, 0.13875993676396547, 0.38026614135824033, 0.14933736117314578, 0.3540906126061175]}}
{"id": "ac7ec7ba-efdd-4108-83ba-6376cec4681d", "fitness": 0.39486328646879, "name": "EnhancedHybridDEPSOLevy", "description": "Enhance exploration-exploitation dynamics through dynamic cluster-based local search and adaptive Lvy flight frequency adjustment.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self):\n        return levy.rvs(size=self.dim)\n\n    def _apply_clustering(self):\n        kmeans = KMeans(n_clusters=min(self.population_size // 10, self.dim))\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._apply_clustering()\n\n            for i in range(self.population_size):\n                if evaluations < self.budget:\n                    diversity_metric = self._calculate_diversity()\n                    levy_threshold = max(5.0, 10.0 - diversity_metric)\n\n                    use_levy_flight = np.random.rand() < (1.0 / levy_threshold)\n                    if use_levy_flight:\n                        step = self._levy_flight()\n                        trial = np.clip(self.population[i] + step, lb, ub)\n                    else:\n                        cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                        if len(cluster_idx) > 3:\n                            idxs = np.random.choice(cluster_idx, 3, replace=False)\n                        else:\n                            idxs = np.random.choice(self.population_size, 3, replace=False)\n                        x0, x1, x2 = self.population[idxs]\n                        mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                        crossover = np.random.rand(self.dim) < self.cr\n                        trial = np.where(crossover, mutant, self.population[i])\n\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 41, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39486 with standard deviation 0.34520.", "error": "", "parent_ids": ["f27955ce-9ba6-44c8-8fdb-c6717706216d"], "operator": null, "metadata": {"aucs": [0.985856809557228, 0.7536175384665222, 0.8855967154553273, 0.1699949977363624, 0.21131768881004176, 0.17615417329926075, 0.15001317190584573, 0.11225524547303056, 0.10896323751549075]}}
{"id": "b2f02fd9-d26f-4593-9c7f-f91c563e265d", "fitness": 0.47185679018617854, "name": "EnhancedHybridDEPSOLevy", "description": "Enhance RefinedHybridDEPSOLevy by integrating dynamic cluster adaptation and adaptive Lvy flight scaling for improved convergence.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip(diversity / self.dim, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47186 with standard deviation 0.34856.", "error": "", "parent_ids": ["f27955ce-9ba6-44c8-8fdb-c6717706216d"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2122694716411051, 0.2278345351852018, 0.11682085351857596, 0.1297237175614212, 0.36769276452686417]}}
{"id": "5d1fbc2e-a939-46a6-ba12-6b2f3f6a2cdd", "fitness": 0.4269896401204884, "name": "EnhancedHybridDEPSOLevy", "description": "Introduced adaptive mutation scaling based on the success rate of trial vectors to improve algorithm convergence.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n        self.success_rate = 0.5  # Start with a neutral success rate\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip(diversity / self.dim, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    if score < self.personal_best_scores[i]:\n                        self.success_rate = 0.9 * self.success_rate + 0.1  # Update success rate\n                    else:\n                        self.success_rate *= 0.9  # Update success rate\n                    self.f *= 1 + 0.1 * (self.success_rate - 0.5)  # Adaptive mutation scaling\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42699 with standard deviation 0.36770.", "error": "", "parent_ids": ["b2f02fd9-d26f-4593-9c7f-f91c563e265d"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8727051907577366, 0.9652608722604454, 0.1298278709207945, 0.20371289133675097, 0.24153419732898396, 0.1599222348510847, 0.1297237175614212, 0.14866407254831504]}}
{"id": "5ab3978e-2612-4b02-b9e7-38f51b6d4713", "fitness": 0.4291577562663422, "name": "RefinedAdaptiveClusterDEPSO", "description": "RefinedAdaptiveClusterDEPSO integrates environmental feedback for dynamic parameter tuning, exploiting convergence insights via memory and clustering for enhanced diverse exploration and exploitation balance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass RefinedAdaptiveClusterDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.memory = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        return np.mean(np.linalg.norm(self.population - centroid, axis=1))\n\n    def _adaptive_parameters(self, evaluations):\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.3 + 0.6 * sigmoid_adaptation\n        self.w = 0.3 + 0.6 * sigmoid_adaptation\n        if len(self.memory) > 5:\n            recent_improvement = np.mean(np.diff(self.memory[-5:]))\n            self.c1 = 1.5 + recent_improvement\n            self.c2 = 1.5 - recent_improvement\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip(diversity / self.dim, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.memory.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 44, "feedback": "The algorithm RefinedAdaptiveClusterDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42916 with standard deviation 0.36774.", "error": "", "parent_ids": ["b2f02fd9-d26f-4593-9c7f-f91c563e265d"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8820390697597178, 0.9652608722604454, 0.158013064867214, 0.19408432485061922, 0.23482151869824563, 0.13105908063450744, 0.1613699134037091, 0.14421624840375857]}}
{"id": "ce27bffa-8a5f-4fe2-9c3e-4c5d9dbb3773", "fitness": 0.423701515882071, "name": "EnhancedHybridDEPSOLevy", "description": "Enhance the mutation strategy by introducing an additional random scaling factor to increase exploration without changing more than the allowed lines.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.prev_global_best_scores = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip(diversity / self.dim, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    random_scale = np.random.rand()  # Introduced random scaling\n                    mutant = np.clip(x0 + random_scale * self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.prev_global_best_scores.append(self.global_best_score)\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42370 with standard deviation 0.38963.", "error": "", "parent_ids": ["b2f02fd9-d26f-4593-9c7f-f91c563e265d"], "operator": null, "metadata": {"aucs": [0.9831114270377264, 0.9716247606085944, 0.9652608722604454, 0.10654525704802253, 0.2133716978367185, 0.12532278740710578, 0.13762335669275638, 0.1545002048803884, 0.15595327916688162]}}
{"id": "016f9ee2-76cf-4b2d-bee1-e7646f662fd6", "fitness": 0.47601048820386527, "name": "AdaptiveHybridDEPSOLevy", "description": "Enhance AdaptiveHybridDEPSOLevy with multi-scale dynamic clustering and temperature-driven Lvy flight scaling for improved exploration and exploitation balance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 46, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47601 with standard deviation 0.34551.", "error": "", "parent_ids": ["b2f02fd9-d26f-4593-9c7f-f91c563e265d"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
{"id": "262b6e84-eb20-4e5a-9768-0923dfe333ae", "fitness": 0.32744209611810815, "name": "EnhancedHierarchicalSwarm", "description": "Introduce a hierarchical multi-swarm strategy with adaptive learning rates and a memory-based reinitialization for enhanced global search capabilities.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHierarchicalSwarm:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.global_population = np.random.uniform(-5, 5, (self.population_size, self.dim))\n        self.global_velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.global_personal_best_positions = np.copy(self.global_population)\n        self.global_personal_best_scores = np.full(self.population_size, np.inf)\n        self.global_best_position = None\n        self.global_best_score = np.inf\n        self.leader_swarm = np.random.uniform(-5, 5, (self.population_size // 5, self.dim))\n        self.leader_velocity = np.random.uniform(-1, 1, (self.population_size // 5, self.dim))\n        self.leader_best_positions = np.copy(self.leader_swarm)\n        self.leader_best_scores = np.full(self.population_size // 5, np.inf)\n        self.leader_global_best_position = None\n        self.leader_global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _dynamic_clustering(self, population):\n        num_clusters = max(2, len(population) // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(population)\n        return kmeans.labels_\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            global_cluster_labels = self._dynamic_clustering(self.global_population)\n            leader_cluster_labels = self._dynamic_clustering(self.leader_swarm)\n\n            for i in range(self.population_size):\n                if np.random.rand() < 0.1:\n                    step = self._levy_flight(scale=self.temperature)\n                    trial = np.clip(self.global_population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(global_cluster_labels == global_cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.global_population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.global_population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.global_population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.global_personal_best_scores[i]:\n                        self.global_personal_best_scores[i] = score\n                        self.global_personal_best_positions[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best_position = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.global_personal_best_positions[i] - self.global_population[i])\n                social = self.c2 * r2 * (self.global_best_position - self.global_population[i])\n                self.global_velocity[i] = self.w * self.global_velocity[i] + cognitive + social\n                self.global_population[i] = np.clip(self.global_population[i] + self.global_velocity[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            for j in range(len(self.leader_swarm)):\n                if np.random.rand() < 0.2:\n                    step = self._levy_flight(scale=self.temperature)\n                    trial = np.clip(self.leader_swarm[j] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(leader_cluster_labels == leader_cluster_labels[j])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(len(self.leader_swarm), 3, replace=False)\n                    x0, x1, x2 = self.leader_swarm[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.leader_swarm[j] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.leader_swarm[j])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.leader_best_scores[j]:\n                        self.leader_best_scores[j] = score\n                        self.leader_best_positions[j] = trial\n                        if score < self.leader_global_best_score:\n                            self.leader_global_best_score = score\n                            self.leader_global_best_position = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.leader_best_positions[j] - self.leader_swarm[j])\n                social = self.c2 * r2 * (self.leader_global_best_position - self.leader_swarm[j])\n                self.leader_velocity[j] = self.w * self.leader_velocity[j] + cognitive + social\n                self.leader_swarm[j] = np.clip(self.leader_swarm[j] + self.leader_velocity[j], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.global_personal_best_scores)\n                x0 = self.global_personal_best_positions[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best_position = new_x\n\n        return self.global_best_position", "configspace": "", "generation": 47, "feedback": "The algorithm EnhancedHierarchicalSwarm got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.32744 with standard deviation 0.37595.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.8949346318625797, 0.7878693091067559, 0.8784420403753915, 0.00782233903135987, 0.0050000000000000044, 0.023304490830135638, 0.14694329692127794, 0.12294481867550744, 0.07971793825996543]}}
{"id": "3c47d9a7-1929-4d82-b146-2edc93e7a0c5", "fitness": -Infinity, "name": "AdaptiveHybridDEPSOLevy", "description": "Enhance AdaptiveHybridDEPSOLevy with multi-scale dynamic clustering and temperature-driven Lvy flight scaling for improved exploration and exploitation balance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 47, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": "\n    Refine the strategy of the selected solution to improve it. Make sure you only change 0.9% of the code, which means if the code has 100 lines, you can only change 0.9259259259259258 lines, and the rest of the lines should remain unchanged. This input code has 108 lines, so you can only change 1 lines, the rest 107 lines should remain unchanged. This changing rate 0.9% is the mandatory requirement, you cannot change more or less than this rate.\n    ", "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
{"id": "c9dc67e2-7a45-41c8-a404-316713355ff0", "fitness": 0.47601048820386527, "name": "EnhancedAdaptiveHybridDEPSOLevy", "description": "Integrate adaptive local search intensity and swarm intelligence balance dynamically based on diversity and stagnation to enhance convergence efficiency.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.stagnation_counter = 0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0, iterations):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=iterations)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n            is_stagnant = self.stagnation_counter > 50\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                            self.stagnation_counter = 0\n                    else:\n                        self.stagnation_counter += 1\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and (evaluations % 100 == 0 or is_stagnant):\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                iterations = 5 if not is_stagnant else 10\n                new_x, new_score = self._local_search(func, x0, iterations)\n                evaluations += iterations\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n                    self.stagnation_counter = 0\n\n        return self.global_best", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedAdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47601 with standard deviation 0.34551.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
{"id": "ddf1fed8-7998-4833-9838-1c08d55b13ad", "fitness": -Infinity, "name": "AdaptiveHybridDEPSOLevyEnhanced", "description": "Introduce adaptive memory-based mutation and competition-based clustering to enhance exploration and exploitation in multi-modal search spaces.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevyEnhanced:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.memory = []\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def _adaptive_memory_mutation(self, idx, lb, ub):\n        if len(self.memory) < 3:\n            return np.random.uniform(lb, ub, self.dim)\n        idxs = np.random.choice(len(self.memory), 3, replace=False)\n        x0, x1, x2 = self.memory[idxs]\n        mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n        return mutant\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    trial = self._adaptive_memory_mutation(i, lb, ub)\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            self.memory.append(self.global_best)\n            if len(self.memory) > self.population_size:\n                self.memory.pop(0)\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 50, "feedback": "An exception occurred: TypeError('only integer scalar arrays can be converted to a scalar index').", "error": "TypeError('only integer scalar arrays can be converted to a scalar index')", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {}}
{"id": "22a5f0c1-eec8-4b9d-b090-89f3e316a681", "fitness": 0.4022238695187405, "name": "AdaptiveHybridDEPSOLevy", "description": "Improve adaptive parameters and exploration by introducing a decay factor and weighted mutation to enhance convergence precision and speed.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        decay_factor = 0.99  # New decay factor for smoothing adaptation\n        self.temperature = max(0.1, decay_factor * (1.0 - evaluations / self.budget))\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation * decay_factor  # Apply decay to inertia weight\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    weighted_diff = (x1 - x2) + 0.5 * (self.population[i] - self.global_best)  # Modified mutation\n                    mutant = np.clip(x0 + self.f * weighted_diff, lb, ub)  # Enhance weighted mutation\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 51, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40222 with standard deviation 0.38310.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [1.0, 0.856563095256282, 0.9652608722604454, 0.14086029543710554, 0.13076476210620214, 0.18940335043267664, 0.11623952304182139, 0.12994076821387524, 0.09098215892025574]}}
{"id": "0384be6a-2e56-425e-8443-cad5b669b842", "fitness": 0.44986943405683466, "name": "AdaptiveHybridDEPSOLevy", "description": "Introduce a momentum factor in velocity updates to improve convergence speed and solution quality.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.momentum = 0.8  # Added momentum factor\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.momentum * self.velocities[i] + self.w * self.velocities[i] + cognitive + social  # Added momentum\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 52, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44987 with standard deviation 0.36042.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.873628070341857, 0.9652608722604454, 0.1298278709207945, 0.12458121690092139, 0.27987229755079, 0.12087743116537397, 0.4010240512850477, 0.16219738256741867]}}
{"id": "6e97e9ee-9e21-49a6-b86b-3e7f363ad703", "fitness": 0.4526829331100162, "name": "AdaptiveHybridDEPSOLevy", "description": "Improve velocity update by incorporating a nonlinear inertia weight decay to better balance exploration and exploitation.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = (0.5 + 0.4 * np.cos(np.pi * evaluations / self.budget)) * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 53, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.45268 with standard deviation 0.35654.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8813031914194304, 0.9652608722604454, 0.14969422996204618, 0.20897289859916923, 0.24582155815504225, 0.11733062283397433, 0.36734996345041604, 0.14685734779075899]}}
{"id": "c34997ae-38f6-41da-b567-042d4df231e2", "fitness": 0.4179343885987587, "name": "AdaptiveHybridDEPSOLevy", "description": "Fine-tune cross-over probability to improve exploration-exploitation balance for better convergence.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.85  # Changed from 0.9 to 0.85\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 54, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41793 with standard deviation 0.38006.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.15510934962617895, 0.19817561767966752, 0.2278345351852018, 0.08431308640574908, 0.1297237175614212, 0.11198790902950806]}}
{"id": "f1dec47f-7a75-4015-8db2-0db858ea55ad", "fitness": 0.4499039037767425, "name": "AdaptiveHybridDEPSOLevy", "description": "Fine-tune the inertia weight decay in AdaptiveHybridDEPSOLevy to enhance convergence.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        # Changed line to improve the inertia weight decay\n        self.w = 0.4 + 0.6 * sigmoid_adaptation \n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 55, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44990 with standard deviation 0.36516.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.897448693823225, 0.9652608722604454, 0.30052754690960815, 0.11243526977899332, 0.15976689815160805, 0.3783682695876639, 0.1297237175614212, 0.11404815239885369]}}
{"id": "b1587304-38af-4e3a-9a45-2853e2eb4d4d", "fitness": 0.4345166137029386, "name": "AdaptiveHybridDEPSOLevy", "description": "Fine-tune the mutation strategy to enhance convergence by adjusting the scale of the mutation factor.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + 0.6 * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)  # Mutation factor adjusted\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 56, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43452 with standard deviation 0.37394.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652652289441503, 0.1298278709207945, 0.17146755448233586, 0.13841534669520017, 0.37876897122339137, 0.1297237175614212, 0.10817642385849768]}}
{"id": "1b8b82ce-a9f9-4a93-b734-2a16fded4141", "fitness": 0.4430688144962368, "name": "AdaptiveHybridDEPSOLevy", "description": "Integrate differential mutation scaling into clustering-based PSO for enhanced diversity management in AdaptiveHybridDEPSOLevy.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    self.f = 0.5 * (1 + np.random.rand())  # Adjust differential mutation scaling\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 57, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44307 with standard deviation 0.38141.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9831114270377264, 0.9716247606085944, 0.9652608722604454, 0.15624050665853617, 0.15817383212513347, 0.13946053740010567, 0.36562826147593097, 0.13850772737904993, 0.10961140552060922]}}
{"id": "56372e43-94b2-42c1-98fb-a19430b7e146", "fitness": 0.43547788643762864, "name": "AdaptiveHybridDEPSOLevy", "description": "Integrate chaotic maps for parameter adaptation and improve diversity maintenance in AdaptiveHybridDEPSOLevy.  ", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.x_chaos = 0.7  # Initial value for chaotic map\n\n    def _chaotic_map_update(self):\n        self.x_chaos = 4 * self.x_chaos * (1 - self.x_chaos)\n        return self.x_chaos\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n        self.cr = self._chaotic_map_update()  # Update cr using chaotic map\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 58, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43548 with standard deviation 0.37220.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.1767144195648912, 0.1185490844780328, 0.18505501887577336, 0.09830148377255743, 0.1297237175614212, 0.3566919717848802]}}
{"id": "854048c5-79fb-4ce0-bcb7-5cb31c216e40", "fitness": -Infinity, "name": "EnhancedAdaptiveHybridDEPSOLevy", "description": "Integrate adaptive self-organizing maps with temperature-scaled Lvy flights for enhanced global and local search synergy.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom scipy.stats import levy\nfrom minisom import MiniSom\n\nclass EnhancedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.som = MiniSom(x=5, y=5, input_len=dim, sigma=0.3, learning_rate=0.5)\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _som_clustering(self):\n        self.som.train_random(self.population, 100)\n        return np.array([self.som.winner(p) for p in self.population])\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._som_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 59, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'minisom'\").", "error": "ModuleNotFoundError(\"No module named 'minisom'\")", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {}}
{"id": "407fdff7-6cf6-4b36-84ed-208dbcb484e8", "fitness": 0.4115759086335241, "name": "AdaptiveHybridDEPSOLevy", "description": "Introduce a periodic adaptive Lvy flight with feedback mechanism for enhanced global exploration and local exploitation.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.levy_interval = max(1, self.budget // (10 * self.population_size))\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                if evaluations % self.levy_interval == 0:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 60, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41158 with standard deviation 0.40475.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.9952707934347658, 0.9609184812930011, 0.17255290391564704, 0.14991893875452877, 0.1361822825765533, 0.08964462229039905, 0.1297237175614212, 0.07841572435653732]}}
{"id": "8e162927-a240-46c9-98e3-5232494143d5", "fitness": 0.4307713148908786, "name": "AdaptiveHybridDEPSOLevy", "description": "Improve AdaptiveHybridDEPSOLevy by integrating a stochastic ranking selection mechanism and enhancing cluster-based exploration with weighted distance metrics for better optimization performance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                # Stochastic Ranking Selection\n                if np.random.rand() < 0.5:\n                    self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                else:\n                    self.velocities[i] = self.w * self.velocities[i] + social - cognitive\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 61, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43077 with standard deviation 0.37200.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9831114270377264, 0.953927034272829, 0.9305217445208909, 0.1794200108993571, 0.17150250688359026, 0.211940031046423, 0.14381971559224582, 0.160318046781457, 0.14238131698338774]}}
{"id": "7145efde-0086-47c0-aa07-5d2ae3e12344", "fitness": 0.3807772694519762, "name": "EnhancedMetaheuristic", "description": "Incorporate fitness landscape learning using surrogate modeling and adaptive perturbation mechanisms to dynamically guide exploration and exploitation for enhanced optimization performance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.kernel = Matern(nu=2.5)\n        self.gpr = GaussianProcessRegressor(kernel=self.kernel, normalize_y=True, n_restarts_optimizer=3)\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def _update_gpr_model(self, func, sample_points):\n        sampled_scores = np.array([func(x) for x in sample_points])\n        self.gpr.fit(sample_points, sampled_scores)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n        init_samples = np.random.uniform(lb, ub, (10, self.dim))\n        self._update_gpr_model(func, init_samples)\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                sample_points = np.random.uniform(lb, ub, (5, self.dim))\n                self._update_gpr_model(func, sample_points)\n\n        return self.global_best", "configspace": "", "generation": 62, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.38078 with standard deviation 0.35492.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9364172470606228, 0.8529676882431827, 0.8538188974591961, 0.09687451763584942, 0.12583839756194526, 0.11100405641346445, 0.14128783198656847, 0.15053869153314148, 0.15824809717381516]}}
{"id": "7af478cb-089f-4bfe-9fde-62302112de25", "fitness": 0.39215282099448706, "name": "AdaptiveHybridDEPSOLevy", "description": "Introduce a dynamic learning rate for personal best updates and incorporate elite selection in AdaptiveHybridDEPSOLevy to refine exploration and exploitation.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        learning_rate = 0.1 + 0.9 * np.exp(-evaluations/self.budget)\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = learning_rate * trial + (1 - learning_rate) * self.personal_best[i]\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                elite_indices = np.argsort(self.personal_best_scores)[:3]\n                x0 = self.personal_best[np.random.choice(elite_indices)]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 63, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39215 with standard deviation 0.39727.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974158836505937, 0.9652608722604454, 0.08325385569860733, 0.07407426270725859, 0.17128788851259702, 0.07711095583484362, 0.12538597841778387, 0.14402997834939069]}}
{"id": "d690ad6a-21c0-4c9a-bfb8-c22311920e03", "fitness": 0.4007349235819552, "name": "AdaptiveMultiPopDEPSOLevy", "description": "Integrate adaptive multi-population strategies with temperature and diversity-driven scaling for improved convergence and robustness in complex search landscapes.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveMultiPopDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.num_subpopulations = 5\n        self.subpopulation_size = self.population_size // self.num_subpopulations\n        self.populations = [np.random.uniform(-5, 5, (self.subpopulation_size, dim)) for _ in range(self.num_subpopulations)]\n        self.velocities = [np.random.uniform(-1, 1, (self.subpopulation_size, dim)) for _ in range(self.num_subpopulations)]\n        self.personal_best = [np.copy(pop) for pop in self.populations]\n        self.personal_best_scores = [np.full(self.subpopulation_size, np.inf) for _ in range(self.num_subpopulations)]\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _global_search(self, func, evaluations, lb, ub):\n        for pop_idx in range(self.num_subpopulations):\n            population = self.populations[pop_idx]\n            velocities = self.velocities[pop_idx]\n            personal_best = self.personal_best[pop_idx]\n            personal_best_scores = self.personal_best_scores[pop_idx]\n\n            for i in range(self.subpopulation_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=self.temperature)\n                    trial = np.clip(population[i] + step, lb, ub)\n                else:\n                    idxs = np.random.choice(self.subpopulation_size, 3, replace=False)\n                    x0, x1, x2 = population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, population[i])\n\n                score = func(trial)\n                evaluations += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (personal_best[i] - population[i])\n                social = self.c2 * r2 * (self.global_best - population[i])\n                velocities[i] = self.w * velocities[i] + cognitive + social\n                population[i] = np.clip(population[i] + velocities[i], lb, ub)\n\n        return evaluations\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n            evaluations = self._global_search(func, evaluations, lb, ub)\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                for pop_idx in range(self.num_subpopulations):\n                    best_idx = np.argmin(self.personal_best_scores[pop_idx])\n                    x0 = self.personal_best[pop_idx][best_idx]\n                    new_x, new_score = self._local_search(func, x0)\n                    evaluations += 5\n                    \n                    if new_score < self.global_best_score:\n                        self.global_best_score = new_score\n                        self.global_best = new_x\n\n        return self.global_best\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=3)\n        return result.x, result.fun", "configspace": "", "generation": 64, "feedback": "The algorithm AdaptiveMultiPopDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40073 with standard deviation 0.39388.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9731433812200059, 1.0, 0.8964780399930944, 0.1298278709207945, 0.1298278709207945, 0.1298278709207945, 0.11494808028610437, 0.12142488102679616, 0.11113631694921233]}}
{"id": "9a3391ed-51c6-4fed-99f2-5d175bcdc93a", "fitness": 0.47601048820386527, "name": "AdaptiveHybridDEPSOLevyImproved", "description": "Integrate self-adaptive mutation rates and feedback-driven clustering to enhance convergence and robustness in high-dimensional optimization.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevyImproved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self, evaluations):\n        num_clusters = max(2, self.population_size // (10 + evaluations // (self.budget // 10)))\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering(evaluations)\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 65, "feedback": "The algorithm AdaptiveHybridDEPSOLevyImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47601 with standard deviation 0.34551.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
{"id": "92113c8e-873a-4de2-87a6-24cb25c8fb8c", "fitness": 0.3944069080163858, "name": "EnhancedAdaptiveHybridDEPSOLevy", "description": "Use AdaptiveHybridDEPSOLevy with enhanced positional updates through covariance-based sampling and nonlinear fitness landscape adaptation for refined search efficiency.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def _covariance_update(self):\n        # Compute the covariance matrix of the population\n        covariance_matrix = np.cov(self.population, rowvar=False)\n        return covariance_matrix\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            covariance_matrix = self._covariance_update()\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + np.dot(self.f * (self.population[i] - x0), covariance_matrix), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + np.dot(self.velocities[i], covariance_matrix), lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 66, "feedback": "The algorithm EnhancedAdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39441 with standard deviation 0.39870.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.9135433948483761, 0.9652608722604454, 0.12421037652192168, 0.13208298211002079, 0.1298278709207945, 0.05746939628573744, 0.13139299074650723, 0.10431857493480579]}}
{"id": "76f7f04c-dabd-4d38-8bc0-cf5376bb84b3", "fitness": 0.43274605910629, "name": "EnhancedAdaptiveHybridDEPSOLevy", "description": "Introduce stochastic charge-based movement and self-adaptive mutation rates to balance exploration and exploitation dynamically across the search space.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.charge_sign = np.random.choice([-1, 1], size=self.population_size)\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                charge_effect = self.charge_sign[i] * np.random.rand()\n                mutation_rate = np.clip(self.f + charge_effect, 0.1, 1.0)\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + mutation_rate * (x1 - x2) + mutation_rate * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedAdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43275 with standard deviation 0.37049.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [1.0, 0.8982286196253337, 0.9641873055419722, 0.22627915585933867, 0.18548418018693535, 0.19248541371446437, 0.14953991325492066, 0.12318244321251637, 0.15532750056112865]}}
{"id": "1c57ee81-46f4-4be0-bdb9-b1a2a9ab3f54", "fitness": 0.42897994000527895, "name": "AdaptiveHybridDEPSOLevy", "description": "Improve exploration-exploitation balance by enhancing diversity control through adaptive personal best inclusion and refined cluster strategies.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 8)  # Change 1: increase number of clusters\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    personal_influence = self.personal_best[i] - x0  # Change 2: incorporate personal best\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * personal_influence, lb, ub)  # Change 3: adaptive personal factor\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 68, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42898 with standard deviation 0.37312.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.14457903752328605, 0.295183968569519, 0.15938333852343756, 0.14129355382276332, 0.13014542955981556, 0.13596885014758753]}}
{"id": "97b8b6c6-1801-40b4-883b-07f478177ae4", "fitness": -Infinity, "name": "EnhancedAdaptiveHybridDEPSOLevy", "description": "Enhance AdaptiveHybridDEPSOLevy with a multi-agent simulated annealing approach and random subgroup dynamic recombination for robust global search and local intensification.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def _simulated_annealing_move(self, current_solution, func):\n        perturbation = np.random.uniform(-0.5, 0.5, self.dim) * self.temperature\n        new_solution = np.clip(current_solution + perturbation, func.bounds.lb, func.bounds.ub)\n        new_score = func(new_solution)\n        return new_solution, new_score\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    subgroup_size = max(2, len(cluster_idx) // 2)\n                    subgroup = np.random.choice(cluster_idx, subgroup_size, replace=False)\n                    x0, x1 = self.population[np.random.choice(subgroup, 2, replace=False)]\n                    mutant = np.clip(x0 + self.f * (x1 - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 50 == 0:\n                for agent in range(self.population_size):\n                    new_x, new_score = self._simulated_annealing_move(self.population[agent], func)\n                    evaluations += 1\n                    if new_score < self.personal_best_scores[agent]:\n                        self.personal_best_scores[agent] = new_score\n                        self.personal_best[agent] = new_x\n                        if new_score < self.global_best_score:\n                            self.global_best_score = new_score\n                            self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 69, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {}}
{"id": "d485cf26-7021-48aa-b161-9231942d238f", "fitness": 0.47694874225732486, "name": "AdaptiveHybridDEPSOLevy", "description": "Integrate parallel elite solutions and adaptive boundary handling to enhance convergence speed and precision.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 70, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47695 with standard deviation 0.34691.", "error": "", "parent_ids": ["016f9ee2-76cf-4b2d-bee1-e7646f662fd6"], "operator": null, "metadata": {"aucs": [1.0, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
{"id": "8b9a38ac-08b4-4e99-b347-6ee5f73c4115", "fitness": 0.41630097388272974, "name": "EnhancedAdaptiveHybridDEPSOLevy", "description": "Enhance population diversity and adaptive strategies by introducing a hierarchical clustering mechanism and tuned evolutionary parameters.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.7\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n    \n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.3 + 0.7 * sigmoid_adaptation\n        self.w = 0.3 + 0.6 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _hierarchical_clustering(self):\n        clusterer = AgglomerativeClustering(n_clusters=None, distance_threshold=0.1)\n        cluster_labels = clusterer.fit_predict(self.population)\n        return cluster_labels\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._hierarchical_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]\n        self.population[elite_population_idx] = self.global_best + 0.05 * np.random.randn(5, self.dim)\n\n        return self.global_best", "configspace": "", "generation": 71, "feedback": "The algorithm EnhancedAdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41630 with standard deviation 0.38017.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9387254544500956, 1.0, 0.9165205084487701, 0.21795380237172213, 0.1298278709207945, 0.14275107515528163, 0.12216216199661545, 0.15914821855498762, 0.11961967304630017]}}
{"id": "1f420eab-3535-4dd5-9367-abb1fff49e20", "fitness": 0.4363196445539095, "name": "EnhancedHybridOptimizer", "description": "Integrate adaptive clustering with parallel search strategies and temperature-controlled mutation to balance exploration and exploitation effectively.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"L-BFGS-B\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 5)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                for elite_idx in np.argsort(self.personal_best_scores)[:5]:\n                    x0 = self.personal_best[elite_idx]\n                    new_x, new_score = self._local_search(func, x0)\n                    evaluations += 5\n                    if new_score < self.global_best_score:\n                        self.global_best_score = new_score\n                        self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 72, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.43632 with standard deviation 0.37336.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9570766726590763, 0.9545274452429235, 0.9555335091417111, 0.1059679809509656, 0.1624366248520921, 0.2237564244221173, 0.1259662060710498, 0.10515307686840358, 0.33645886077684606]}}
{"id": "257a3e27-b018-45d9-8836-7f72adf9fca5", "fitness": 0.4137360788556121, "name": "EnhancedHybridDEPSOLevy", "description": "Enhance convergence by incorporating adaptive parameter tuning with opposition-based learning and dynamic topology adjustment.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n    \n    def _adaptive_parameters(self, evaluations):\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n        self.c1 = 1.5 + sigmoid_adaptation\n        self.c2 = 1.5 + sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def _opposition_based_learning(self, x, lb, ub):\n        return lb + ub - x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim), 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                opposite = self._opposition_based_learning(self.population[i], lb, ub)\n                candidate = np.clip((self.population[i] + opposite) / 2 + self.velocities[i], lb, ub)\n                self.population[i] = candidate\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        return self.global_best", "configspace": "", "generation": 73, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41374 with standard deviation 0.38342.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.9052817413364362, 0.9652608722604454, 0.18134021239907394, 0.17559893507826618, 0.11353935073653654, 0.15981867398900174, 0.1297237175614212, 0.1015054928204644]}}
{"id": "40cedb69-21f1-4d43-b16f-a545bad4a950", "fitness": 0.4428685714451359, "name": "AdaptiveHybridDEPSOLevy", "description": "Enhance the exploration capability by adjusting the cognitive component of the velocity update dynamically.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i]) * self.temperature\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 74, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44287 with standard deviation 0.36668.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.16030311506523487, 0.18000542277443843, 0.1364250208474702, 0.13837585989888657, 0.3716233389595507, 0.14481910355954086]}}
{"id": "a629380b-93f6-48fc-a5a4-0ca8319b129c", "fitness": 0.4075988203267802, "name": "AdaptiveHybridDEPSOLevy", "description": "Introduce learning momentum to the velocity update for enhanced convergence and stability.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.momentum = 0.5  # New parameter for learning momentum\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                # Modified line: Apply momentum to velocity update\n                self.velocities[i] = self.momentum * self.velocities[i] + self.w * (cognitive + social)\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 75, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40760 with standard deviation 0.38686.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8970198663575939, 0.9652608722604454, 0.2232087656343934, 0.15413018466784212, 0.11800195272042746, 0.08164900223751215, 0.1297237175614212, 0.10783930798252339]}}
{"id": "3afe4695-7eeb-400f-ba0a-370442016f95", "fitness": 0.4049086879255792, "name": "AdaptiveHybridDEPSOLevy", "description": "Introduce chaotic maps for parameter tuning and use an adaptive crossover to balance exploration and exploitation.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        # Use a chaotic map for adaptive f parameter\n        self.f = 0.5 * (1 + np.sin(np.pi * evaluations / self.budget))\n        # Adaptive crossover using diversity\n        self.cr = 0.5 + 0.4 * (self._calculate_diversity() / self.dim)\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 76, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40491 with standard deviation 0.38354.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8725913770203424, 0.9652608722604454, 0.08709670718669371, 0.1298278709207945, 0.20930603001349501, 0.08774409240793202, 0.1820013741622164, 0.1187941538394307]}}
{"id": "0a088def-7845-428b-895e-25ed5bc0d10b", "fitness": 0.40235341652076284, "name": "EnhancedChaosHybridDEPSO", "description": "Enhance convergence by incorporating chaos-based dynamic parameter control and Gaussian mutation to maintain exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedChaosHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.chaos_lambda = 3.7  # Logistic map constant\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _chaos_based_adaptive_parameters(self, evaluations):\n        chaos_factor = np.sin(self.chaos_lambda * evaluations / self.budget)\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        self.f = 0.4 + 0.5 * chaos_factor\n        self.w = 0.4 + 0.5 * chaos_factor\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=3)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._chaos_based_adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                mutation_noise = np.random.normal(0, 0.1, self.dim)\n                self.population[i] = np.clip(self.population[i] + mutation_noise, lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 3\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)\n\n        return self.global_best", "configspace": "", "generation": 77, "feedback": "The algorithm EnhancedChaosHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40235 with standard deviation 0.38197.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9201831887866251, 0.9636180218840888, 0.9401725157342933, 0.1455494987406658, 0.13598847300924044, 0.1827851055133468, 0.0887190084881766, 0.1297237175614212, 0.11444121896900772]}}
{"id": "85a2dacf-7725-41f4-946c-18d286ad4469", "fitness": 0.44032369535663696, "name": "AdaptiveHybridDEPSOLevy", "description": "Boost convergence by fine-tuning crossover rate to 0.95 for improved exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.95  # Adjusted crossover rate\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 78, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44032 with standard deviation 0.36929.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.3502269042928663, 0.22438339380911787, 0.20577182354065826, 0.09163245603329728, 0.1297237175614212, 0.10690968107126997]}}
{"id": "2d3961bb-86bc-4462-8d68-880df17524fa", "fitness": 0.47601048820386527, "name": "AdaptiveHybridDEPSOLevy", "description": "Enhance convergence by adjusting the crossover rate dynamically based on the clustering diversity.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n            self.cr = 0.6 + 0.3 * np.tanh(diversity)  # Adjusted line\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 79, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47601 with standard deviation 0.34551.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
{"id": "7b661f4c-beb4-4d12-9c40-5964a6a93fa1", "fitness": 0.4583467234815271, "name": "AdaptiveHybridDEPSOLevy", "description": "Enhance exploration by adjusting levy scale using a non-linear function of diversity.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip(np.tanh(diversity / self.dim) * self.temperature, 0.1, 1.0)  # Changed line\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 80, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.45835 with standard deviation 0.35774.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915348053283939, 0.8969910617140731, 0.9652323897138103, 0.17903865365123872, 0.21132255767042873, 0.20054560957862644, 0.11999598686809765, 0.4120294873589002, 0.1484299594501759]}}
{"id": "5880d30a-d56f-4bd6-bdff-d0d2cef1e906", "fitness": 0.4164307061746464, "name": "EnhancedSwarmAdaptiveDEPSOLevy", "description": "Enhance convergence and precision by integrating adaptive mutation strategies and dynamic swarm intelligence for balanced exploration and exploitation.  ", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedSwarmAdaptiveDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                use_levy_flight = i % 3 == 0\n                if use_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  \n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  \n\n        return self.global_best", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedSwarmAdaptiveDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41643 with standard deviation 0.39196.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9620007108348843, 0.971647175170336, 0.9740508463058284, 0.13886438869610895, 0.20940679066953582, 0.15024205745601293, 0.09814603599609673, 0.1297237175614212, 0.11379463288159286]}}
{"id": "ab6a9e25-c79a-49c6-81ea-aa74518c18e5", "fitness": 0.36178435763339106, "name": "QuantumHybridDEPSO", "description": "Enhance convergence by integrating quantum-inspired tunneling for escaping local optima and stochastic rank-based selection for diversity maintenance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass QuantumHybridDEPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def _quantum_tunneling(self):\n        tunneling_scale = np.random.exponential(scale=1.0, size=self.dim)\n        return tunneling_scale * np.random.choice([-1, 1], size=self.dim)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                use_quantum_tunneling = i % 3 == 0\n                if use_quantum_tunneling:\n                    step = self._quantum_tunneling()\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        sorted_indices = np.argsort(self.personal_best_scores)\n        top_elites = sorted_indices[:5]\n        self.population[top_elites] = self.global_best + 0.1 * np.random.randn(5, self.dim)\n\n        return self.global_best", "configspace": "", "generation": 82, "feedback": "The algorithm QuantumHybridDEPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.36178 with standard deviation 0.31098.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.793476045227488, 0.7906655314467251, 0.8166414394217651, 0.10934371115765362, 0.1843553662231393, 0.1735472470573889, 0.1288288349385871, 0.11426736374741608, 0.14493367948035607]}}
{"id": "f4b358dc-f1fd-48cf-9fa6-c8b6c57a33e6", "fitness": 0.4437250920612938, "name": "AdaptiveHybridDEPSOLevy", "description": "Refine adaptive mechanisms by tuning diversity calculations to improve convergence and exploration balance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1)) * 1.1  # Adjusting diversity calculation\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 83, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.44373 with standard deviation 0.36563.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2122694716411051, 0.2278345351852018, 0.11561460718047234, 0.1297237175614212, 0.11571372774100452]}}
{"id": "3fc77375-ea31-4e52-bcab-3755a6241ebf", "fitness": 0.41831817253718706, "name": "AdaptiveHybridDEPSOLevy", "description": "Enhance convergence by using an elite-guided levy flight and adaptive mutation strategy.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    elite_idx = np.argmin(self.personal_best_scores)\n                    elite_solution = self.personal_best[elite_idx]  # Change 1\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (elite_solution - x0), lb, ub)  # Change 2\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 84, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41832 with standard deviation 0.37325.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8738871310919543, 0.9652825329169974, 0.20447721542401753, 0.14686693524908223, 0.16772827895653364, 0.13431298367918376, 0.16453393844139086, 0.11621882355666058]}}
{"id": "16de493b-c832-4b3a-8b0b-c52564c25809", "fitness": 0.4070811053416225, "name": "EnhancedAdaptiveHybridDEPSOLevy", "description": "Enhance adaptive control by integrating chaotic maps for parameter tuning and employing dynamic population resizing to improve exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy, uniform\n\nclass EnhancedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.05, 1.0 - evaluations / self.budget)\n        chaotic_factor = (3.9 * uniform.rvs() * (1 - uniform.rvs()))  # Logistic map for chaos\n        self.f = 0.4 + 0.6 * chaotic_factor\n        self.w = 0.3 + 0.7 * chaotic_factor\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Powell\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def _dynamic_population_resizing(self, evaluations):\n        if evaluations / self.budget > 0.5:\n            self.population_size = max(self.initial_population_size // 2, 5)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n            self._dynamic_population_resizing(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                if i % 2 == 0:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                \n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 85, "feedback": "The algorithm EnhancedAdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40708 with standard deviation 0.41431.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9820096578860609, 1.0, 0.9628225979019607, 0.028087778373969585, 0.06860580505306002, 0.023750274372561386, 0.13407515914427737, 0.3130898156500357, 0.1512888596926768]}}
{"id": "42a8bea5-effb-4588-8fd5-36d561bc99bb", "fitness": 0.38921634342303957, "name": "EnhancedAdaptiveHybridDEPSOLevy", "description": "Enhance convergence by integrating adaptive Levy flight, elite perturbation, and swarm intelligence with dynamic parameter tuning.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f_base = 0.5\n        self.cr = 0.9\n        self.w_base = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        progress_ratio = evaluations / self.budget\n        self.w = self.w_base - 0.5 * progress_ratio\n        self.f = self.f_base + 0.3 * np.sin(2 * np.pi * progress_ratio)\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim), 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 3 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]\n        for idx in elite_population_idx:\n            self.population[idx] = np.clip(self.global_best + 0.1 * np.random.randn(self.dim), lb, ub)\n\n        return self.global_best", "configspace": "", "generation": 86, "feedback": "The algorithm EnhancedAdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.38922 with standard deviation 0.40754.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9746500935615637, 1.0, 0.9173161794534482, 0.1257415998527337, 0.1298278709207945, 0.1298278709207945, 0.06339347371324056, 0.07336152889206882, 0.08882847349271217]}}
{"id": "b96cfbc6-366d-4989-ae22-2054ae5bb20d", "fitness": 0.4233666978068029, "name": "QuantumAdaptiveDEPSOLevy", "description": "Incorporate differential mutation with adaptive scale factor and quantum-inspired local search to enhance exploration and convergence.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass QuantumAdaptiveDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f_min = 0.1\n        self.f_max = 0.9\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Powell\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n            self.f = self.f_min + (self.f_max - self.f_min) * (diversity / self.dim)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)\n\n        return self.global_best", "configspace": "", "generation": 87, "feedback": "The algorithm QuantumAdaptiveDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42337 with standard deviation 0.39439.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8970198663575939, 0.9656673079635034, 0.02842584528627401, 0.02527151316685039, 0.03555581955717235, 0.3415010841725814, 0.1296216942049847, 0.3956814360334032]}}
{"id": "98e183cd-0bcb-4966-9f67-fc541f7d3378", "fitness": 0.3997646502915608, "name": "EnhancedAdaptiveHybridDEPSOLevy", "description": "Enhance adaptability by integrating a multi-phase learning strategy and an adaptive inertia weight mechanism to improve exploration-exploitation balance and convergence.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.phase = 1\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            phase_threshold = self.budget // 3\n            if evaluations > phase_threshold and evaluations <= 2 * phase_threshold:\n                self.phase = 2\n            elif evaluations > 2 * phase_threshold:\n                self.phase = 3\n\n            for i in range(self.population_size):\n                if self.phase == 1 or i % 2 == 0:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 88, "feedback": "The algorithm EnhancedAdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39976 with standard deviation 0.41261.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.9952707934347658, 0.9609184812930011, 0.1298278709207945, 0.08218599091533618, 0.1298278709207945, 0.09878216698738984, 0.12972371767057633, 0.07978924696252532]}}
{"id": "ed628d58-ae67-40ff-9812-b733ad34e7a8", "fitness": 0.3713892178803555, "name": "AdaptiveHybridDEPSOLevy", "description": "Introduce a dynamic levy-scaled perturbation during local search to enhance exploration.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        levy_step = self._levy_flight(scale=self.temperature)  # Modified line\n        result = basinhopping(lambda x: func(x), x0 + levy_step, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 89, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.37139 with standard deviation 0.41227.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.09770424741772199, 0.04063392186024828, 0.00741289294474623, 0.11875654865926466, 0.12538597841778387, 0.09834408972233277]}}
{"id": "469d489d-b365-4a57-93f7-4f7de4035ba3", "fitness": 0.42407526948707236, "name": "EnhancedHybridDEPSOLevy", "description": "Enhance convergence by integrating adaptive learning rates and stochastic sampling strategies with elite preservation.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f_min = 0.4\n        self.f_max = 0.9\n        self.cr = 0.9\n        self.w_min = 0.4\n        self.w_max = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        progress_ratio = evaluations / self.budget\n        self.temperature = max(0.1, 1.0 - progress_ratio)\n        self.f = self.f_min + (self.f_max - self.f_min) * (1 - progress_ratio)\n        self.w = self.w_min + (self.w_max - self.w_min) * (1 - progress_ratio)\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                if i % 2 == 0:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n                \n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)\n\n        return self.global_best", "configspace": "", "generation": 90, "feedback": "The algorithm EnhancedHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42408 with standard deviation 0.36895.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.872496682829246, 0.9653629255420463, 0.1984810138783213, 0.20194076531169214, 0.1298278709207945, 0.13390566173832408, 0.17126957892715178, 0.1518372127172123]}}
{"id": "fa1fc1f2-513d-4c78-bbcb-cdaa415df890", "fitness": -Infinity, "name": "EnhancedHybridDEPSOLevy", "description": "Incorporate feedback-driven parameter tuning and cooperative subpopulation strategies to dynamically adapt exploration and exploitation balance.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n        self.evaluations = 0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self):\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (self.evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n        if self.evaluations % 50 == 0:\n            self.c1, self.c2 = np.random.uniform(1.5, 2.5, 2)  # Randomly adjust cognitive and social factors\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def _cooperative_subpopulations(self, func):\n        num_subpopulations = self.population_size // 5\n        for _ in range(num_subpopulations):\n            indices = np.random.choice(self.population_size, 5, replace=False)\n            subpopulation = self.population[indices]\n            sub_best_idx = np.argmin(self.personal_best_scores[indices])\n            sub_best = subpopulation[sub_best_idx]\n            sub_best_score = self.personal_best_scores[indices[sub_best_idx]]\n            for i in indices:\n                if self.evaluations >= self.budget:\n                    break\n                step = self._levy_flight(scale=self.temperature)\n                trial = np.clip(subpopulation[i] + step, func.bounds.lb, func.bounds.ub)\n                score = func(trial)\n                self.evaluations += 1\n                if score < self.personal_best_scores[i]:\n                    self.personal_best_scores[i] = score\n                    self.personal_best[i] = trial\n                    if score < self.global_best_score:\n                        self.global_best_score = score\n                        self.global_best = trial\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        while self.evaluations < self.budget:\n            self._adaptive_parameters()\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if self.evaluations < self.budget:\n                    score = func(trial)\n                    self.evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if self.evaluations >= self.budget:\n                    break\n\n            if self.evaluations < self.budget and self.evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                self.evaluations += 5  # Increment evaluation count by 5 for local search\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n            self._cooperative_subpopulations(func)\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 91, "feedback": "An exception occurred: IndexError('index 16 is out of bounds for axis 0 with size 5').", "error": "IndexError('index 16 is out of bounds for axis 0 with size 5')", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {}}
{"id": "2eb482a0-93ed-420f-87f9-a8281a1d0853", "fitness": 0.42623977514278355, "name": "AdaptiveHybridDEPSOLevy", "description": "Enhance solution diversity and convergence with adaptive cluster-based mutation strategy.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n            elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n            self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 92, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.42624 with standard deviation 0.37550.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [1.0, 0.8970198663575939, 0.9652608722604454, 0.18236464852345013, 0.1944483808227937, 0.22210343563400803, 0.13645231217914155, 0.13091546571611046, 0.10759299479150852]}}
{"id": "1e86e953-fd9e-4233-a1c1-572b4a48c443", "fitness": 0.47601048820386527, "name": "AdaptiveHybridDEPSOLevy", "description": "Incorporate Gaussian noise instead of random noise to refine elite solutions, ensuring precision in convergence.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject Gaussian noise\n\n        return self.global_best", "configspace": "", "generation": 93, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47601 with standard deviation 0.34551.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
{"id": "1efe0532-6662-4fd5-a209-9d32b46837ae", "fitness": 0.398338735256959, "name": "EnhancedAdaptiveHybrid", "description": "Integrate adaptive parallel elite solutions with dynamic parameter tuning and enhanced exploitation through local search to improve convergence and precision.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass EnhancedAdaptiveHybrid:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"L-BFGS-B\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters, n_init=10)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n            elite_population_idx = np.argsort(self.personal_best_scores)[:5]\n            self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)\n\n        return self.global_best", "configspace": "", "generation": 94, "feedback": "The algorithm EnhancedAdaptiveHybrid got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.39834 with standard deviation 0.39180.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8970198663575939, 0.9652608722604454, 0.12468064009309232, 0.13039432273335239, 0.1131013805211536, 0.13553138096920936, 0.1295842950612227, 0.09792014579769803]}}
{"id": "6fcba335-062a-445a-8c0e-0877d06d63e9", "fitness": 0.47601048820386527, "name": "AdaptiveHybridDEPSOLevy", "description": "Introduce Gaussian perturbation to diversify exploration and refine convergence.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n    \n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim) + np.random.normal(0, 0.1, (5, self.dim))  # Gaussian perturbation\n\n        return self.global_best", "configspace": "", "generation": 95, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47601 with standard deviation 0.34551.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
{"id": "411f7d54-2acd-402f-b04a-19d62e5480cb", "fitness": 0.47601048820386527, "name": "AdaptiveHybridDEPSOLevy", "description": "Slightly increase exploration by enlarging the mutation factor range.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.7  # Changed from 0.5 to 0.7 for more exploration\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 96, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47601 with standard deviation 0.34551.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
{"id": "2a0498cf-c4f1-44be-a5f5-30b9340b8b79", "fitness": 0.4013977109106181, "name": "RefinedAdaptiveHybridDEPSOLevy", "description": "Incorporate adaptive learning rates and chaotic local search to dynamically enhance exploration and exploitation balance for improved convergence.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass RefinedAdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.w_initial = 0.9\n        self.w_final = 0.4\n        self.c1_initial = 2.5\n        self.c1_final = 0.5\n        self.c2_initial = 0.5\n        self.c2_final = 2.5\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        fraction = evaluations / self.budget\n        self.w = self.w_initial - fraction * (self.w_initial - self.w_final)\n        self.c1 = self.c1_initial - fraction * (self.c1_initial - self.c1_final)\n        self.c2 = self.c2_initial + fraction * (self.c2_final - self.c2_initial)\n\n    def _chaotic_local_search(self, func, x0, evaluations):\n        chaotic_factor = (1.0 + np.sin(np.pi * evaluations / self.budget)) / 2.0\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        local_optimum = result.x + chaotic_factor * np.random.randn(self.dim)\n        local_score = func(local_optimum)\n        return local_optimum, local_score\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * (1.0 - evaluations / self.budget), 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.w * (x1 - x2) + self.w * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.c1\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._chaotic_local_search(func, x0, evaluations)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)\n\n        return self.global_best", "configspace": "", "generation": 97, "feedback": "The algorithm RefinedAdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.40140 with standard deviation 0.38027.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8478859980755208, 0.9652608722604454, 0.1695753243125323, 0.14855436349452833, 0.18440219307451133, 0.06969068764372965, 0.1297237175614212, 0.10593052825401128]}}
{"id": "fac99e7f-c628-4664-9ecf-4c5bc2bf3089", "fitness": 0.4170061798032921, "name": "AdaptiveHybridDEPSOLevy", "description": "Improve convergence by dynamically adjusting exploration-exploitation balance and integrating a mutation operator.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                mutation = 0.01 * np.random.randn(self.dim)  # Mutation operator added\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social + mutation  # Include mutation\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.1 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 98, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.41701 with standard deviation 0.37250.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9207595119642301, 0.963618455451342, 0.9401725157342933, 0.17406696911944253, 0.22495599569212754, 0.1744624772308715, 0.10782151099874326, 0.12972371777014602, 0.11747446426843255]}}
{"id": "c664b78d-4e14-4123-ae76-12a5aa8d3df2", "fitness": 0.47601048820386527, "name": "AdaptiveHybridDEPSOLevy", "description": "Introduce strategic noise injection to enhance exploration capabilities in elite solutions.", "code": "import numpy as np\nfrom scipy.optimize import basinhopping\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import levy\n\nclass AdaptiveHybridDEPSOLevy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.population = None\n        self.velocities = None\n        self.personal_best = None\n        self.personal_best_scores = None\n        self.global_best = None\n        self.global_best_score = np.inf\n        self.f = 0.5\n        self.cr = 0.9\n        self.w = 0.9\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1.0\n\n    def _calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def _adaptive_parameters(self, evaluations):\n        self.temperature = max(0.1, 1.0 - evaluations / self.budget)\n        sigmoid_adaptation = 1.0 / (1.0 + np.exp(-0.1 * (evaluations - self.budget / 2)))\n        self.f = 0.4 + 0.5 * sigmoid_adaptation\n        self.w = 0.4 + 0.5 * sigmoid_adaptation\n\n    def _local_search(self, func, x0):\n        result = basinhopping(lambda x: func(x), x0, minimizer_kwargs={\"method\": \"Nelder-Mead\"}, niter=5)\n        return result.x, result.fun\n\n    def _levy_flight(self, scale=1.0):\n        return levy.rvs(size=self.dim) * scale\n\n    def _dynamic_clustering(self):\n        num_clusters = max(2, self.population_size // 10)\n        kmeans = KMeans(n_clusters=num_clusters)\n        kmeans.fit(self.population)\n        return kmeans.labels_\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        self.personal_best = np.copy(self.population)\n        self.personal_best_scores = np.full(self.population_size, np.inf)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            self._adaptive_parameters(evaluations)\n\n            cluster_labels = self._dynamic_clustering()\n            diversity = self._calculate_diversity()\n            levy_scale = np.clip((diversity / self.dim) * self.temperature, 0.1, 1.0)\n\n            for i in range(self.population_size):\n                has_levy_flight = i % 2 == 0\n                if has_levy_flight:\n                    step = self._levy_flight(scale=levy_scale)\n                    trial = np.clip(self.population[i] + step, lb, ub)\n                else:\n                    cluster_idx = np.where(cluster_labels == cluster_labels[i])[0]\n                    if len(cluster_idx) > 3:\n                        idxs = np.random.choice(cluster_idx, 3, replace=False)\n                    else:\n                        idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x0, x1, x2 = self.population[idxs]\n                    mutant = np.clip(x0 + self.f * (x1 - x2) + self.f * (self.population[i] - x0), lb, ub)\n                    crossover = np.random.rand(self.dim) < self.cr\n                    trial = np.where(crossover, mutant, self.population[i])\n\n                if evaluations < self.budget:\n                    score = func(trial)\n                    evaluations += 1\n                    if score < self.personal_best_scores[i]:\n                        self.personal_best_scores[i] = score\n                        self.personal_best[i] = trial\n                        if score < self.global_best_score:\n                            self.global_best_score = score\n                            self.global_best = trial\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                cognitive = self.c1 * r1 * (self.personal_best[i] - self.population[i])\n                social = self.c2 * r2 * (self.global_best - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive + social\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], lb, ub)\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations < self.budget and evaluations % 100 == 0:\n                best_idx = np.argmin(self.personal_best_scores)\n                x0 = self.personal_best[best_idx]\n                new_x, new_score = self._local_search(func, x0)\n                evaluations += 5\n                if new_score < self.global_best_score:\n                    self.global_best_score = new_score\n                    self.global_best = new_x\n\n        elite_population_idx = np.argsort(self.personal_best_scores)[:5]  # Select top 5 elite solutions\n        self.population[elite_population_idx] = self.global_best + 0.05 * np.random.randn(5, self.dim)  # Inject noise\n\n        return self.global_best", "configspace": "", "generation": 99, "feedback": "The algorithm AdaptiveHybridDEPSOLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.47601 with standard deviation 0.34551.", "error": "", "parent_ids": ["d485cf26-7021-48aa-b161-9231942d238f"], "operator": null, "metadata": {"aucs": [0.9915557135188632, 0.8974486961217929, 0.9652608722604454, 0.33810448734133725, 0.2254762830453142, 0.2278345351852018, 0.3768976312390341, 0.1297237175614212, 0.13179245756137692]}}
