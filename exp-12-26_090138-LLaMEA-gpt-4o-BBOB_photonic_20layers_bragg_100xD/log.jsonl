{"id": "9689bb0e-41c5-442f-9afb-89bace3fdc19", "fitness": 0.06412636216845469, "name": "HybridOptimizer", "description": "A hybrid optimization algorithm combining differential evolution and adaptive local search to efficiently explore and exploit the search space.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n        \n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Adaptive Local Search\n                if eval_count < self.budget and np.random.rand() < 0.1:  # 10% chance for local search\n                    new_trial = trial + np.random.normal(0, 0.1, self.dim)\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 0, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06413 with standard deviation 0.05680.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13840725198197212, 0.13840725198197212, 0.13840725198197212, 0.05347183452339199, 0.05347183452339199, 0.05347183452339199, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "492cc14d-b736-490e-b61f-112b5203ed61", "fitness": 0.060767837740955365, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid algorithm integrating differential evolution with adaptive local search and dynamic parameter tuning to optimize exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F = 0.8  # Initial differential weight\n        CR = 0.9  # Initial crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n        \n        # Track best fitness for dynamic parameter adjustment\n        best_fitness = np.min(fitness)\n        \n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic adjustment of F and CR\n                F = 0.7 + 0.3 * np.random.rand()  # Randomize F for diversity\n                CR = 0.8 + 0.2 * np.random.rand()  # Randomize CR for diversity\n                \n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    # Update best fitness\n                    if f_trial < best_fitness:\n                        best_fitness = f_trial\n                \n                # Adaptive Local Search\n                if eval_count < self.budget and np.random.rand() < 0.2:  # 20% chance for local search\n                    new_trial = trial + np.random.normal(0, 0.1, self.dim)\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if f_new_trial < best_fitness:\n                            best_fitness = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06077 with standard deviation 0.05582.", "error": "", "parent_ids": ["9689bb0e-41c5-442f-9afb-89bace3fdc19"], "operator": null, "metadata": {"aucs": [0.13505727997786232, 0.13505727997786232, 0.13505727997786232, 0.04674623324500382, 0.04674623324500382, 0.04674623324500382, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "38b1f928-1041-455f-98d3-fdeb84e493c0", "fitness": 0.06579640116684778, "name": "HybridOptimizer", "description": "A refined hybrid optimizer that combines adaptive differential evolution with stochastic local search, introducing dynamic parameter adaptation and selective amplification to enhance search efficiency.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.15:  # 15% chance for local search\n                    perturbation = np.random.normal(0, 0.2, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 2, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06580 with standard deviation 0.05785.", "error": "", "parent_ids": ["9689bb0e-41c5-442f-9afb-89bace3fdc19"], "operator": null, "metadata": {"aucs": [0.14113474761231293, 0.14113474761231293, 0.14113474761231293, 0.05575445588823047, 0.05575445588823047, 0.05575445588823047, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d21bca7d-c755-41dd-a967-5f7eefb8a974", "fitness": 0.06368256249839639, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer leveraging adaptive differential evolution blended with a progressive stochastic local search, incorporating a dynamic inertia control mechanism for improved convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        inertia_base = 0.9  # Initial inertia factor\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            inertia = inertia_base * (1 - eval_count / self.budget) + 0.4  # Dynamic inertia\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i]) * inertia + population[i] * (1 - inertia)\n\n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Progressive stochastic local search\n                if eval_count < self.budget and np.random.rand() < 0.2:  # 20% chance for local search\n                    perturbation = np.random.normal(0, 0.1 * inertia, self.dim)  # Scaled by inertia\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06368 with standard deviation 0.05638.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.13740112763027135, 0.13740112763027135, 0.13740112763027135, 0.0531465598649179, 0.0531465598649179, 0.0531465598649179, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c867704d-c122-45cb-8042-7f76b8bf203d", "fitness": 0.06230845078198118, "name": "HybridOptimizer", "description": "Hybrid optimizer enhancement with improved local search and dynamic population size adjustment.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.2:  # 20% chance for local search\n                    perturbation = np.random.normal(0, 0.1, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 4, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06231 with standard deviation 0.05654.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.1371438911488413, 0.1371438911488413, 0.1371438911488413, 0.049281461197102305, 0.049281461197102305, 0.049281461197102305, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c28a0f10-1fcc-495a-8fd2-147e111c4123", "fitness": 0.06270740889935689, "name": "HybridOptimizer", "description": "Enhanced hybrid optimizer with adaptive population size and stochastic local search prioritization to improve convergence efficiency and accuracy.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n\n        # Adjusted adaptive population size based on dimension\n        population_size = max(10 * self.dim, 100)  # Ensure a minimum size for diverse exploration\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n\n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                # Stochastic Local Search with enhanced prioritization\n                if eval_count < self.budget and np.random.rand() < 0.25:  # Increased chance for local search\n                    perturbation = np.random.normal(0, 0.2, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 5, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06271 with standard deviation 0.05694.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.1380867195428852, 0.1380867195428852, 0.1380867195428852, 0.04953550715518551, 0.04953550715518551, 0.04953550715518551, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "44535575-28b8-462a-8be0-08d02315aa75", "fitness": 0.0638438062124032, "name": "HybridOptimizer", "description": "Enhance mutation strategy by incorporating best solution guidance and adjust stochastic search probability for improved convergence.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            best_idx = np.argmin(fitness)  # Keep track of the best solution\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c) + F * (population[best_idx] - a), bounds[:, 0], bounds[:, 1])  # Adjust mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.10:  # Adjusted local search probability to 10%\n                    perturbation = np.random.normal(0, 0.2, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 6, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06384 with standard deviation 0.05735.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.13938452138097757, 0.13938452138097757, 0.13938452138097757, 0.05164689725623206, 0.05164689725623206, 0.05164689725623206, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "405004df-4ab6-4af9-bf91-5750e56d1a3a", "fitness": 0.06385121055174776, "name": "EnhancedHybridOptimizer", "description": "An enhanced hybrid optimizer incorporating adaptive differential evolution, stochastic local search, and a novel elitism strategy that emphasizes diversity maintenance and convergence balance to improve optimization performance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Elitism parameters\n        elite_fraction = 0.1\n        num_elites = max(1, int(elite_fraction * population_size))\n        \n        while eval_count < self.budget:\n            elites_idx = np.argsort(fitness)[:num_elites]\n            elites = population[elites_idx]\n\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.3\n                CR = CR_base + np.random.rand() * 0.2\n                \n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.15:  # 15% chance for local search\n                    perturbation = np.random.normal(0, 0.2, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n            \n            # Replace worst individuals with elites to maintain diversity\n            worst_idx = np.argsort(fitness)[-num_elites:]\n            population[worst_idx] = elites\n            fitness[worst_idx] = np.array([func(ind) for ind in elites])\n            eval_count += num_elites\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06385 with standard deviation 0.05790.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.14045822552163478, 0.14045822552163478, 0.14045822552163478, 0.05059540613360858, 0.05059540613360858, 0.05059540613360858, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "638e4d85-6b0c-4050-a793-dd51765ec402", "fitness": 0.06624732289336206, "name": "HybridOptimizer", "description": "Enhanced hybrid optimizer using adaptive local search intensity and adjusted mutation strategy for improved convergence.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5  # Base Differential weight\n        CR_base = 0.7  # Base Crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Dynamic parameter adaptation\n                F = F_base + np.random.randn() * 0.1  # Changed randomness multiplier\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with selective amplification\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Stochastic Local Search with increased chance and perturbation size\n                if eval_count < self.budget and np.random.rand() < 0.25:  # Increased chance for local search\n                    perturbation = np.random.normal(0, 0.15, self.dim)  # Adjusted perturbation size\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 8, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06625 with standard deviation 0.06341.", "error": "", "parent_ids": ["38b1f928-1041-455f-98d3-fdeb84e493c0"], "operator": null, "metadata": {"aucs": [0.15192920014257005, 0.15192920014257005, 0.15192920014257005, 0.0463127685375162, 0.0463127685375162, 0.0463127685375162, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7b6163ef-323a-4307-a645-62eb7f920dab", "fitness": 0.06665392505964285, "name": "ImprovedHybridOptimizer", "description": "Multi-strategy optimizer with enhanced dynamic parameter adaptation and adaptive local search to boost convergence and solution quality.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5\n        CR_base = 0.7\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Improved dynamic parameter adaptation\n                F = F_base + np.random.randn() * 0.1\n                CR = CR_base + np.random.rand() * 0.3  # Adjusted crossover range\n\n                # Mutation and crossover with diversity preservation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Adaptive local search with dynamic intensity\n                if eval_count < self.budget and np.random.rand() < 0.3:  # Increased local search probability\n                    perturbation_scale = 0.1 if f_trial < np.min(fitness) else 0.15  # Dynamic perturbation size\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 9, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06665 with standard deviation 0.05962.", "error": "", "parent_ids": ["638e4d85-6b0c-4050-a793-dd51765ec402"], "operator": null, "metadata": {"aucs": [0.14500832323458757, 0.14500832323458757, 0.14500832323458757, 0.054453451944341036, 0.054453451944341036, 0.054453451944341036, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7c0c36cf-c686-4b84-afb5-742bc331ac3a", "fitness": 0.06434302950266368, "name": "ImprovedHybridOptimizer", "description": "Introduced adaptive scaling in local search based on fitness trends to enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.5\n        CR_base = 0.7\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Improved dynamic parameter adaptation\n                F = F_base + np.random.randn() * 0.1\n                CR = CR_base + np.random.rand() * 0.3  # Adjusted crossover range\n\n                # Mutation and crossover with diversity preservation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Adaptive local search with dynamic intensity\n                if eval_count < self.budget and np.random.rand() < 0.3:  # Increased local search probability\n                    perturbation_scale = 0.05 if f_trial < np.mean(fitness) else 0.2  # Adaptive perturbation based on fitness trend\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 10, "feedback": "The algorithm ImprovedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06434 with standard deviation 0.05701.", "error": "", "parent_ids": ["7b6163ef-323a-4307-a645-62eb7f920dab"], "operator": null, "metadata": {"aucs": [0.1389063218712372, 0.1389063218712372, 0.1389063218712372, 0.053622766636753916, 0.053622766636753916, 0.053622766636753916, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "68d24be9-11e6-4400-aa4d-fcc958e4b2d7", "fitness": 0.06609141784815238, "name": "EnhancedHybridOptimizer", "description": "Incorporate adaptive population resizing and fitness diversity maintenance to enhance exploration-exploitation balance and avoid premature convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        base_population_size = 10 * self.dim\n        F_base = 0.5\n        CR_base = 0.7\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(base_population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = base_population_size\n\n        while eval_count < self.budget:\n            # Adaptive population resizing\n            population_size = max(4, int(base_population_size * (1 - eval_count / self.budget)))\n            \n            for i in range(population_size):\n                # Improved dynamic parameter adaptation\n                F = F_base + np.random.rand() * 0.1\n                CR = CR_base + np.random.rand() * 0.2\n\n                # Mutation and crossover with diversity preservation\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Adaptive local search with dynamic intensity\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 if f_trial < np.min(fitness) else 0.15\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n            # Maintain diversity by re-initializing part of the population\n            if eval_count < self.budget and np.random.rand() < 0.1:\n                worst_idx = np.argsort(fitness)[-population_size//5:]\n                for idx in worst_idx:\n                    population[idx] = bounds[:, 0] + np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0])\n                    fitness[idx] = func(population[idx])\n                    eval_count += 1\n                    if eval_count >= self.budget:\n                        break\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06609 with standard deviation 0.05793.", "error": "", "parent_ids": ["7b6163ef-323a-4307-a645-62eb7f920dab"], "operator": null, "metadata": {"aucs": [0.14139039786064578, 0.14139039786064578, 0.14139039786064578, 0.056383855683811435, 0.056383855683811435, 0.056383855683811435, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "69f188cf-df48-4333-91f8-80f445293e98", "fitness": 0.06945076362739229, "name": "EnhancedHybridOptimizer", "description": "Enhanced Multi-strategy optimizer with adaptive parameter tuning and dynamic local search prioritization for accelerated convergence and solution refinement.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 12 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with increased diversity\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n                \n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n                # Dynamic local search with intensified focus on potential solutions\n                if eval_count < self.budget and np.random.rand() < 0.4:  # Increased local search probability\n                    perturbation_scale = 0.08 if f_trial < np.min(fitness) else 0.12  # Dynamic perturbation size\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = trial + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06945 with standard deviation 0.06289.", "error": "", "parent_ids": ["7b6163ef-323a-4307-a645-62eb7f920dab"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05527023508362505, 0.05527023508362505, 0.05527023508362505, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7898ce2e-2739-4f13-b7d4-e2783267d5d2", "fitness": 0.0707449590767439, "name": "EnhancedHybridOptimizerV2", "description": "EnhancedHybridOptimizerV2: Integrates adaptive differential evolution with memory-based local search and opposition-based learning for robust global exploration and fast convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 13, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07074 with standard deviation 0.06263.", "error": "", "parent_ids": ["69f188cf-df48-4333-91f8-80f445293e98"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05915282143167988, 0.05915282143167988, 0.05915282143167988, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "2ad4bd5a-b5f1-4d4b-9dbd-67dc1ab8177f", "fitness": 0.0707449590767439, "name": "EnhancedHybridOptimizerV3", "description": "EnhancedHybridOptimizerV3: Introduces adaptive population size and elite retention strategy for improved exploration and convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            # Adaptive population size\n            if eval_count % (self.budget // 10) == 0 and population_size > 5:\n                population_size = max(5, population_size // 2)\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n            \n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Elite retention strategy for best solution\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedHybridOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07074 with standard deviation 0.06263.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05915282143167988, 0.05915282143167988, 0.05915282143167988, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "85f4391a-0316-4997-ac8e-43bbf64e258a", "fitness": 0.06873855817898437, "name": "EnhancedHybridOptimizerV3", "description": "An advanced adaptive hybrid optimizer incorporating a memory-based search strategy with adaptive mutation and crossover rates for enhanced global exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = [np.inf] * 5\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                adapt_factor = (1 - eval_count / self.budget) ** 0.5\n                F = F_base + np.random.uniform(-0.1, 0.3) * adapt_factor\n                CR = CR_base + np.random.uniform(-0.2, 0.2) * adapt_factor\n\n                # Mutation and crossover with enhanced memory-based local search\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem.remove(max(best_mem))\n\n                # Memory-based local search with adaptive perturbation\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5) * adapt_factor\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if f_new_trial < max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem.remove(max(best_mem))\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedHybridOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06874 with standard deviation 0.06306.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05313361873840128, 0.05313361873840128, 0.05313361873840128, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "691f6ac4-fe47-47d5-8583-b1f31d8a4ddc", "fitness": 0.0707449590767439, "name": "EnhancedHybridOptimizerV3", "description": "Implements adaptive population size and a restart mechanism to escape local optima and enhance exploration.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV3:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            # Adaptive population size\n            if eval_count % 100 == 0 and len(best_mem) > 0: \n                population_size = max(10, int(population_size * 0.9))\n                population = population[:population_size]\n                fitness = fitness[:population_size]\n\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n            \n            # Restart mechanism to escape local optima\n            if eval_count < self.budget and eval_count % 200 == 0 and np.min(fitness) > np.min(best_mem):\n                population = np.random.rand(population_size, self.dim)\n                population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n                fitness = np.array([func(ind) for ind in population])\n                eval_count += population_size\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridOptimizerV3 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07074 with standard deviation 0.06263.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05915282143167988, 0.05915282143167988, 0.05915282143167988, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "ace9da3b-0900-4ea4-a916-a1c78727a03c", "fitness": 0.06904688217209483, "name": "EnhancedHybridOptimizerV2", "description": "Integrates adaptive differential evolution with elite local search and strategy diversification for improved global exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Elite local search strategy\n                if eval_count < self.budget and np.random.rand() < 0.7:\n                    perturbation_scale = 0.03 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06905 with standard deviation 0.06299.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05405859071773267, 0.05405859071773267, 0.05405859071773267, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "1829c2d8-5dbd-4c1a-b681-bd5974fc9447", "fitness": 0.07144133915579487, "name": "EnhancedHybridOptimizerV2", "description": "Introduces a restart mechanism to escape local optima, enhancing exploration without sacrificing convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizerV2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Forced restart mechanism\n                if eval_count < self.budget and np.random.rand() < 0.01:\n                    population[i] = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    fitness[i] = func(population[i])\n                    eval_count += 1\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.5:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedHybridOptimizerV2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07144 with standard deviation 0.06260.", "error": "", "parent_ids": ["7898ce2e-2739-4f13-b7d4-e2783267d5d2"], "operator": null, "metadata": {"aucs": [0.1527787938461973, 0.1527787938461973, 0.1527787938461973, 0.06104522362118736, 0.06104522362118736, 0.06104522362118736, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "5ddb25ef-6915-45de-822d-dc0abe6f7a6d", "fitness": 0.06749312098570863, "name": "AdaptiveMultiPopOptimizer", "description": "Implements adaptive learning rate adjustment and multi-population strategy to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass AdaptiveMultiPopOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        num_populations = 3\n        population_size = 10 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        max_eval_per_pop = self.budget // num_populations\n\n        # Create multiple populations\n        populations = [np.random.rand(population_size, self.dim) for _ in range(num_populations)]\n        populations = [bounds[:, 0] + pop * (bounds[:, 1] - bounds[:, 0]) for pop in populations]\n        fitness = [np.array([func(ind) for ind in pop]) for pop in populations]\n        eval_count = population_size * num_populations\n\n        # Initialize adaptive learning rate\n        learning_rate = 0.1\n\n        while eval_count < self.budget:\n            for p in range(num_populations):\n                for i in range(population_size):\n                    # Adaptive F and CR based on performance\n                    F = F_base + np.random.uniform(-0.1, 0.3) * (1 - fitness[p][i] / np.max(fitness[p]))\n                    CR = CR_base + np.random.uniform(-0.2, 0.2) * (1 - fitness[p][i] / np.max(fitness[p]))\n\n                    # Mutation and crossover\n                    indices = np.random.choice(population_size, 3, replace=False)\n                    a, b, c = populations[p][indices]\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n\n                    trial = np.where(cross_points, mutant, populations[p][i])\n                    \n                    # Evaluate new candidate\n                    f_trial = func(trial)\n                    eval_count += 1\n\n                    # Selection with adaptive learning\n                    if f_trial < fitness[p][i]:\n                        populations[p][i] = trial\n                        fitness[p][i] = f_trial\n                        learning_rate = max(0.01, learning_rate * 0.9)\n                    else:\n                        learning_rate = min(0.2, learning_rate * 1.1)\n\n                # Inter-population migration\n                if p < num_populations - 1 and eval_count < self.budget:\n                    top_individuals = np.argsort(fitness[p])[:population_size // 5]\n                    for idx in top_individuals:\n                        target_pop = populations[p+1]\n                        target_fitness = fitness[p+1]\n                        if eval_count < self.budget:\n                            target_pop[np.argmax(target_fitness)] = populations[p][idx]\n                            target_fitness[np.argmax(target_fitness)] = fitness[p][idx]\n                            eval_count += 1\n\n        # Return the best solution found across all populations\n        best_idx, best_fit = None, float('inf')\n        for p in range(num_populations):\n            idx = np.argmin(fitness[p])\n            if fitness[p][idx] < best_fit:\n                best_fit = fitness[p][idx]\n                best_idx = (p, idx)\n        return populations[best_idx[0]][best_idx[1]]", "configspace": "", "generation": 19, "feedback": "The algorithm AdaptiveMultiPopOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06749 with standard deviation 0.06339.", "error": "", "parent_ids": ["1829c2d8-5dbd-4c1a-b681-bd5974fc9447"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.049397307158574044, 0.049397307158574044, 0.049397307158574044, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "77a7ac18-58b1-48e4-8165-ddc492571bf2", "fitness": 0.06876083664906285, "name": "RefinedEnhancedHybridOptimizer", "description": "Combines dynamic parameter adjustment with adaptive elitism and stochastic restarts to enhance convergence and exploration in complex landscapes.", "code": "import numpy as np\n\nclass RefinedEnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.5\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n\n        while eval_count < self.budget:\n            for i in range(population_size):\n                # Enhanced dynamic parameter adaptation\n                F = F_base + np.random.uniform(-0.1, 0.2)\n                CR = CR_base + np.random.uniform(-0.15, 0.15)\n\n                # Mutation and crossover with opposition-based learning\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                oppo_mutant = bounds[:, 0] + bounds[:, 1] - mutant\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                oppo_trial = np.where(cross_points, oppo_mutant, population[i])\n                \n                # Evaluate new candidates\n                f_trial = func(trial)\n                f_oppo_trial = func(oppo_trial)\n                eval_count += 2\n                \n                # Select the better trial\n                if f_oppo_trial < f_trial:\n                    trial = oppo_trial\n                    f_trial = f_oppo_trial\n\n                # Selection with fitness improvement check\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n\n                # Stochastic restart mechanism\n                if eval_count < self.budget and np.random.rand() < 0.02:\n                    population[i] = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    fitness[i] = func(population[i])\n                    eval_count += 1\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.4:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 20, "feedback": "The algorithm RefinedEnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06876 with standard deviation 0.06305.", "error": "", "parent_ids": ["1829c2d8-5dbd-4c1a-b681-bd5974fc9447"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05320045414863672, 0.05320045414863672, 0.05320045414863672, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "435b66f6-e0af-4f79-ab91-ec15d5c84271", "fitness": -Infinity, "name": "AdaptiveMemoryHybridOptimizer", "description": "Introduces adaptive memory and self-adaptive parameter tuning for dynamic exploration-exploitation balance, enhancing convergence robustness.", "code": "import numpy as np\n\nclass AdaptiveMemoryHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        F_adapt = 0.6\n        CR_adapt = 0.8\n\n        while eval_count < self.budget:\n            sorted_indices = np.argsort(fitness)\n            top_25_percent = sorted_indices[:population_size // 4]\n            \n            for i in range(population_size):\n                # Self-adaptive parameter tuning\n                F = F_adapt + np.random.uniform(-0.1, 0.1)\n                CR = CR_adapt + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover with memory-based enhancement\n                indices = np.random.choice(top_25_percent, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate new candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                \n                # Adaptive memory update\n                if eval_count < self.budget and np.random.rand() < 0.1:\n                    memory_candidate = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                    f_memory_candidate = func(memory_candidate)\n                    eval_count += 1\n                    if f_memory_candidate < np.max(best_mem):\n                        best_mem.append(f_memory_candidate)\n                        best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 21, "feedback": "An exception occurred: ValueError('zero-size array to reduction operation maximum which has no identity').", "error": "ValueError('zero-size array to reduction operation maximum which has no identity')", "parent_ids": ["1829c2d8-5dbd-4c1a-b681-bd5974fc9447"], "operator": null, "metadata": {}}
{"id": "25c3e898-9a8d-4a6e-b706-3e37d1cccbd8", "fitness": 0.08564999146632939, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration by implementing adaptive mutation strategies using successful solutions and dynamic population resizing based on diversity.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 22, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08565 with standard deviation 0.07435.", "error": "", "parent_ids": ["1829c2d8-5dbd-4c1a-b681-bd5974fc9447"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.19894280552721244, 0.05633047513704825, 0.05633047513704825, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "cff72225-cc7c-44c2-8f1c-44e63d61d2e0", "fitness": 0.0636042531147543, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduce adaptive learning rates and a feedback mechanism from past successes to control exploration and exploitation balance for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 10 * self.dim\n        F_base = 0.7\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.8 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.2 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.2), 0.3, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.2, 0.4)\n\n                CR = CR_base + np.random.uniform(-0.3, 0.3)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search with feedback mechanism\n                if eval_count < self.budget and np.random.rand() < 0.2:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06360 with standard deviation 0.05785.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.1402423018619351, 0.1402423018619351, 0.1402423018619351, 0.05007045748232786, 0.05007045748232786, 0.05007045748232786, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d664ae18-a6ec-4bcf-86b2-d04f364b59c0", "fitness": 0.06980417697853336, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduce dynamic learning rates and adaptive restart mechanisms to enhance global search and escape local optima.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n        dynamic_restart_threshold = 50  # Threshold for restart based on memory diversity\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05: \n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n                # Dynamic restart if memory diversity is low\n                if len(best_mem) >= dynamic_restart_threshold and np.std(best_mem) < 0.001:\n                    population = np.random.rand(population_size, self.dim)\n                    population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n                    fitness = np.array([func(ind) for ind in population])\n                    eval_count += population_size\n                    best_mem.clear()\n                    success_mem.clear()\n                    continue\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < dynamic_restart_threshold or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:dynamic_restart_threshold]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < dynamic_restart_threshold or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:dynamic_restart_threshold]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 24, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06980 with standard deviation 0.06281.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05633047513704825, 0.05633047513704825, 0.05633047513704825, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "14e5fc16-7519-4708-9b5b-dafc8e5f2b45", "fitness": 0.06826055959742865, "name": "AdvancedHybridOptimizer", "description": "Integrates a multi-strategy approach combining adaptive mutation, diversity preservation, and elite learning to enhance convergence and exploration in black box optimization.", "code": "import numpy as np\n\nclass AdvancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n\n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and success history\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = 0.6 + np.random.uniform(-0.1, 0.3)\n\n                CR = 0.8 + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search with elite learning\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n            # Elite learning phase based on best memory\n            if best_mem and eval_count < self.budget:\n                elite_candidate = np.random.choice(best_mem)\n                perturbation = np.random.normal(0, 0.02, self.dim)\n                elite_trial = elite_candidate + perturbation\n                elite_trial = np.clip(elite_trial, bounds[:, 0], bounds[:, 1])\n                f_elite_trial = func(elite_trial)\n                eval_count += 1\n                if f_elite_trial < np.min(best_mem):\n                    best_mem.append(f_elite_trial)\n                    best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 25, "feedback": "The algorithm AdvancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06826 with standard deviation 0.06318.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.051699622993734096, 0.051699622993734096, 0.051699622993734096, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "70b3a601-662c-40f5-8bb4-e60f1efc7df3", "fitness": 0.07776974500509326, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduces a diversity-driven dynamic scaling mechanism and strategic exploration using best-so-far solutions to enhance convergence and robustness.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 20 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            diversity = np.mean(np.std(population, axis=0))\n            if eval_count % (2 * population_size) == 0:\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.8 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.2 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.5, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3 and best_mem:\n                    best_candidate = population[np.argmin(fitness)]\n                    perturbation_scale = 0.02 * np.std(population, axis=0)\n                    new_trial = best_candidate + np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < np.min(fitness):\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07777 with standard deviation 0.06211.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.08022717921672795, 0.08022717921672795, 0.08022717921672795, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "e05eb570-2342-41cf-8544-2b3345624d63", "fitness": 0.06980417697853336, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration and exploitation by incorporating adaptive learning from the best solutions and adjusting population size based on recent improvements.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n        recent_improvements = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                improvement_rate = np.mean(recent_improvements[-5:]) if recent_improvements else 0.1\n                if diversity < 0.05 or improvement_rate < 0.01:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n                \n                recent_improvements.clear()\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    recent_improvements.append(fitness[i] - f_trial)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 27, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06980 with standard deviation 0.06281.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05633047513704825, 0.05633047513704825, 0.05633047513704825, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "209a280a-7f16-4309-9aaf-ddd3c2689680", "fitness": 0.06851664934842087, "name": "AdaptiveHybridOptimizer", "description": "Augments local search by integrating a covariance-based perturbation for enhanced convergence in diverse landscapes.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search with covariance perturbation\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    cov = np.cov(population.T)\n                    perturbation = np.random.multivariate_normal(np.zeros(self.dim), cov)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 28, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06852 with standard deviation 0.06312.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05246789224671078, 0.05246789224671078, 0.05246789224671078, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "63979492-21ff-4dce-840c-b4ffb2b913f9", "fitness": 0.06885884265954441, "name": "AdaptiveHybridOptimizer", "description": "Utilize memory-based local search more effectively by refining perturbation conditions.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.1:  # Reduced the probability from 0.3 to 0.1\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 29, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06886 with standard deviation 0.06303.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05349447218008141, 0.05349447218008141, 0.05349447218008141, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f6407410-a7d9-40a8-9ba8-b6ac98c0a216", "fitness": 0.06741755054526644, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration and exploitation by introducing a tournament selection mechanism and refining memory-based local search strategies.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 4, replace=False)\n                a, b, c, d = population[indices]\n                if func(b) < func(c):  # Tournament selection\n                    c = b\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.03 * np.random.uniform(0.5, 1.5)  # Refined local search\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 30, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06742 with standard deviation 0.06341.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.04917059583724748, 0.04917059583724748, 0.04917059583724748, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "23a95230-97bf-472a-8c20-a7026007e8e2", "fitness": 0.0699254376017376, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration by implementing adaptive mutation strategies using successful solutions and dynamic population resizing based on diversity, with improved local search exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Improved memory-based local search exploration\n                if eval_count < self.budget and np.random.rand() < 0.35:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 31, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06993 with standard deviation 0.06279.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.056694257006660975, 0.056694257006660975, 0.056694257006660975, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "bd328d1d-1ee8-4af0-aa39-b8ceba0048e7", "fitness": 0.06980417697853336, "name": "AdaptiveHybridOptimizer", "description": "Introduces a memory-based strategy for dynamic mutation scale adjustment to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n            \n            # Dynamic mutation scale adjustment\n            if best_mem:\n                F_base = max(0.4, np.mean(best_mem) / np.min(best_mem) * 0.5)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 32, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06980 with standard deviation 0.06281.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05633047513704825, 0.05633047513704825, 0.05633047513704825, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "af9fb5a1-0bc8-4946-8874-e1ece281afba", "fitness": 0.06799613809060419, "name": "EnhancedAdaptiveOptimizer", "description": "Incorporates dynamic learning rate scaling and elite opposition-based learning to enhance diversity and convergence speed in adaptive mutation strategies.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection with dynamic learning rate scaling\n                F = F_base + np.random.uniform(-0.1, 0.3)\n                F *= 1 + np.random.normal(0, 0.1) * (np.std(fitness) / (np.mean(fitness) + 1e-9))\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Elite opposition-based learning\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    opp_trial = bounds[:, 1] + bounds[:, 0] - trial\n                    opp_trial = np.clip(opp_trial, bounds[:, 0], bounds[:, 1])\n                    f_opp_trial = func(opp_trial)\n                    eval_count += 1\n                    if f_opp_trial < f_trial:\n                        population[i] = opp_trial\n                        fitness[i] = f_opp_trial\n                        if len(best_mem) < 5 or f_opp_trial < np.max(best_mem):\n                            best_mem.append(f_opp_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06800 with standard deviation 0.06325.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05090635847326075, 0.05090635847326075, 0.05090635847326075, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "21947b77-9b3c-4074-9f4c-41ca3bdb8e13", "fitness": 0.0711748452508502, "name": "AdaptiveHybridOptimizer", "description": "Introduces a stochastic adaptive memory-based search strategy with diversity boosting.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n            # Diversity boosting\n            if np.random.rand() < 0.1:\n                diversity_perturbation = np.random.normal(0, 0.01, (population_size, self.dim))\n                population += diversity_perturbation\n                population = np.clip(population, bounds[:, 0], bounds[:, 1])\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 34, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07117 with standard deviation 0.06255.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.06044247995399876, 0.06044247995399876, 0.06044247995399876, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "3f8d2ddf-d457-4a66-8387-7fb27ebea65a", "fitness": 0.06857922778119778, "name": "AdaptiveHybridOptimizer", "description": "Enhances exploration and exploitation by introducing adaptive learning rates for mutation and leveraging historical success to guide search.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.15, 0.25)  # Adjusted range for CR\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c + np.random.normal(0, 0.1, self.dim)), bounds[:, 0], bounds[:, 1])  # Added noise\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 35, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06858 with standard deviation 0.06310.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.052655627545041495, 0.052655627545041495, 0.052655627545041495, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "080dd883-2852-4639-94bf-47ce410a4e55", "fitness": -Infinity, "name": "EnhancedAdaptiveOptimizer", "description": "Improve convergence by incorporating adaptive learning rates and a rotationally invariant local search to enhance diversity and fine-tuning.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.2, 0.2)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    perturbation = np.dot(func.rotation, perturbation)  # Applying rotation for invariance\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 36, "feedback": "An exception occurred: AttributeError(\"'ioh.iohcpp.problem.Katsuura' object has no attribute 'rotation'\").", "error": "AttributeError(\"'ioh.iohcpp.problem.Katsuura' object has no attribute 'rotation'\")", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {}}
{"id": "7e64d60e-dc93-4191-82c7-87b3a3cf2032", "fitness": -Infinity, "name": "MultiStrategyAdaptiveOptimizer", "description": "Introduce multi-strategy adaptation by combining diverse mutation tactics and adaptive parameter tuning for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass MultiStrategyAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 20 * self.dim\n        F_base = 0.7\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n\n        while eval_count < self.budget:\n            if eval_count % (3 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.85 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.15 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.1), 0.5, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.3, 0.3)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n\n                if np.random.rand() < 0.5:\n                    mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(a + F * (best_mem[np.random.randint(len(best_mem))] - a) + F * (b - c), bounds[:, 0], bounds[:, 1])\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 37, "feedback": "An exception occurred: ValueError('high <= 0').", "error": "ValueError('high <= 0')", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {}}
{"id": "3fd234d0-74f3-4d77-a7f6-cc198b1f9f78", "fitness": 0.10099214978279297, "name": "AdaptiveHybridOptimizer", "description": "Incorporates a success-based parameter adaptation and an elitist archive mechanism to enhance convergence and solution quality.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0) # Decreased F variation\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1) # Reduced CR variation\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 38, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10099 with standard deviation 0.13595.", "error": "", "parent_ids": ["25c3e898-9a8d-4a6e-b706-3e37d1cccbd8"], "operator": null, "metadata": {"aucs": [0.4516802105252712, 0.15258205579855189, 0.15258205579855189, 0.050195008640920635, 0.050195008640920635, 0.050195008640920635, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f4b0d28c-e0a2-4fe1-b456-6bacf6335ea8", "fitness": -Infinity, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduces dynamic scaling of parameters based on diversity and a memory-based perturbation mechanism to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            diversity = np.mean(np.std(population, axis=0))\n            if diversity < 0.05:\n                population_size = max(self.dim, int(0.9 * population_size))\n            else:\n                population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * (1.0 - diversity) * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 39, "feedback": "An exception occurred: IndexError('index 306 is out of bounds for axis 0 with size 300').", "error": "IndexError('index 306 is out of bounds for axis 0 with size 300')", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {}}
{"id": "ec9ec25b-84d2-4fba-a239-6dae1aec6ad9", "fitness": 0.08286843967121499, "name": "EnhancedAdaptiveOptimizer", "description": "Introduces a multilevel adaptive mechanism with feedback-driven diversity control to enhance robustness and convergence speed.", "code": "import numpy as np\n\nclass EnhancedAdaptiveOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        population_size = 20 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        best_mem = []\n        success_mem = []\n        \n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.85 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.15 * population_size))\n\n                # Refresh archive when diversity is low\n                if diversity < 0.02 and len(archive) > 0:\n                    random_archive_idx = np.random.choice(len(archive), size=5, replace=False)\n                    for idx in random_archive_idx:\n                        population[np.random.randint(0, population_size)] = archive[idx]\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.3, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10:\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n                        archive.append(trial)\n\n                if eval_count < self.budget and np.random.rand() < 0.2:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 2.0)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n                            archive.append(new_trial)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedAdaptiveOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08287 with standard deviation 0.09032.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.28397377260426404, 0.15258205579855189, 0.15258205579855189, 0.05172602427985573, 0.05172602427985573, 0.05172602427985573, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "400d270c-5bdc-4027-b82c-fbe8cb44e675", "fitness": 0.06939827396541627, "name": "AdaptiveHybridOptimizer", "description": "Enhanced solution diversity by incorporating a novel random restart mechanism for global exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n                \n            # Random restart mechanism\n            if eval_count < self.budget and np.random.rand() < 0.01:\n                population = np.random.rand(population_size, self.dim)\n                population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n                fitness = np.array([func(ind) for ind in population])\n                eval_count += population_size\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 41, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06940 with standard deviation 0.06290.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.055112766097697, 0.055112766097697, 0.055112766097697, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "cd3d8e18-299f-4fab-87eb-60f42ccdb40e", "fitness": 0.0688309559271375, "name": "EnhancedAdaptiveHybridOptimizer", "description": "Introduces adaptive memory-based mutation and dynamic population scaling to enhance exploration and convergence efficiency.", "code": "import numpy as np\n\nclass EnhancedAdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions and success\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.85 * population_size))\n                else:\n                    population_size = min(25 * self.dim, int(1.15 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10]\n                        archive.append(trial)\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.25:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10]\n                            archive.append(new_trial)\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedAdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06883 with standard deviation 0.06304.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.053410811982860684, 0.053410811982860684, 0.053410811982860684, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "b89a6513-ed38-4be5-ae06-2e3d84cb4625", "fitness": 0.06775902147982416, "name": "AdaptiveHybridOptimizer", "description": "Introduced successful trial probability adjustment for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0) # Decreased F variation\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1) # Reduced CR variation\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and (np.random.rand() < 0.3 or len(success_mem) / eval_count > 0.1): # Adjusted success probability\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 43, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06776 with standard deviation 0.06332.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.050195008640920635, 0.050195008640920635, 0.050195008640920635, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "5195bbc5-be49-46bf-a446-802693feb38d", "fitness": 0.0680443682860985, "name": "EnhancedHybridOptimizer", "description": "Enhances solution diversity and convergence speed by incorporating adaptive parameter tuning and stochastic ranking within a dynamic archive-based hybrid optimizer.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters\n        population_size = 15 * self.dim\n        F_base = 0.5\n        CR_base = 0.9\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial) # Add to archive\n\n            # Adaptive archive influence on population\n            if len(archive) > 5:\n                archive.sort(key=lambda ind: func(ind))\n                best_archive = archive[:5]\n                for idx in range(min(5, len(best_archive))):\n                    if func(best_archive[idx]) < fitness[idx]:\n                        population[idx] = best_archive[idx]\n                        fitness[idx] = func(best_archive[idx])\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06804 with standard deviation 0.06324.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05105104905974367, 0.05105104905974367, 0.05105104905974367, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "50e1bb11-2913-4fac-9989-d68a52069f88", "fitness": 0.06806923715666253, "name": "EnhancedHybridOptimizer", "description": "Integrates adaptive differential evolution with a memory-based perturbation strategy and dynamic population resizing for enhanced convergence and diversity maintenance.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            if eval_count % (2 * population_size) == 0:  # Dynamic resizing\n                diversity = np.mean(np.std(population, axis=0))\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                # Adaptive parameter selection based on success history\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0) # Decreased F variation\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1) # Reduced CR variation\n\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                mutant = np.clip(a + F * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate the trial candidate\n                f_trial = func(trial)\n                eval_count += 1\n\n                # Selection and success memory update\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 10: # Increased memory size for success rate\n                        success_mem.pop(0)\n                    if len(best_mem) < 10 or f_trial < np.max(best_mem): # Expanded best memory\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:10] # Keep top 10\n                        archive.append(trial) # Add to archive\n\n                # Memory-based local search with improved perturbations\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.1 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 10 or f_new_trial < np.max(best_mem): # Expanded best memory\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:10] # Keep top 10\n                            archive.append(new_trial) # Add to archive\n\n        # Return the best solution found\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06807 with standard deviation 0.06323.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.05112565567143579, 0.05112565567143579, 0.05112565567143579, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "3a91d9b0-9dc0-4175-ade3-db095381bf29", "fitness": 0.07297973375571772, "name": "AdaptiveHybridOptimizer", "description": "Introduces a dynamic scaling of mutation rates based on population diversity to enhance exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        np.random.seed(42)\n        \n        # Initialize parameters for Differential Evolution\n        population_size = 15 * self.dim\n        F_base = 0.6\n        CR_base = 0.8\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(population_size, self.dim)\n        population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = population_size\n\n        # Memory for best solutions\n        best_mem = []\n        success_mem = []\n        \n        # Initialize elitist archive\n        archive = []\n\n        while eval_count < self.budget:\n            diversity = np.mean(np.std(population, axis=0))  # Modify: Calculate population diversity dynamically\n            if eval_count % (2 * population_size) == 0:  # Modify: Adjust mutation scaling based on diversity\n                if diversity < 0.05:\n                    population_size = max(self.dim, int(0.9 * population_size))\n                else:\n                    population_size = min(20 * self.dim, int(1.1 * population_size))\n\n            for i in range(population_size):\n                if success_mem:\n                    F = np.mean(success_mem)\n                    F = np.clip(F + np.random.normal(0, 0.05), 0.4, 1.0)\n                else:\n                    F = F_base + np.random.uniform(-0.1, 0.3)\n\n                CR = CR_base + np.random.uniform(-0.1, 0.1)\n\n                indices = np.random.choice(population_size, 3, replace=False)\n                a, b, c = population[indices]\n                F_scaled = F * (1 + diversity)  # Modify: Scale F with diversity\n                mutant = np.clip(a + F_scaled * (b - c), bounds[:, 0], bounds[:, 1])  # Modify: Apply scaled F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    success_mem.append(F)\n                    if len(success_mem) > 5:\n                        success_mem.pop(0)\n                    if len(best_mem) < 5 or f_trial < np.max(best_mem):\n                        best_mem.append(f_trial)\n                        best_mem = sorted(best_mem)[:5]\n                        archive.append(trial)\n\n                if eval_count < self.budget and np.random.rand() < 0.3:\n                    perturbation_scale = 0.05 * np.random.uniform(0.5, 1.5)\n                    perturbation = np.random.normal(0, perturbation_scale, self.dim)\n                    new_trial = population[i] + perturbation\n                    new_trial = np.clip(new_trial, bounds[:, 0], bounds[:, 1])\n                    f_new_trial = func(new_trial)\n                    eval_count += 1\n                    if f_new_trial < f_trial:\n                        population[i] = new_trial\n                        fitness[i] = f_new_trial\n                        if len(best_mem) < 5 or f_new_trial < np.max(best_mem):\n                            best_mem.append(f_new_trial)\n                            best_mem = sorted(best_mem)[:5]\n                            archive.append(new_trial)\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "configspace": "", "generation": 46, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07298 with standard deviation 0.06229.", "error": "", "parent_ids": ["3fd234d0-74f3-4d77-a7f6-cc198b1f9f78"], "operator": null, "metadata": {"aucs": [0.15258205579855189, 0.15258205579855189, 0.15258205579855189, 0.06585714546860133, 0.06585714546860133, 0.06585714546860133, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
