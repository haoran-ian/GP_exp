{"id": "f3c3acc0-7c91-4824-9649-98039cb37d21", "fitness": 0.16368901301543304, "name": "HybridPSOSA", "description": "A hybrid Particle Swarm Optimization and Simulated Annealing algorithm that balances exploration and exploitation for efficient black-box optimization.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.5  # inertia\n        self.c1 = 2.0  # cognitive (particle)\n        self.c2 = 2.0  # social (swarm)\n        self.temperature = 1000\n        self.cooling_rate = 0.995\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n            \n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Simulated Annealing component\n                candidate_solution = x[i] + np.random.normal(0, 1, self.dim)\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < cost or np.random.rand() < np.exp((cost - candidate_cost) / self.temperature):\n                    x[i] = candidate_solution\n                    if candidate_cost < personal_best_cost[i]:\n                        personal_best_cost[i] = candidate_cost\n                        personal_best[i] = candidate_solution.copy()\n                    if candidate_cost < self.best_cost:\n                        self.best_cost = candidate_cost\n                        self.best_solution = candidate_solution.copy()\n\n            self.temperature *= self.cooling_rate", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16369 with standard deviation 0.02563.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13115605247008755, 0.16611683961067536, 0.1937941469655362]}}
{"id": "6d67d39b-956d-437c-b44d-a5bb403e81e4", "fitness": 0.1592659776309561, "name": "HybridPSOSA", "description": "Enhanced HybridPSOSA by integrating adaptive velocity and mutation strategy for improved convergence in black-box optimization.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w_max = 0.9  # increased inertia for exploration\n        self.w_min = 0.4  # decreased inertia for exploitation\n        self.c1 = 2.0  # cognitive (particle)\n        self.c2 = 2.0  # social (swarm)\n        self.temperature = 1000\n        self.cooling_rate = 0.995\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n            \n            w = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)  # adaptive inertia weight\n\n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (w * v[i] \n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Simulated Annealing component\n                candidate_solution = x[i] + np.random.normal(0, 1, self.dim) * (self.temperature / 1000) # mutation scale\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < cost or np.random.rand() < np.exp((cost - candidate_cost) / self.temperature):\n                    x[i] = candidate_solution\n                    if candidate_cost < personal_best_cost[i]:\n                        personal_best_cost[i] = candidate_cost\n                        personal_best[i] = candidate_solution.copy()\n                    if candidate_cost < self.best_cost:\n                        self.best_cost = candidate_cost\n                        self.best_solution = candidate_solution.copy()\n\n            self.temperature *= self.cooling_rate", "configspace": "", "generation": 1, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15927 with standard deviation 0.02384.", "error": "", "parent_ids": ["f3c3acc0-7c91-4824-9649-98039cb37d21"], "operator": null, "metadata": {"aucs": [0.12918863146688242, 0.16111622917057655, 0.18749307225540934]}}
{"id": "5331a299-3cd8-4ee5-a177-538de4c9c24a", "fitness": 0.09534781472710208, "name": "EnhancedHybridPSOSA", "description": "Introducing Adaptive Inertia and Multi-Swarm Strategy to enhance the exploration-exploitation balance in HybridPSOSA for better convergence in black-box optimization.", "code": "import numpy as np\n\nclass EnhancedHybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.initial_w = 0.9\n        self.final_w = 0.4\n        self.c1 = 2.0\n        self.c2 = 2.0\n        self.temperature = 1000\n        self.cooling_rate = 0.995\n        self.best_cost = float('inf')\n        self.best_solution = None\n        self.swarm_division = 5  # Number of sub-swarms\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.full(self.particles, float('inf'))\n        evaluations = 0\n        \n        iteration = 0\n        while evaluations < self.budget:\n            w = (self.initial_w - self.final_w) * (self.budget - evaluations) / self.budget + self.final_w\n            \n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n            \n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                p_best_index = (i // (self.particles // self.swarm_division)) * (self.particles // self.swarm_division)\n                v[i] = (w * v[i] \n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (personal_best[p_best_index] - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                candidate_solution = x[i] + np.random.normal(0, 1, self.dim)\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < cost or np.random.rand() < np.exp((cost - candidate_cost) / self.temperature):\n                    x[i] = candidate_solution\n                    if candidate_cost < personal_best_cost[i]:\n                        personal_best_cost[i] = candidate_cost\n                        personal_best[i] = candidate_solution.copy()\n                    if candidate_cost < self.best_cost:\n                        self.best_cost = candidate_cost\n                        self.best_solution = candidate_solution.copy()\n\n            self.temperature *= self.cooling_rate\n            iteration += 1", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedHybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09535 with standard deviation 0.00659.", "error": "", "parent_ids": ["f3c3acc0-7c91-4824-9649-98039cb37d21"], "operator": null, "metadata": {"aucs": [0.0876864259515222, 0.09457608980980559, 0.10378092841997844]}}
{"id": "3b528a25-38c0-409b-9d59-60489546bec5", "fitness": 0.14674770555584674, "name": "HybridPSOSA", "description": "Enhanced HybridPSOSA by adjusting inertia dynamically for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.5  # inertia\n        self.c1 = 2.0  # cognitive (particle)\n        self.c2 = 2.0  # social (swarm)\n        self.temperature = 1000\n        self.cooling_rate = 0.995\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n            \n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                self.w = 0.5 + 0.4 * (1 - evaluations / self.budget)  # Dynamically adjusted inertia\n                v[i] = (self.w * v[i] \n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Simulated Annealing component\n                candidate_solution = x[i] + np.random.normal(0, 1, self.dim)\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < cost or np.random.rand() < np.exp((cost - candidate_cost) / self.temperature):\n                    x[i] = candidate_solution\n                    if candidate_cost < personal_best_cost[i]:\n                        personal_best_cost[i] = candidate_cost\n                        personal_best[i] = candidate_solution.copy()\n                    if candidate_cost < self.best_cost:\n                        self.best_cost = candidate_cost\n                        self.best_solution = candidate_solution.copy()\n\n            self.temperature *= self.cooling_rate", "configspace": "", "generation": 3, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14675 with standard deviation 0.01640.", "error": "", "parent_ids": ["f3c3acc0-7c91-4824-9649-98039cb37d21"], "operator": null, "metadata": {"aucs": [0.12362267889336642, 0.1598923555347923, 0.1567280822393815]}}
{"id": "9180e5c1-9763-4988-ac4b-6a9f974ba514", "fitness": 0.14920588163722584, "name": "HybridPSOSA", "description": "Enhanced hybrid PSO and SA algorithm with adaptive inertia weight for better convergence.", "code": "import numpy as np\n\nclass HybridPSOSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.5  # inertia\n        self.c1 = 2.0  # cognitive (particle)\n        self.c2 = 2.0  # social (swarm)\n        self.temperature = 1000\n        self.cooling_rate = 0.995\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            # Adaptive inertia weight update\n            self.w = 0.9 - (0.5 / self.budget) * evaluations\n\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n            \n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Simulated Annealing component\n                candidate_solution = x[i] + np.random.normal(0, 1, self.dim)\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < cost or np.random.rand() < np.exp((cost - candidate_cost) / self.temperature):\n                    x[i] = candidate_solution\n                    if candidate_cost < personal_best_cost[i]:\n                        personal_best_cost[i] = candidate_cost\n                        personal_best[i] = candidate_solution.copy()\n                    if candidate_cost < self.best_cost:\n                        self.best_cost = candidate_cost\n                        self.best_solution = candidate_solution.copy()\n\n            self.temperature *= self.cooling_rate", "configspace": "", "generation": 4, "feedback": "The algorithm HybridPSOSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14921 with standard deviation 0.02043.", "error": "", "parent_ids": ["f3c3acc0-7c91-4824-9649-98039cb37d21"], "operator": null, "metadata": {"aucs": [0.12148902538720929, 0.15600429309167474, 0.17012432643279352]}}
{"id": "3ef32a00-81fd-4dc5-b130-b860b864793d", "fitness": 0.1710413110506548, "name": "HybridPSOGWO", "description": "HybridPSOGWO - A hybrid Particle Swarm Optimization and Grey Wolf Optimizer that enhances convergence by leveraging swarm intelligence and social hierarchies.", "code": "import numpy as np\n\nclass HybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.5  # inertia\n        self.c1 = 2.0  # cognitive (particle)\n        self.c2 = 2.0  # social (swarm)\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()", "configspace": "", "generation": 5, "feedback": "The algorithm HybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17104 with standard deviation 0.02282.", "error": "", "parent_ids": ["f3c3acc0-7c91-4824-9649-98039cb37d21"], "operator": null, "metadata": {"aucs": [0.13883432986520639, 0.18534760675396778, 0.1889419965327902]}}
{"id": "ea4b8302-b9a4-4ace-9bec-37da525b5027", "fitness": 0.170274286204119, "name": "HybridPSOGWOv2", "description": "HybridPSOGWOv2 - An enhanced hybrid Particle Swarm Optimization and Grey Wolf Optimizer incorporating adaptive inertia and dynamic social-cognitive coefficients to boost convergence and diversity.", "code": "import numpy as np\n\nclass HybridPSOGWOv2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1_start = 2.5\n        self.c1_end = 0.5\n        self.c2_start = 0.5\n        self.c2_end = 2.5\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            w = self.w_max - ((self.w_max - self.w_min) * evaluations / self.budget)\n            c1 = self.c1_start - ((self.c1_start - self.c1_end) * evaluations / self.budget)\n            c2 = self.c2_start + ((self.c2_end - self.c2_start) * evaluations / self.budget)\n\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()", "configspace": "", "generation": 6, "feedback": "The algorithm HybridPSOGWOv2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17027 with standard deviation 0.01153.", "error": "", "parent_ids": ["3ef32a00-81fd-4dc5-b130-b860b864793d"], "operator": null, "metadata": {"aucs": [0.1539890680470618, 0.17766597507102633, 0.17916781549426886]}}
{"id": "be27f98e-863f-4e07-9760-a2ad56eb86ad", "fitness": 0.1709314821456622, "name": "HybridPSOGWO", "description": "Improved HybridPSOGWO by introducing adaptive inertia weight and dynamic particle count to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia weight start\n        self.w_end = 0.4  # adaptive inertia weight end\n        self.c1 = 2.0  # cognitive (particle)\n        self.c2 = 2.0  # social (swarm)\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            current_w = self.w - ((self.w - self.w_end) * (evaluations / self.budget))  # adaptive inertia\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (current_w * v[i]  # use adaptive inertia weight\n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()", "configspace": "", "generation": 7, "feedback": "The algorithm HybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17093 with standard deviation 0.00965.", "error": "", "parent_ids": ["3ef32a00-81fd-4dc5-b130-b860b864793d"], "operator": null, "metadata": {"aucs": [0.1573655581132578, 0.17905208019929875, 0.17637680812443002]}}
{"id": "216b6bfc-7002-4766-a2fd-69655388318d", "fitness": 0.1709314821456622, "name": "HybridPSOGWO", "description": "Improved HybridPSOGWO - Enhances the hybrid approach by introducing adaptive weight and diversity mechanisms to improve exploration and convergence.", "code": "import numpy as np\n\nclass HybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # start inertia\n        self.w_min = 0.4  # minimum inertia\n        self.c1 = 2.0  # cognitive (particle)\n        self.c2 = 2.0  # social (swarm)\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            w_adaptive = self.w_min + (self.w - self.w_min) * (1 - evaluations / self.budget)\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (w_adaptive * v[i] \n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()", "configspace": "", "generation": 8, "feedback": "The algorithm HybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17093 with standard deviation 0.00965.", "error": "", "parent_ids": ["3ef32a00-81fd-4dc5-b130-b860b864793d"], "operator": null, "metadata": {"aucs": [0.1573655581132578, 0.17905208019929875, 0.17637680812443002]}}
{"id": "db741ec5-f7e2-49aa-a928-349895f8be9c", "fitness": 0.1611269176917407, "name": "HybridPSOGWOPlus", "description": "HybridPSOGWOPlus - Enhances HybridPSOGWO by incorporating adaptive parameters for improved exploration-exploitation balance and convergence.", "code": "import numpy as np\n\nclass HybridPSOGWOPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w_max = 0.9\n        self.w_min = 0.4\n        self.c1 = 2.0  # cognitive (particle)\n        self.c2 = 2.0  # social (swarm)\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            w = self.w_max - (self.w_max - self.w_min) * (evaluations / self.budget)\n            \n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (w * v[i] \n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component with adaptive coefficients\n                A1, A2, A3 = 2 * (1 - evaluations / self.budget) * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()", "configspace": "", "generation": 9, "feedback": "The algorithm HybridPSOGWOPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16113 with standard deviation 0.01609.", "error": "", "parent_ids": ["3ef32a00-81fd-4dc5-b130-b860b864793d"], "operator": null, "metadata": {"aucs": [0.15043022118583416, 0.14908690060661212, 0.18386363128277583]}}
{"id": "981b62ba-5014-4def-84fe-ff0b5dc83c38", "fitness": 0.16353164337998635, "name": "HybridPSOGWO", "description": "Enhanced HybridPSOGWO - Incorporates adaptive inertia and dynamic social factor adjustments to augment convergence efficiency.", "code": "import numpy as np\n\nclass HybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia start\n        self.w_min = 0.4\n        self.c1 = 2.0  # cognitive (particle)\n        self.c2 = 1.5  # dynamic social start\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            # Adaptive inertia and social factor\n            self.w = self.w_min + (0.9 - self.w_min) * (1 - evaluations / self.budget)\n            self.c2 = 2.5 - (2.0 * evaluations / self.budget)\n\n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + self.c1 * r1 * (personal_best[i] - x[i]) \n                        + self.c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()", "configspace": "", "generation": 10, "feedback": "The algorithm HybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16353 with standard deviation 0.02172.", "error": "", "parent_ids": ["3ef32a00-81fd-4dc5-b130-b860b864793d"], "operator": null, "metadata": {"aucs": [0.13411194945732574, 0.17060525891758538, 0.1858777217650479]}}
{"id": "c50febee-232b-4bb5-9c77-b8e2bfa7c4d1", "fitness": 0.1800076967765233, "name": "HybridPSOGWO", "description": "Improved HybridPSOGWO - An enhanced version of HybridPSOGWO that incorporates adaptive parameters to improve convergence through dynamic adjustment of exploration and exploitation.", "code": "import numpy as np\n\nclass HybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.5  # inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                # Adaptive coefficients\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()", "configspace": "", "generation": 11, "feedback": "The algorithm HybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18001 with standard deviation 0.00904.", "error": "", "parent_ids": ["3ef32a00-81fd-4dc5-b130-b860b864793d"], "operator": null, "metadata": {"aucs": [0.19278304354870346, 0.1739515211668985, 0.17328852561396801]}}
{"id": "8945501b-acb7-4e02-a2f8-ff1570c15e24", "fitness": 0.1802906572922179, "name": "EnhancedHybridPSOGWO", "description": "Enhanced HybridPSOGWO with Adaptive Vortex Search - Incorporates a vortex search mechanism and adaptive inertia to improve solution diversity and convergence speed.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                vortex_solution = x[i] + vortex_vector * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18029 with standard deviation 0.00429.", "error": "", "parent_ids": ["c50febee-232b-4bb5-9c77-b8e2bfa7c4d1"], "operator": null, "metadata": {"aucs": [0.18631822499418915, 0.17786574642609998, 0.17668800045636457]}}
{"id": "a380b6bd-bfcb-45ac-84c1-3e0ebb4a51bc", "fitness": 0.17806340146029645, "name": "OptimizedHybridPSOGWO", "description": "OptimizedHybridPSOGWO with Enhanced Adaptive Strategies - Introduces adaptive learning coefficients and an elite re-evaluation mechanism to significantly enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass OptimizedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w_init = 0.9\n        self.w_end = 0.4\n        self.c1_init = 2.0\n        self.c2_init = 2.0\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            w = self.w_init - (self.w_init - self.w_end) * (evaluations / self.budget)\n            c1 = self.c1_init * (1 - evaluations / self.budget)\n            c2 = self.c2_init * (evaluations / self.budget)\n\n            for i in range(self.particles):\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                vortex_solution = x[i] + vortex_vector * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()\n\n            # Elite re-evaluation mechanism to intensify search around best solutions\n            if evaluations < self.budget:\n                elite_re_eval = 0.1 * self.particles\n                for i in range(int(elite_re_eval)):\n                    elite_solution = personal_best[sorted_indices[i]]\n                    elite_cost = func(elite_solution)\n                    evaluations += 1\n                    if elite_cost < self.best_cost:\n                        self.best_cost = elite_cost\n                        self.best_solution = elite_solution.copy()", "configspace": "", "generation": 13, "feedback": "The algorithm OptimizedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17806 with standard deviation 0.00801.", "error": "", "parent_ids": ["8945501b-acb7-4e02-a2f8-ff1570c15e24"], "operator": null, "metadata": {"aucs": [0.18688170405146964, 0.17981919926566603, 0.1674893010637537]}}
{"id": "da28dc25-817d-413f-9f8d-a41889dfab2e", "fitness": 0.1601322852736867, "name": "EnhancedHybridPSOGWOElite", "description": "Enhanced HybridPSOGWO with Adaptive Dynamic Search and Elite Memory - Introduces dynamic exploration and an elite memory mechanism boosting exploration and exploitation balance for superior convergence.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWOElite:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n        self.elite_memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n\n                cost = func(x[i])\n                evaluations += 1\n\n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n                    if len(self.elite_memory) < 5:\n                        self.elite_memory.append((self.best_solution.copy(), self.best_cost))\n                    else:\n                        # Replace the worst elite member\n                        worst_elite_index = max(range(len(self.elite_memory)),\n                                                key=lambda j: self.elite_memory[j][1])\n                        if self.best_cost < self.elite_memory[worst_elite_index][1]:\n                            self.elite_memory[worst_elite_index] = (self.best_solution.copy(), self.best_cost)\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i]\n                        + c1 * r1 * (personal_best[i] - x[i])\n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n\n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n\n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n\n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Dynamic search enhancement\n                dynamic_vector = np.random.uniform(-1, 1, self.dim)\n                dynamic_solution = x[i] + dynamic_vector * 0.1 * (ub - lb)\n                dynamic_solution = np.clip(dynamic_solution, lb, ub)\n                dynamic_cost = func(dynamic_solution)\n                evaluations += 1\n\n                if dynamic_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = dynamic_cost\n                    personal_best[i] = dynamic_solution.copy()\n                if dynamic_cost < self.best_cost:\n                    self.best_cost = dynamic_cost\n                    self.best_solution = dynamic_solution.copy()\n\n                # Elite memory search\n                if len(self.elite_memory) > 0:\n                    elite_choice = np.random.choice(len(self.elite_memory))\n                    elite_vector = np.random.normal(0, 1, self.dim)\n                    elite_solution = self.elite_memory[elite_choice][0] + elite_vector * 0.05\n                    elite_solution = np.clip(elite_solution, lb, ub)\n                    elite_cost = func(elite_solution)\n                    evaluations += 1\n\n                    if elite_cost < personal_best_cost[i]:\n                        personal_best_cost[i] = elite_cost\n                        personal_best[i] = elite_solution.copy()\n                    if elite_cost < self.best_cost:\n                        self.best_cost = elite_cost\n                        self.best_solution = elite_solution.copy()", "configspace": "", "generation": 14, "feedback": "The algorithm EnhancedHybridPSOGWOElite got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16013 with standard deviation 0.01612.", "error": "", "parent_ids": ["8945501b-acb7-4e02-a2f8-ff1570c15e24"], "operator": null, "metadata": {"aucs": [0.1400426103713167, 0.16085644598650228, 0.17949779946324107]}}
{"id": "7031f742-cd54-4c0b-a31e-909d0487e1be", "fitness": 0.17186361066144773, "name": "EnhancedHybridPSOGWO", "description": "Enhanced Hybrid PSOGWO with Adaptive Hyperparameters - Introduces dynamic adjustment of coefficients and exploration-exploitation balance to enhance performance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.7 + 0.2 * np.cos(np.pi * evaluations / self.budget)  # Adaptive inertia with cosine modulation\n                c1 = self.c1_init * (1 - evaluations / self.budget) * np.random.rand()  # Dynamic adjustment\n                c2 = self.c2_init * (evaluations / self.budget) * np.random.rand()  # Dynamic adjustment\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                vortex_solution = x[i] + vortex_vector * (0.5 * evaluations / self.budget)  # Adjust exploration-exploitation balance\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 15, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17186 with standard deviation 0.00810.", "error": "", "parent_ids": ["8945501b-acb7-4e02-a2f8-ff1570c15e24"], "operator": null, "metadata": {"aucs": [0.16209372869208583, 0.18192278923387806, 0.1715743140583793]}}
{"id": "fb0114a4-c066-4805-9929-93857ed6b9e6", "fitness": 0.15386757954297145, "name": "EnhancedHybridPSOGWO", "description": "Enhanced HybridPSOGWO with Adaptive Differential Mutation - Integrates differential mutation strategies for increased exploration and adaptive refinement of solutions.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                vortex_solution = x[i] + vortex_vector * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()\n\n                # Differential Mutation addition\n                if evaluations < self.budget - 2:\n                    idxs = np.random.choice(self.particles, 3, replace=False)\n                    F = 0.8  # Differential weight\n                    mutant_vector = personal_best[idxs[0]] + F * (personal_best[idxs[1]] - personal_best[idxs[2]])\n                    mutant_vector = np.clip(mutant_vector, lb, ub)\n                    mutant_cost = func(mutant_vector)\n                    evaluations += 1\n\n                    if mutant_cost < personal_best_cost[i]:\n                        personal_best_cost[i] = mutant_cost\n                        personal_best[i] = mutant_vector.copy()\n                    if mutant_cost < self.best_cost:\n                        self.best_cost = mutant_cost\n                        self.best_solution = mutant_vector.copy()", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15387 with standard deviation 0.00728.", "error": "", "parent_ids": ["8945501b-acb7-4e02-a2f8-ff1570c15e24"], "operator": null, "metadata": {"aucs": [0.14362424248063133, 0.15984371735549685, 0.15813477879278615]}}
{"id": "be3d7273-69da-4878-95dd-c79d405d90ac", "fitness": 0.18221700312175085, "name": "EnhancedHybridPSOGWO", "description": "Introduced dynamic adjustment of vortex search intensity based on solution quality.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement with dynamic adjustment\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                intensity = (personal_best_cost[i] / self.best_cost) if self.best_cost != 0 else 1\n                vortex_solution = x[i] + vortex_vector * intensity * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18222 with standard deviation 0.00343.", "error": "", "parent_ids": ["8945501b-acb7-4e02-a2f8-ff1570c15e24"], "operator": null, "metadata": {"aucs": [0.18699651187626154, 0.17913829461765984, 0.1805162028713312]}}
{"id": "967486ed-7c95-4691-a7c0-b0ee592be85a", "fitness": 0.1802392799677125, "name": "EnhancedHybridPSOGWO", "description": "Enhanced dynamic adaptation of swarm and vortex intensities using individual progress metrics to improve convergence.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n\n                cost = func(x[i])\n                evaluations += 1\n\n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i]\n                        + c1 * r1 * (personal_best[i] - x[i])\n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n\n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n\n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n\n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement with refined dynamic adjustment\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                intensity_factor = np.exp(-personal_best_cost[i] / (self.best_cost + 1e-9))\n                vortex_solution = x[i] + vortex_vector * intensity_factor * (1 - evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18024 with standard deviation 0.00357.", "error": "", "parent_ids": ["be3d7273-69da-4878-95dd-c79d405d90ac"], "operator": null, "metadata": {"aucs": [0.18524238016176886, 0.17830825024516883, 0.1771672094961998]}}
{"id": "ae14efe8-a043-4a1b-9ab9-c470f3224757", "fitness": 0.1809764248732071, "name": "EnhancedDynamicVortexPSOGWO", "description": "EnhancedDynamicVortexPSOGWO introduces a multi-phase vortex search that dynamically adapts both intensity and direction based on historical performance trends.", "code": "import numpy as np\n\nclass EnhancedDynamicVortexPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n        self.history = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n            self.history.append(self.best_cost)\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Multi-phase Vortex search with dynamic adjustment\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                historical_trend = np.std(self.history[-min(len(self.history), 5):]) if len(self.history) > 5 else 1\n                intensity = (personal_best_cost[i] / self.best_cost) * historical_trend if self.best_cost != 0 else 1\n                vortex_solution = x[i] + vortex_vector * intensity * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedDynamicVortexPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18098 with standard deviation 0.00250.", "error": "", "parent_ids": ["be3d7273-69da-4878-95dd-c79d405d90ac"], "operator": null, "metadata": {"aucs": [0.18419395371060443, 0.17808453237141142, 0.1806507885376054]}}
{"id": "af30df8f-1c7a-43a6-b520-fee29b2029fd", "fitness": 0.18028081423790257, "name": "EnhancedHybridPSOGWO", "description": "Adjusted the vortex search intensity calculation to use a more sensitive scaling factor based on the logarithm of the fitness ratio to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement with dynamic adjustment\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                intensity = np.log1p(personal_best_cost[i] / self.best_cost) if self.best_cost != 0 else 1\n                vortex_solution = x[i] + vortex_vector * intensity * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18028 with standard deviation 0.00401.", "error": "", "parent_ids": ["be3d7273-69da-4878-95dd-c79d405d90ac"], "operator": null, "metadata": {"aucs": [0.18589682583634692, 0.1781576025748195, 0.17678801430254132]}}
{"id": "bbeef298-74fd-4a8b-9476-34639617a60f", "fitness": 0.18221611241779237, "name": "EnhancedHybridPSOGWO_Improved", "description": "Introduced adaptive learning rates based on convergence speed to balance exploration and exploitation dynamically.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w_init = 0.9  # initial inertia\n        self.w_final = 0.4  # final inertia\n        self.c1_init = 2.0\n        self.c2_init = 2.0\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                progress = evaluations / self.budget\n                self.w = self.w_init - progress * (self.w_init - self.w_final)\n                c1 = self.c1_init * (1 - progress)\n                c2 = self.c2_init * progress\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement with dynamic adjustment\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                intensity = (personal_best_cost[i] / self.best_cost) if self.best_cost != 0 else 1\n                vortex_solution = x[i] + vortex_vector * intensity * progress\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 21, "feedback": "The algorithm EnhancedHybridPSOGWO_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18222 with standard deviation 0.00343.", "error": "", "parent_ids": ["be3d7273-69da-4878-95dd-c79d405d90ac"], "operator": null, "metadata": {"aucs": [0.18699522597978802, 0.17913691567736423, 0.1805161955962249]}}
{"id": "7cd24c1f-384c-43f1-adcc-a8aa6d479c9f", "fitness": 0.17327738880771537, "name": "EnhancedHybridPSOGWO", "description": "Introduced adaptive predator-prey dynamics to enhance exploration and exploitation balance in the search process.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n\n                cost = func(x[i])\n                evaluations += 1\n\n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.5 * (evaluations / self.budget)  # Adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i]\n                        + c1 * r1 * (personal_best[i] - x[i])\n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n\n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n\n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n\n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Predator-prey dynamics\n                intensity = (personal_best_cost[i] / self.best_cost) if self.best_cost != 0 else 1\n                prey_position = candidate_solution + np.random.normal(0, 1, self.dim) * intensity * (evaluations / self.budget)\n                prey_position = np.clip(prey_position, lb, ub)\n                prey_cost = func(prey_position)\n                evaluations += 1\n\n                if prey_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = prey_cost\n                    personal_best[i] = prey_position.copy()\n                if prey_cost < self.best_cost:\n                    self.best_cost = prey_cost\n                    self.best_solution = prey_position.copy()", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17328 with standard deviation 0.01326.", "error": "", "parent_ids": ["be3d7273-69da-4878-95dd-c79d405d90ac"], "operator": null, "metadata": {"aucs": [0.15498047822734828, 0.17885804212805634, 0.1859936460677415]}}
{"id": "0af801ab-cc03-433b-be2e-4a8f8d256a80", "fitness": 0.1827045291628254, "name": "EnhancedHybridPSOGWO", "description": "Improved exploration by fine-tuning the inertia weight update strategy for better balance between exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Adjusted adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement with dynamic adjustment\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                intensity = (personal_best_cost[i] / self.best_cost) if self.best_cost != 0 else 1\n                vortex_solution = x[i] + vortex_vector * intensity * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 23, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18270 with standard deviation 0.00411.", "error": "", "parent_ids": ["be3d7273-69da-4878-95dd-c79d405d90ac"], "operator": null, "metadata": {"aucs": [0.18830698341176488, 0.17856971518676779, 0.1812368888899435]}}
{"id": "257284c0-a1ef-4e7f-aee7-921a5d588c7f", "fitness": 0.17305001296375402, "name": "RefinedHybridPSOGWO", "description": "Introducing dynamic particle interactions and adaptive vortex intensity based on convergence trends to enhance search efficiency and solution quality.", "code": "import numpy as np\n\nclass RefinedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Adjusted adaptive inertia\n                convergence_factor = 1 - (self.best_cost / (np.mean(personal_best_cost) + 1e-9))\n                c1 = self.c1_init * (1 - evaluations / self.budget) * convergence_factor\n                c2 = self.c2_init * (evaluations / self.budget) * (2 - convergence_factor)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement with dynamic adjustment\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                intensity = (personal_best_cost[i] / self.best_cost) if self.best_cost != 0 else 1\n                vortex_solution = x[i] + vortex_vector * intensity * (1 - convergence_factor) * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 24, "feedback": "The algorithm RefinedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17305 with standard deviation 0.00619.", "error": "", "parent_ids": ["0af801ab-cc03-433b-be2e-4a8f8d256a80"], "operator": null, "metadata": {"aucs": [0.16754502600134036, 0.18170210913442353, 0.1699029037554982]}}
{"id": "965f3234-32e2-465b-a341-7a3a11abe116", "fitness": 0.17628732252731596, "name": "EnhancedHybridPSOGWO", "description": "Introduce a slight adaptive change to the Grey Wolf's influence to improve convergence rate.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.6 * (evaluations / self.budget)  # Adjusted adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                # Slight adaptive change to improve convergence\n                X1 = X1 * (0.9 + 0.1 * (evaluations / self.budget))\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement with dynamic adjustment\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                intensity = (personal_best_cost[i] / self.best_cost) if self.best_cost != 0 else 1\n                vortex_solution = x[i] + vortex_vector * intensity * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17629 with standard deviation 0.01407.", "error": "", "parent_ids": ["0af801ab-cc03-433b-be2e-4a8f8d256a80"], "operator": null, "metadata": {"aucs": [0.1812750010617905, 0.19047956134091915, 0.15710740517923827]}}
{"id": "7ce44bef-c42e-4b27-82fc-f8cbfb64e418", "fitness": 0.18406273023917, "name": "EnhancedHybridPSOGWO", "description": "Slightly adjusted the inertia weight decrement formula to improve exploitation near the end of the search process.", "code": "import numpy as np\n\nclass EnhancedHybridPSOGWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.particles = 30\n        self.w = 0.9  # adaptive inertia\n        self.c1_init = 2.0  # initial cognitive coefficient\n        self.c2_init = 2.0  # initial social coefficient\n        self.alpha, self.beta, self.delta = None, None, None\n        self.best_cost = float('inf')\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x = np.random.uniform(lb, ub, (self.particles, self.dim))\n        v = np.random.uniform(-1, 1, (self.particles, self.dim))\n        personal_best = x.copy()\n        personal_best_cost = np.array([float('inf')] * self.particles)\n\n        evaluations = 0\n        \n        while evaluations < self.budget:\n            for i in range(self.particles):\n                if evaluations >= self.budget:\n                    break\n                \n                cost = func(x[i])\n                evaluations += 1\n                \n                if cost < personal_best_cost[i]:\n                    personal_best_cost[i] = cost\n                    personal_best[i] = x[i].copy()\n\n                if cost < self.best_cost:\n                    self.best_cost = cost\n                    self.best_solution = x[i].copy()\n\n            sorted_indices = np.argsort(personal_best_cost)\n            self.alpha = personal_best[sorted_indices[0]]\n            self.beta = personal_best[sorted_indices[1]]\n            self.delta = personal_best[sorted_indices[2]]\n\n            for i in range(self.particles):\n                self.w = 0.9 - 0.7 * (evaluations / self.budget)  # Adjusted adaptive inertia\n                c1 = self.c1_init * (1 - evaluations / self.budget)\n                c2 = self.c2_init * (evaluations / self.budget)\n                \n                r1, r2 = np.random.rand(), np.random.rand()\n                v[i] = (self.w * v[i] \n                        + c1 * r1 * (personal_best[i] - x[i]) \n                        + c2 * r2 * (self.best_solution - x[i]))\n                x[i] = np.clip(x[i] + v[i], lb, ub)\n\n                # Grey Wolf Optimization component\n                A1, A2, A3 = 2 * np.random.rand(3, self.dim) - 1\n                C1, C2, C3 = 2 * np.random.rand(3, self.dim)\n                \n                D_alpha = np.abs(C1 * self.alpha - x[i])\n                D_beta = np.abs(C2 * self.beta - x[i])\n                D_delta = np.abs(C3 * self.delta - x[i])\n                \n                X1 = self.alpha - A1 * D_alpha\n                X2 = self.beta - A2 * D_beta\n                X3 = self.delta - A3 * D_delta\n                \n                candidate_solution = (X1 + X2 + X3) / 3\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                candidate_cost = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = candidate_cost\n                    personal_best[i] = candidate_solution.copy()\n                if candidate_cost < self.best_cost:\n                    self.best_cost = candidate_cost\n                    self.best_solution = candidate_solution.copy()\n\n                # Vortex search enhancement with dynamic adjustment\n                vortex_vector = np.random.normal(0, 1, self.dim)\n                intensity = (personal_best_cost[i] / self.best_cost) if self.best_cost != 0 else 1\n                vortex_solution = x[i] + vortex_vector * intensity * (evaluations / self.budget)\n                vortex_solution = np.clip(vortex_solution, lb, ub)\n                vortex_cost = func(vortex_solution)\n                evaluations += 1\n\n                if vortex_cost < personal_best_cost[i]:\n                    personal_best_cost[i] = vortex_cost\n                    personal_best[i] = vortex_solution.copy()\n                if vortex_cost < self.best_cost:\n                    self.best_cost = vortex_cost\n                    self.best_solution = vortex_solution.copy()", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedHybridPSOGWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18406 with standard deviation 0.00505.", "error": "", "parent_ids": ["0af801ab-cc03-433b-be2e-4a8f8d256a80"], "operator": null, "metadata": {"aucs": [0.19094463652495786, 0.18229196192398844, 0.17895159226856372]}}
