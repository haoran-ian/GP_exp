{"id": "60dab772-8aac-4a30-a274-1374a9858270", "fitness": 0.12053367191529538, "name": "AdaptiveMetaheuristic", "description": "Adaptive Population-Based Search with Dynamic Strategy Selection and Local Learning", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.12053 with standard deviation 0.18605.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.6223436889133993, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "af6adfa3-c93c-4cb5-86df-98721cb361b8", "fitness": 0.07359254016723789, "name": "AdaptiveMetaheuristic", "description": "Hybrid Strategy with Adaptive Memory and Selective Pressure Balancing: Enhances exploration-exploitation balance by introducing adaptive memory and selective pressure adjustment mechanisms. ", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.memory = []  # Adaptive memory for storing good solutions\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n                # Add to memory if it's significantly better\n                if len(self.memory) == 0 or (best_fitness < min(x[1] for x in self.memory) * 0.95):\n                    self.memory.append((best_solution.copy(), best_fitness))\n            \n            # Adaptive strategy selection with memory analysis\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Use memory to refine exploration-exploitation balance\n            if len(self.memory) > 0 and evaluations < self.budget * 0.8:\n                memory_idx = np.random.choice(len(self.memory))\n                memory_solution = self.memory[memory_idx][0]\n                new_population = np.vstack([new_population, memory_solution + np.random.randn(self.dim) * 0.01])\n                new_fitness = np.append(new_fitness, func(new_population[-1]))\n                evaluations += 1\n            \n            # Move to the next generation\n            combined_population = np.vstack([population, new_population])\n            combined_fitness = np.append(fitness, new_fitness)\n            selected_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[selected_indices], combined_fitness[selected_indices]\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07359 with standard deviation 0.06158.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1537286358693679, 0.1531515915131837, 0.14626785848228663, 0.07139380761240943, 0.06860094457732746, 0.06769002345056607, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "ccfcc906-d30d-4bba-a675-1865084bff17", "fitness": 0.06722606913708414, "name": "AdaptiveMetaheuristic", "description": "Improved adaptive strategy selection by incorporating exploration-exploitation balance and perturbation scaling.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            exploitation_factor = success.mean()\n            exploration_factor = 1 - exploitation_factor\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (exploitation_factor - exploration_factor))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.05\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06723 with standard deviation 0.06226.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1455935009009126, 0.1611229637237951, 0.14293587811571473, 0.05857047739217114, 0.047375381149801554, 0.0479364209513623, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "0d8e138b-4660-4bb3-8c76-d83d9c2d6bfa", "fitness": 0.06739175788783194, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Adaptive Population Sizing and Hybridized Strategies for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.hybrid_crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = self.initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Adjust population size dynamically\n            population_size = min(self.initial_population_size * 2, max(5, int(self.initial_population_size * (1 + 0.1 * (1 - success.mean())))))\n            if evaluations + population_size > self.budget:\n                population_size = self.budget - evaluations\n\n            # Move to the next generation\n            population, fitness = new_population[:population_size], new_fitness[:population_size]\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def hybrid_crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(len(population)):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06739 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "076365d8-69c5-429e-a349-ae1b8cb5316f", "fitness": 0.06687990658141253, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Memory-Based Strategy Adaptation and Dynamic Learning Rate.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.initial_learning_rate = 0.1\n        self.min_learning_rate = 0.01\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.strategy_success_memory = np.zeros(len(self.strategy_pool))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        learning_rate = self.initial_learning_rate\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            success = new_fitness < fitness\n            self.strategy_success_memory[strategy_idx] = 0.9 * self.strategy_success_memory[strategy_idx] + 0.1 * success.mean()\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + learning_rate * self.strategy_success_memory[strategy_idx])\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            learning_rate = max(self.min_learning_rate, learning_rate * 0.99)\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06688 with standard deviation 0.06151.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13888216049978108, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.05028245243045093, 0.046966842466508396, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "83f04af1-fd22-4546-83e4-0073454f9c5f", "fitness": 0.06656654566594936, "name": "AdaptiveMetaheuristic", "description": "Enhanced AdaptiveMetaheuristic by refining strategy update to account for improved fitness.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (new_fitness.mean() < fitness.mean()))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06657 with standard deviation 0.06136.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13645235956475932, 0.1624167065600619, 0.14385292277490813, 0.05893511916019589, 0.04854466377345734, 0.04739713916016175, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "471a77e1-c566-40b5-ad9e-a5295ebc8d43", "fitness": 0.06704820192125603, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Memory-Augmented Strategies for Improved Search Efficiency and Convergence", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [\n            self.random_search,\n            self.gradient_search,\n            self.crossover_mutation,\n            self.memory_augmentation\n        ]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.best_memory = None\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        self.best_memory = best_solution.copy()\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n\n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n                self.best_memory = best_solution.copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n\n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n\n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def memory_augmentation(self, population, func, lb, ub):\n        if self.best_memory is None:\n            return np.random.uniform(lb, ub, population.shape)\n        perturbation = np.random.randn(self.dim) * 0.1\n        candidates = self.best_memory + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return np.tile(candidates, (self.population_size, 1))", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06705 with standard deviation 0.05819.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13512341281863038, 0.1436578979435965, 0.1460670601063927, 0.052896871869267725, 0.05239819132185841, 0.07179038323155873, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "b64b8d20-64a1-4d83-8275-f60245606c2f", "fitness": 0.06715661903804787, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Population-Based Search with Dual-Rate Strategy Adaptation.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.momentum = 0.9  # New momentum term for strategy adaptation\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.strategy_success_history = np.zeros(len(self.strategy_pool))  # Track success over time\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_success_history[strategy_idx] = self.momentum * self.strategy_success_history[strategy_idx] + (1 - self.momentum) * success.mean()\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * self.strategy_success_history[strategy_idx])\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06716 with standard deviation 0.06173.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13888216049978108, 0.1624167065600619, 0.14469622979798724, 0.05893511916019589, 0.050735229442571916, 0.04724412588183291, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f10bc323-cae9-4d88-9f67-d6ca3b596053", "fitness": 0.06427894465002239, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy Selection with Improved Learning Rate for Better Convergence", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.15  # Adjusted learning rate\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06428 with standard deviation 0.05839.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13451913801217763, 0.1463991893523532, 0.14293587811571473, 0.05893511916019589, 0.046216267526542065, 0.04800490968321813, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "615c0f27-0b74-4a26-8be6-02e0ce1e8038", "fitness": 0.06676294676935471, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Multi-Armed Bandit Strategy for Dynamic Strategy Selection and Feedback-Driven Parameter Tuning.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_success = np.zeros(len(self.strategy_pool))\n        self.strategy_attempts = np.ones(len(self.strategy_pool))  # To avoid division by zero\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = self.select_strategy()\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Update strategy success\n            improvement = fitness - new_fitness\n            success_rate = np.mean(improvement > 0)\n            self.strategy_success[strategy_idx] += success_rate\n            self.strategy_attempts[strategy_idx] += 1\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def select_strategy(self):\n        success_rates = self.strategy_success / self.strategy_attempts\n        exploration_term = np.sqrt(2 * np.log(np.sum(self.strategy_attempts)) / self.strategy_attempts)\n        ucb_values = success_rates + exploration_term\n        return np.argmax(ucb_values)\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06676 with standard deviation 0.05844.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13960433992068488, 0.1422255378918018, 0.14540243404333442, 0.050018272584086754, 0.06495475450993304, 0.057161181974351605, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "e01ed8c1-863c-40cb-adaa-99d823b50678", "fitness": 0.06510987704939951, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Population-Based Search with Dynamic Strategy Selection, Memory-Based Strategy Adaptation, and Self-Adaptive Parameter Tuning to boost convergence efficiency across diverse optimization tasks.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.memory = np.zeros(len(self.strategy_pool))\n        self.parameter_pool = [0.1, 0.05, 0.01]\n        self.param_selection_probability = np.ones(len(self.parameter_pool)) / len(self.parameter_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            param_idx = np.random.choice(len(self.parameter_pool), p=self.param_selection_probability)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub, self.parameter_pool[param_idx])\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n                self.memory[strategy_idx] += 1  # Reward successful strategy\n            \n            # Adaptive strategy selection with memory\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] += self.learning_rate * success.mean()\n            self.strategy_probabilities += self.memory  # Bias towards historically successful strategies\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Self-adaptive parameter tuning\n            self.param_selection_probability[param_idx] += self.learning_rate * success.mean()\n            self.param_selection_probability /= self.param_selection_probability.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub, param):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub, param):\n        perturbation = np.random.randn(*population.shape) * param\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub, param):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * param\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 10, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06511 with standard deviation 0.06028.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1493615069384704, 0.14423348535145053, 0.14293587811571473, 0.04984696732746441, 0.04625339343776347, 0.05185766227373223, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "86c124e2-06bc-4e98-ada2-44fac8ce9513", "fitness": 0.06623349105756482, "name": "AdaptiveMetaheuristic", "description": "Introduced a slight bias towards successful strategies in adaptive strategy selection.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (success.mean() + 0.01))  # Added slight bias\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06623 with standard deviation 0.06087.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13364206119382438, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d4619b5e-f676-4436-a72f-7933ffb21cd2", "fitness": 0.06702174794046337, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic using Self-Adaptive Mutation Rate and Memory-Based Strategy Selection.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.mutation_rate = 0.1\n        self.memory = np.zeros(len(self.strategy_pool))\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with self-adaptive mutation rate\n            success = new_fitness < fitness\n            self.memory[strategy_idx] = max(0, self.memory[strategy_idx] + success.mean() - 0.5)\n            self.strategy_probabilities = np.exp(self.memory) / np.sum(np.exp(self.memory))\n            \n            # Adjust mutation rate based on strategy success\n            if success.mean() > 0.4:\n                self.mutation_rate = max(0.05, self.mutation_rate * 0.9)\n            else:\n                self.mutation_rate = min(0.5, self.mutation_rate * 1.1)\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * self.mutation_rate\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * self.mutation_rate\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06702 with standard deviation 0.06196.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14364761916205548, 0.1611229637237951, 0.14293587811571473, 0.05693672884556267, 0.049606404730874076, 0.04744613688616839, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "709f239d-b29b-4424-af7c-4315bf3ad5c0", "fitness": 0.06745445264640543, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy through Dynamic Learning Rate for Improved Convergence", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with dynamic learning rate\n            success = new_fitness < fitness\n            dynamic_lr = self.learning_rate * (0.5 + 0.5 * success.mean())  # Adjusted learning rate\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + dynamic_lr * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 13, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06745 with standard deviation 0.06220.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.050735229442571916, 0.046477862084368105, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "0affe4b8-7f9b-4185-9579-156f305989a2", "fitness": 0.06511362382510344, "name": "AdaptiveMetaheuristic", "description": "Enhance adaptive strategy by integrating a differential evolution-inspired mutation mechanism.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.differential_evolution_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def differential_evolution_mutation(self, population, func, lb, ub):\n        offspring = []\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            x1, x2, x3 = population[np.random.choice(indices, 3, replace=False)]\n            mutant = x1 + 0.8 * (x2 - x3)\n            np.clip(mutant, lb, ub, out=mutant)\n            offspring.append(mutant)\n        return np.array(offspring)", "configspace": "", "generation": 14, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06511 with standard deviation 0.05804.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13188134045549982, 0.14756783461137857, 0.14315518695447849, 0.04932780757909805, 0.05203081864184578, 0.06055962618363042, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "6d09d99e-7997-4941-9f7f-2fbaf104806a", "fitness": 0.06762978693281192, "name": "AdaptiveMetaheuristic", "description": "Enhanced strategy adaptation with increased exploration through controlled perturbation.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.15  # Increased exploration\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 15, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06763 with standard deviation 0.06230.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14537252754915297, 0.16211188162891277, 0.14300283061930485, 0.05681754076429013, 0.05315306369994666, 0.04671023813370012, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "936d95c7-7e1e-49fb-834d-bd9ea2d17f96", "fitness": 0.06414727045784781, "name": "AdaptiveMetaheuristic", "description": "Improved Adaptive Strategy with Enhanced Exploration and Adaptive Learning Rate for Black Box Optimization", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [\n            self.random_search, \n            self.gradient_search, \n            self.crossover_mutation, \n            self.simulated_annealing\n        ]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with dynamic learning rate\n            success = new_fitness < fitness\n            self.learning_rate = 0.1 / (1 + 0.1 * evaluations / self.budget)\n            self.strategy_probabilities[strategy_idx] = max(0.05, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def simulated_annealing(self, population, func, lb, ub):\n        new_population = []\n        T = 1.0\n        for ind in population:\n            candidate = ind + np.random.normal(0, 0.1, size=self.dim)\n            np.clip(candidate, lb, ub, out=candidate)\n            if np.random.rand() < np.exp((func(ind) - func(candidate)) / T):\n                new_population.append(candidate)\n            else:\n                new_population.append(ind)\n        return np.array(new_population)", "configspace": "", "generation": 16, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06415 with standard deviation 0.05967.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14368874609396387, 0.14150165713089524, 0.14646437054383055, 0.04880139751054369, 0.04369577610178699, 0.05167348673961014, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "dc27e467-0fda-43f2-b913-893a64855875", "fitness": 0.06468704387058583, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Simulated Annealing and Diversity-Preserving Strategy Selection", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.temperature = 1.0  # Initial temperature for simulated annealing\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution with simulated annealing acceptance\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness or np.random.rand() < np.exp((best_fitness - new_fitness[new_best_idx]) / self.temperature):\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with diversity preservation\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (success.mean() + self.diversity_score(population, new_population)))\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n            \n            # Annealing schedule\n            self.temperature *= 0.95\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def diversity_score(self, old_population, new_population):\n        # Calculate diversity as the average pairwise distance between new and old population members\n        diff = new_population - old_population\n        distances = np.linalg.norm(diff, axis=1)\n        return np.mean(distances / (self.dim * (self.population_size ** 0.5)))", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06469 with standard deviation 0.05910.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1437291306596321, 0.1393993582691545, 0.14581769524655874, 0.04572172187712065, 0.049387209985030656, 0.05662827879777588, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "9c1c9617-5dad-477e-8f32-1394efc5c902", "fitness": -Infinity, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Self-Adjusting Population and Strategy Efficiency Tracking", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = max(5, dim * 2)\n        self.population_size = self.initial_population_size\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.strategy_efficiencies = np.zeros(len(self.strategy_pool))\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Track strategy efficiency\n            success = new_fitness < fitness\n            efficiency = success.mean()\n            self.strategy_efficiencies[strategy_idx] = efficiency\n            \n            # Adjust strategy probabilities based on efficiency\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * efficiency)\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Adjust population size\n            if efficiency > 0.2:\n                self.population_size = min(self.initial_population_size * 2, self.population_size + 1)\n            else:\n                self.population_size = max(5, self.population_size - 1)\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 18, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (42,) (40,) ').", "error": "ValueError('operands could not be broadcast together with shapes (42,) (40,) ')", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "07b29ec0-5896-41d9-b9f6-dee118493fdc", "fitness": 0.071521636158384, "name": "EnhancedMetaheuristic", "description": "A hybrid metaheuristic blending quantum-inspired tunneling with adaptive strategy selection for enhanced optimization performance.", "code": "import numpy as np\n\nclass EnhancedMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.quantum_tunneling]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def quantum_tunneling(self, population, func, lb, ub):\n        \"\"\"Quantum tunneling strategy to escape local optima.\"\"\"\n        tunneling_population = population + (ub - lb) * np.random.standard_normal(population.shape) * 0.01\n        np.clip(tunneling_population, lb, ub, out=tunneling_population)\n        return tunneling_population", "configspace": "", "generation": 19, "feedback": "The algorithm EnhancedMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07152 with standard deviation 0.07047.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.2066776721124064, 0.139903781820232, 0.14410334081382092, 0.0547718280061269, 0.04947552282032519, 0.047262579852544784, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "0cd303c5-5323-4bbb-9373-b41dc4db7b2d", "fitness": 0.06722422592275973, "name": "EnhancedAdaptiveMetaheuristic", "description": "Dynamic Cooperative Strategy with Adaptive Learning and Diversity Preservation for Enhanced Black Box Optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.diversity_threshold = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.diversity_maintenance]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n            \n            # Diversity preservation through adaptive learning rate adjustments\n            if np.std(population) < self.diversity_threshold:\n                self.learning_rate = min(0.5, self.learning_rate * 1.1)\n            else:\n                self.learning_rate = max(0.01, self.learning_rate * 0.9)\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def diversity_maintenance(self, population, func, lb, ub):\n        mean_individual = np.mean(population, axis=0)\n        deviation = np.random.uniform(-self.diversity_threshold, self.diversity_threshold, population.shape)\n        diverse_population = population + deviation + mean_individual\n        np.clip(diverse_population, lb, ub, out=diverse_population)\n        return diverse_population", "configspace": "", "generation": 20, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06722 with standard deviation 0.06078.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13743295737999406, 0.15256872220260576, 0.15153153739075043, 0.049399763896351034, 0.058648798322035955, 0.053936254113100435, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "81bd15fe-81bf-47ed-95e7-901a1d28bd70", "fitness": 0.06739175788783194, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy Selection with Probabilistic Update for Improved Performance", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * np.mean(success))  # Change: Using np.mean(success) instead of success.mean()\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 21, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06739 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "6ded4130-098b-4d25-a331-3cf8fd12b76f", "fitness": 0.06503429455799799, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Improved Strategy and Adaptive Learning Rate Adjustment.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 3)  # Increased population size\n        self.learning_rate = 0.05  # Adjusted learning rate\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] += self.learning_rate * (success.mean() - 0.5)  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.15  # Adjusted perturbation scale\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.1  # Adjusted mutation scale\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 22, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06503 with standard deviation 0.05944.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13838477715321706, 0.1427421313680225, 0.15028355450657405, 0.04850470405590879, 0.052172452986325224, 0.05172103095193448, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "99849d75-9380-40d3-a622-d92f76be2b71", "fitness": 0.06674395028524832, "name": "AdaptiveMetaheuristic", "description": "Enhanced adaptive learning rate for strategy probabilities based on recent performance trends to improve convergence.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            improvement = (fitness - new_fitness).mean()  # Improvement metric\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean() * improvement)\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 23, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06674 with standard deviation 0.06080.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1321653175535008, 0.1624167065600619, 0.14488113879709907, 0.05559227141238088, 0.05399536096234692, 0.05014475728184553, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d7bdf827-b75e-4cb5-b233-35e5b9fbc188", "fitness": 0.06714682844143753, "name": "AdaptiveMetaheuristic", "description": "Adaptive Population-Based Search with Enhanced Dynamic Strategy Selection for Improved Optimization", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (success.mean()/2))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 24, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06715 with standard deviation 0.06183.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14128445724000604, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.05028245243045093, 0.046966842466508396, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "3a3ab8fd-784e-4d0e-a380-127c58b57e75", "fitness": 0.06515124532785482, "name": "AdaptiveMetaheuristic", "description": "Introducing diverse mutation scales for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        scales = [0.02, 0.05, 0.1]  # Different mutation scales\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * np.random.choice(scales)  # Diverse mutations\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 25, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06515 with standard deviation 0.05800.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13748550990037, 0.14124683220224543, 0.1446597438742201, 0.05195721416831467, 0.05318213271136374, 0.05632977509417969, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "aec2cd97-da85-4591-a80b-b19c80d7d771", "fitness": 0.06490562945586367, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Entropy-Based Strategy Adjustment and Dynamic Perturbation for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.success_rates = np.zeros(len(self.strategy_pool))\n        self.decay_rate = 0.99\n        self.epsilon = 1e-3\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Entropy-based strategy adjustment\n            success = new_fitness < fitness\n            self.success_rates[strategy_idx] = self.success_rates[strategy_idx] * self.decay_rate + success.mean() * (1 - self.decay_rate)\n            sum_success = self.success_rates.sum()\n            if sum_success > 0:\n                self.strategy_probabilities = (self.success_rates + self.epsilon) / (sum_success + len(self.strategy_pool) * self.epsilon)\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 26, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06491 with standard deviation 0.05822.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13955149754258767, 0.1388258990070913, 0.14587391806898542, 0.052475472238992626, 0.049018951340423556, 0.05690492690469262, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "22fcde4c-e14a-42f4-b0d9-be147981780a", "fitness": 0.06640346362343898, "name": "AdaptiveMetaheuristic", "description": "Improved Adaptive Strategy Selection with Enhanced Learning Rate Adjustment.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            adjustment = self.learning_rate * (2.0 * success.mean() - 1.0)\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + adjustment)  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 27, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06640 with standard deviation 0.06176.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13952513876757666, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.04775233651518307, 0.04456599349221868, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "71484fa4-afc1-4a11-a9f5-1aae71f0592c", "fitness": 0.06524646556148336, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Memory-Based Strategy Improvement and Noise-Resistant Learning for Robust Black Box Optimization", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.05\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.success_memory = np.zeros(len(self.strategy_pool))\n        self.memory_decay = 0.9\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n                \n            # Adaptive strategy selection with memory\n            success_rate = (new_fitness < fitness).mean()\n            self.success_memory[strategy_idx] = (self.memory_decay * self.success_memory[strategy_idx] + \n                                                 (1 - self.memory_decay) * success_rate)\n            self.strategy_probabilities = np.exp(self.success_memory / self.learning_rate)\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.05\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.02\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 28, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06525 with standard deviation 0.05945.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13954721974506235, 0.14876376042168693, 0.14361597252270353, 0.052372987898256396, 0.049276260115095116, 0.05214198935054615, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c17dfd5b-6763-415e-a6b7-2256affe8762", "fitness": 0.06698332931038989, "name": "AdaptiveMetaheuristic", "description": "Enhanced AdaptiveMetaheuristic with increased mutation variability for crossover_mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.1  # Increased mutation variability\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 29, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06698 with standard deviation 0.06106.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1373417338156625, 0.1611229637237951, 0.1434897585036693, 0.05765415374156668, 0.054369871498260336, 0.04737148251055523, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "72a6aaab-837b-4776-9ab1-8895774865e4", "fitness": 0.06713036218859637, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Diversity Preservation and Dynamic Exploitation-Exploration Balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.diversity_preservation_rate = 0.2\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Preserve diversity by reintroducing random individuals\n            num_diverse_individuals = int(self.diversity_preservation_rate * self.population_size)\n            new_population[:num_diverse_individuals] = np.random.uniform(lb, ub, (num_diverse_individuals, self.dim))\n            new_fitness[:num_diverse_individuals] = np.array([func(ind) for ind in new_population[:num_diverse_individuals]])\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 30, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06713 with standard deviation 0.06022.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406240017198535, 0.14585775119247113, 0.14896413938195086, 0.05339684449207782, 0.059408371043981334, 0.05098375341490102, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "df5e9342-862d-4086-a62b-1f8b3978a13b", "fitness": 0.06739175788783194, "name": "AdaptiveMetaheuristic", "description": "Improved strategy probability update by normalizing success rate.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = (new_fitness < fitness).mean()  # Normalize success rate\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success)\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 31, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06739 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7dfb4d88-c657-48da-9c43-151c555b41a5", "fitness": -Infinity, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Diversity Preservation and Memory-based Strategy Adjustment.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.05  # Adjusted\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.memory = []  # New memory for storing best solutions\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n                self.memory.append(best_solution)  # Storing best solutions\n\n            # Diversity preservation by injecting random individuals\n            if evaluations % (self.population_size * 2) == 0:\n                random_individuals = np.random.uniform(lb, ub, (self.population_size // 5, self.dim))\n                new_population = np.vstack((new_population, random_individuals))\n                new_fitness = np.append(new_fitness, [func(ind) for ind in random_individuals])\n\n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population[:self.population_size], new_fitness[:self.population_size]\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 32, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (48,) (40,) ').", "error": "ValueError('operands could not be broadcast together with shapes (48,) (40,) ')", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "f1c70841-cd42-4723-afb8-9fc15bc140d1", "fitness": 0.0654205625675695, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Hybrid Adaptation with Iterative Strategy Refinement and Gradient Learning for Optimized Search Efficiency", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.gradient_descent]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.local_search_intensity = 0.05\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success_rate = np.sum(new_fitness < fitness) / len(fitness)\n            self.strategy_probabilities[strategy_idx] += self.learning_rate * success_rate\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * self.local_search_intensity\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * self.local_search_intensity\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def gradient_descent(self, population, func, lb, ub):\n        gradient_step = 0.01\n        gradients = np.zeros_like(population)\n        for i, ind in enumerate(population):\n            original_value = func(ind)\n            for j in range(self.dim):\n                ind[j] += gradient_step\n                if ind[j] > ub[j]:\n                    ind[j] = ub[j]\n                elif ind[j] < lb[j]:\n                    ind[j] = lb[j]\n                new_value = func(ind)\n                gradients[i, j] = (new_value - original_value) / gradient_step\n                ind[j] -= gradient_step  # restore\n            \n            gradients[i] /= np.linalg.norm(gradients[i]) + 1e-8\n            population[i] -= self.local_search_intensity * gradients[i]\n            np.clip(population[i], lb, ub, out=population[i])\n        \n        return population", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06542 with standard deviation 0.06400.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.17431003927205135, 0.13635392572062044, 0.14293587811571473, 0.04241740120123538, 0.04772416083108588, 0.04354365796741788, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "b15d1faf-3b58-4aec-af43-d07a41a88c20", "fitness": 0.06500553671248396, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Reinforced Learning and Dynamic Neighborhood Exploration to improve convergence on various optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.dynamic_local_search]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.success_rates = np.zeros(len(self.strategy_pool))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection using reinforced learning\n            success = new_fitness < fitness\n            self.success_rates[strategy_idx] += success.mean()\n            self.strategy_probabilities = self.success_rates + 1e-2  # Avoid zero division\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def dynamic_local_search(self, population, func, lb, ub):\n        # Dynamic neighborhood exploration\n        neighborhood_size = max(1, int(0.1 * self.dim))\n        improved_population = []\n        for ind in population:\n            local_best = ind\n            local_best_fitness = func(ind)\n            for _ in range(neighborhood_size):\n                candidate = ind + np.random.uniform(-0.1, 0.1, self.dim)\n                np.clip(candidate, lb, ub, out=candidate)\n                candidate_fitness = func(candidate)\n                if candidate_fitness < local_best_fitness:\n                    local_best = candidate\n                    local_best_fitness = candidate_fitness\n            improved_population.append(local_best)\n        return np.array(improved_population)", "configspace": "", "generation": 34, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06501 with standard deviation 0.05771.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13954721974506235, 0.13846141503769815, 0.14293587811571473, 0.052372987898256396, 0.04783281368776593, 0.06239951592785831, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d7c7aebe-7434-4c19-8405-672849a07be2", "fitness": 0.06612471715496282, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Dynamic Learning Rate and Improved Strategy Pool for Comprehensive Exploration and Exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.initial_learning_rate = 0.1\n        self.strategy_pool = [\n            self.random_search, \n            self.gradient_search, \n            self.crossover_mutation,\n            self.self_adaptive_mutation\n        ]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            dynamic_lr = self.initial_learning_rate * (1 - evaluations / self.budget)\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + dynamic_lr * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def self_adaptive_mutation(self, population, func, lb, ub):\n        tau = 1.0 / np.sqrt(2.0 * np.sqrt(self.dim))\n        tau_prime = 1.0 / np.sqrt(2.0 * self.dim)\n        offspring = np.empty_like(population)\n        for i in range(self.population_size):\n            sigma = np.abs(np.random.normal(0, 0.1, self.dim))\n            new_sigma = sigma * np.exp(tau_prime * np.random.normal() + tau * np.random.normal(size=self.dim))\n            mutation = new_sigma * np.random.normal(size=self.dim)\n            offspring[i] = population[i] + mutation\n            np.clip(offspring[i], lb, ub, out=offspring[i])\n        return offspring", "configspace": "", "generation": 35, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06612 with standard deviation 0.06025.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13849384563892841, 0.14674853048192338, 0.1522284827373721, 0.051533451416435616, 0.05224584124811393, 0.052372302871892096, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "267b01a8-c88c-4741-901b-4484e0d4d075", "fitness": 0.06746370220994397, "name": "AdaptiveMetaheuristic", "description": "Enhanced strategy selection with adaptive learning rate adjustment based on success rate.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.learning_rate = max(0.01, self.learning_rate * (1 + (success.mean() - 0.5)))  # Line changed\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 36, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06746 with standard deviation 0.06238.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14507964862451983, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.050735229442571916, 0.045547922197923585, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "18c30fcf-89ef-4646-9dc3-5dc8326c3a19", "fitness": -Infinity, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Dynamic Strategy Reinforcement and Memory-Based Learning for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.memory_factor = 0.5\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.memory_based_search]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with reinforcement\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Update memory with best solutions\n            self.memory.append(best_solution)\n            if len(self.memory) > self.population_size:\n                self.memory.pop(0)\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def memory_based_search(self, population, func, lb, ub):\n        if not self.memory:\n            return self.random_search(population, func, lb, ub)\n        memory_population = np.array(self.memory)\n        perturbation = (np.random.randn(*memory_population.shape) * 0.1) * self.memory_factor\n        candidates = memory_population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates", "configspace": "", "generation": 37, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (40,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (40,) ')", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "aa0481b3-8f75-4da6-8d58-d3dda72ff4fe", "fitness": -Infinity, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Bayesian Strategy Selection and Dynamic Population Sizing", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.success_rates = np.zeros(len(self.strategy_pool))\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = self.select_strategy()\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.update_success_rates(strategy_idx, success.mean())\n            \n            # Dynamic population resizing\n            if evaluations < self.budget / 2 and np.random.rand() < 0.1:\n                self.population_size = min(self.population_size + 1, self.budget - evaluations)\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def select_strategy(self):\n        probabilities = self.strategy_probabilities * (1.0 + self.success_rates)\n        probabilities /= probabilities.sum()\n        return np.random.choice(len(self.strategy_pool), p=probabilities)\n\n    def update_success_rates(self, strategy_idx, success_rate):\n        self.success_rates[strategy_idx] = (self.success_rates[strategy_idx] + success_rate) / 2\n        self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success_rate)  \n        self.strategy_probabilities /= self.strategy_probabilities.sum()\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 38, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (41,) (40,) ').", "error": "ValueError('operands could not be broadcast together with shapes (41,) (40,) ')", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "7e4a533a-d695-416d-8da0-7159701dc6eb", "fitness": -Infinity, "name": "AdaptiveMetaheuristic", "description": "Adaptive Population-Based Search with Dynamic Strategy Selection and Enhanced Local Learning", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation * np.sign(population - best_solution)  # Slight change here\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 39, "feedback": "An exception occurred: NameError(\"name 'best_solution' is not defined\").", "error": "NameError(\"name 'best_solution' is not defined\")", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "011952f5-1119-4dca-a3bb-fc28e6cacb6f", "fitness": 0.09830453539158099, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy with Success History and Adaptive Budget Allocation.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.success_history = np.zeros(len(self.strategy_pool))  # Track success for each strategy\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            adaptive_budget = max(1, int(self.budget * (self.strategy_probabilities[strategy_idx] / 2)))\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)[:adaptive_budget]\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            success = new_fitness < fitness[:len(new_fitness)]\n            self.success_history[strategy_idx] += success.mean()\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (success.mean() - 0.5))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 40, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.09830 with standard deviation 0.12724.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.42149161891561326, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.04915336469689369, 0.048308131075749605, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "746ac059-23fd-42b4-9850-37d3206b6958", "fitness": 0.06479694740447359, "name": "AdaptiveMetaheuristic", "description": "Adaptive Search with Enhanced Strategy Balancing for Improved Exploration", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.15  # Increased learning rate for faster adaptation\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (success.mean() + 0.05))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 41, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06480 with standard deviation 0.05834.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1350767854097904, 0.1463991893523532, 0.14293587811571473, 0.059081839866765073, 0.05017392421242095, 0.04800490968321813, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7f5cd78f-c2cb-4ffb-903a-7f0171243395", "fitness": 0.06475230105690646, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Memory-Based Strategy Selection and Adaptive Learning Rates.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.strategy_memory = np.zeros(len(self.strategy_pool))\n        self.learning_rate = 0.1\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n\n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Memory-based Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_memory[strategy_idx] += success.sum() / len(success)\n            self.strategy_probabilities = self.strategy_memory / self.strategy_memory.sum()\n            \n            # Dynamic learning rate adjustment\n            self.learning_rate = max(0.01, self.learning_rate * 0.95) if success.any() else min(0.2, self.learning_rate * 1.05)\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06475 with standard deviation 0.05830.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13954721974506235, 0.13724116459456548, 0.1475724361059979, 0.052372987898256396, 0.05608455520504296, 0.04845234596323322, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "bf49b761-fce8-422d-9080-5b70cffe02c5", "fitness": -Infinity, "name": "SynergisticMetaheuristic", "description": "Synergistic Strategy Fusion with Memory and Adaptive Hyperparameters, leveraging past successes to dynamically refine search strategies.", "code": "import numpy as np\n\nclass SynergisticMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_success_memory = np.zeros(len(self.strategy_pool))\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with historical memory\n            improvement = fitness - new_fitness\n            self.strategy_success_memory[strategy_idx] += improvement.sum()\n            success_rate = self.strategy_success_memory / self.strategy_success_memory.sum()\n            self.strategy_probabilities = (1 - self.learning_rate) * self.strategy_probabilities + self.learning_rate * success_rate\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n\n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 43, "feedback": "An exception occurred: ValueError('probabilities are not non-negative').", "error": "ValueError('probabilities are not non-negative')", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "23b73ec3-c6b6-4ac5-8381-344baa673c3c", "fitness": 0.08486527902956906, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy with Adaptive Memory and Fitness-Based Selection for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.05\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.memory_size = 5\n        self.success_memory = np.zeros((len(self.strategy_pool), self.memory_size))\n        self.memory_index = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Fitness-based selection for next generation\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.hstack((fitness, new_fitness))\n            selected_indices = np.argsort(combined_fitness)[:self.population_size]\n            population, fitness = combined_population[selected_indices], combined_fitness[selected_indices]\n            \n            # Adaptive strategy selection using memory\n            success_rate = (new_fitness < fitness[:len(new_fitness)]).mean()\n            self.success_memory[strategy_idx][self.memory_index] = success_rate\n            self.memory_index = (self.memory_index + 1) % self.memory_size\n            avg_success = self.success_memory.mean(axis=1)\n            self.strategy_probabilities = np.clip(self.strategy_probabilities + self.learning_rate * (avg_success - avg_success.mean()), 0.1, 0.9)\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 44, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.08487 with standard deviation 0.08104.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.24985737590113333, 0.16127222475082004, 0.1465735603614109, 0.06597518539869562, 0.0736806104110369, 0.06492855444302492, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "5e26a9ac-911b-47be-8a30-a5cf4d429245", "fitness": 0.06686355077172448, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Historical Memory and Dynamic Learning Rate for Improved Convergence", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.base_learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.success_memory = np.zeros(len(self.strategy_pool))\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Calculate improvement and update historical memory\n            improvement = fitness - new_fitness\n            self.success_memory[strategy_idx] *= 0.9  # decay factor\n            self.success_memory[strategy_idx] += improvement.mean()\n            \n            # Adjust learning rate based on historical performance\n            dynamic_learning_rate = self.base_learning_rate * (1 + self.success_memory[strategy_idx])\n            \n            # Adaptive strategy selection with dynamic learning rate\n            success = improvement > 0\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + dynamic_learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 45, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06686 with standard deviation 0.06116.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1359887518322659, 0.1624167065600619, 0.14293587811571473, 0.04762025014455873, 0.06391323113275749, 0.04739713916016175, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "4c6e9340-82f2-4df7-8b52-07edc998b719", "fitness": 0.06708624317676581, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Search with Rank-Based Strategy Adjustment and Nonlinear Perturbation for Improved Convergence.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Rank-based adaptive strategy selection\n            ranks = np.argsort(new_fitness)\n            success = ranks < ranks.mean()\n            adjustment = self.learning_rate * (success - 0.5)\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + adjustment.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * (0.5 / (1 + np.arange(self.population_size)[:, None]))\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.tanh(np.random.randn(self.dim) * 0.05)\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 46, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06709 with standard deviation 0.06272.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14827931906407554, 0.1611229637237951, 0.14293587811571473, 0.054618320617797145, 0.04932257460645817, 0.045997132463051815, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c05f0c99-fa3d-4a41-b988-e67b037b1bd2", "fitness": -Infinity, "name": "ImprovedAdaptiveMetaheuristic", "description": "Adaptive Hybrid Metaheuristic with Enhanced Strategy Pool, Dynamic Resource Allocation, and Contextual Learning for Improved Black Box Optimization.", "code": "import numpy as np\n\nclass ImprovedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [\n            self.random_search,\n            self.gradient_search,\n            self.crossover_mutation,\n            self.particle_swarm,\n            self.simulated_annealing\n        ]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with contextual learning\n            improvement = fitness - new_fitness\n            self.strategy_probabilities[strategy_idx] = max(\n                0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * improvement.mean()\n            )\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Dynamic resource allocation\n            adapt_population_size = int(self.population_size * (1 + improvement.mean()))\n            self.population_size = max(5, min(self.budget - evaluations, adapt_population_size))\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def particle_swarm(self, population, func, lb, ub):\n        velocities = np.random.randn(*population.shape) * 0.1\n        personal_best = population.copy()\n        personal_best_fitness = np.array([func(ind) for ind in personal_best])\n        \n        for _ in range(5):  # Limited iterations for budget constraint\n            r1, r2 = np.random.rand(), np.random.rand()\n            cognitive = r1 * (personal_best - population)\n            social = r2 * (personal_best[np.argmin(personal_best_fitness)] - population)\n            velocities = 0.7 * velocities + cognitive + social\n            population += velocities\n            np.clip(population, lb, ub, out=population)\n            fitness = np.array([func(ind) for ind in population])\n            \n            improved = fitness < personal_best_fitness\n            personal_best[improved] = population[improved]\n            personal_best_fitness[improved] = fitness[improved]\n        \n        return personal_best\n    \n    def simulated_annealing(self, population, func, lb, ub):\n        temperature = 100.0\n        cooling = 0.95\n        new_population = population.copy()\n        \n        for i in range(len(population)):\n            candidate = population[i] + np.random.randn(self.dim) * 0.1\n            np.clip(candidate, lb, ub, out=candidate)\n            candidate_fitness = func(candidate)\n            if candidate_fitness < func(population[i]) or np.exp(-(candidate_fitness - func(population[i])) / temperature) > np.random.rand():\n                new_population[i] = candidate\n            temperature *= cooling\n            \n        return new_population", "configspace": "", "generation": 47, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (40,) (5,) ').", "error": "ValueError('operands could not be broadcast together with shapes (40,) (5,) ')", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "38fb6e47-7233-4ad3-8a08-182e5364a35f", "fitness": 0.0689475125418482, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy with Memory-based Strategy Selection and Fine-tuned Local Learning", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.15  # Increase learning rate for faster adaptation\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.strategy_memory = np.zeros(len(self.strategy_pool))  # Initialize memory for strategy success\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with memory\n            success = new_fitness < fitness\n            self.strategy_memory[strategy_idx] += success.mean()\n            self.strategy_probabilities = np.exp(self.strategy_memory) / np.sum(np.exp(self.strategy_memory))\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.05  # Fine-tune perturbation scale\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 48, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06895 with standard deviation 0.06393.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.17603088589633875, 0.1400500888024393, 0.14293587811571473, 0.052372987898256396, 0.0526936492038006, 0.05494412296008411, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "b9fb26d5-d187-455b-91da-fec6242e102c", "fitness": 0.06607512251269161, "name": "AdaptiveMetaheuristic", "description": "Enhanced AdaptiveMetaheuristic with improved gradient search by scaling the perturbation dynamically based on the dimensionality.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * (0.1 / np.sqrt(self.dim))\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 49, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06608 with standard deviation 0.06095.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13575755120940125, 0.1611229637237951, 0.14293587811571473, 0.056774362340562434, 0.05090784276085225, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "62e21c7a-3567-4a91-8a05-9e4c21ac3458", "fitness": 0.06651313477562813, "name": "HybridDynamicStrategy", "description": "Hybrid Dynamic Strategy with Adaptive Differential Evolution and Enhanced Local Search to Improve Convergence and Exploration.", "code": "import numpy as np\n\nclass HybridDynamicStrategy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 3)\n        self.learning_rate = 0.05\n        self.strategy_pool = [\n            self.random_search,\n            self.gradient_search,\n            self.crossover_mutation,\n            self.differential_evolution\n        ]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_fitness)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, \n                    self.strategy_probabilities[strategy_idx] + self.learning_rate * (success.mean() - 0.5))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.05\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.02\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def differential_evolution(self, population, func, lb, ub):\n        F = 0.8  # Differential weight\n        CR = 0.9  # Crossover probability\n        offspring = []\n        for i in range(self.population_size):\n            indices = np.random.choice(range(self.population_size), 3, replace=False)\n            x0, x1, x2 = population[indices]\n            mutant_vector = np.clip(x0 + F * (x1 - x2), lb, ub)\n            trial_vector = np.where(np.random.rand(self.dim) < CR, mutant_vector, population[i])\n            offspring.append(trial_vector)\n        return np.array(offspring)", "configspace": "", "generation": 50, "feedback": "The algorithm HybridDynamicStrategy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06651 with standard deviation 0.05966.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.147817012888517, 0.1415401595210367, 0.1455410461848885, 0.05642598397763254, 0.052371253696696374, 0.053422756711882236, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "5d2f128b-645b-410e-9e0e-ecdc5e0b2ec1", "fitness": 0.06427894465002239, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced AdaptiveMetaheuristic with Adaptive Learning Rates and Historical Knowledge Integration for Improved Black Box Optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.history = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n\n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n\n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.history.append(success.mean())\n            avg_success = np.mean(self.history[-5:]) if len(self.history) >= 5 else success.mean()\n            adaptive_learning_rate = self.learning_rate * (1 + avg_success)\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + adaptive_learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n\n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n\n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n\n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n\n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06428 with standard deviation 0.05839.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13451913801217763, 0.1463991893523532, 0.14293587811571473, 0.05893511916019589, 0.046216267526542065, 0.04800490968321813, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "86eede4c-9b47-44ad-ba48-23545ef542ac", "fitness": -Infinity, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy with Fitness-based Weight Adjustment.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean()) * (1 - new_fitness.mean()/fitness.mean()) \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 52, "feedback": "An exception occurred: ValueError('probabilities are not non-negative').", "error": "ValueError('probabilities are not non-negative')", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "06f11f59-bf44-4b81-91c4-40cd7badbc87", "fitness": 0.07154953741766229, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Self-adaptive Learning Rates and Optimized Strategy Selection Probabilities", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.base_learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.learning_rates = np.full(len(self.strategy_pool), self.base_learning_rate)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            success_rate = success.mean()\n            self.learning_rates[strategy_idx] *= (1 + 0.1 * success_rate - 0.1 * (1 - success_rate))\n            self.strategy_probabilities[strategy_idx] += self.learning_rates[strategy_idx] * success_rate\n            self.strategy_probabilities = np.maximum(self.strategy_probabilities, 0.1)\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 53, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07155 with standard deviation 0.06745.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1779584579524529, 0.1624167065600619, 0.14300283061930485, 0.05893511916019589, 0.05382670033371206, 0.04630602213323309, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "cb95270b-0ce9-4f10-b6b6-9d9bf928e431", "fitness": 0.06475230105690646, "name": "AdaptiveMetaheuristic", "description": "Improved Adaptive Strategy Selection using Success History and Dynamic Mutation.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.history_success = np.zeros(len(self.strategy_pool))  # Track success\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.history_success[strategy_idx] += success.sum()  # Track cumulative success\n            self.strategy_probabilities = self.history_success / self.history_success.sum()  # Dynamic adjustment\n\n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 54, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06475 with standard deviation 0.05830.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13954721974506235, 0.13724116459456548, 0.1475724361059979, 0.052372987898256396, 0.05608455520504296, 0.04845234596323322, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "56bfa571-ad84-4b23-ba2b-a312f6688dfb", "fitness": 0.06329073666407686, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy with Improved Perturbation Control.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * (0.05 + 0.05 * np.random.rand())\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 55, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06329 with standard deviation 0.05812.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13954830831091114, 0.13768975200558486, 0.14442081541521756, 0.04687675907151867, 0.05080143488363209, 0.04877956028982766, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "7505de84-8c2a-4d61-8392-4d00a980e0eb", "fitness": -Infinity, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Self-Adaptive Learning Rate and Local Optima Escape Strategies.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.learning_rate_adjustment = 0.95\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n                self.learning_rate *= self.learning_rate_adjustment\n            else:\n                self.learning_rate /= self.learning_rate_adjustment\n            \n            # Adaptive strategy selection and adjustment\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation with diversity control\n            population, fitness = self.diversity_control(new_population, new_fitness, lb, ub)\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def diversity_control(self, population, fitness, lb, ub):\n        # Introduce diversity by reinitializing part of the population\n        diversity_threshold = 0.1\n        new_population = population.copy()\n        if np.std(fitness) < diversity_threshold:\n            num_to_replace = int(self.population_size * 0.2)\n            indices_to_replace = np.random.choice(self.population_size, num_to_replace, replace=False)\n            new_population[indices_to_replace] = np.random.uniform(lb, ub, (num_to_replace, self.dim))\n        new_fitness = np.array([func(ind) for ind in new_population])\n        return new_population, new_fitness", "configspace": "", "generation": 56, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "75d31fd7-e9f8-4f33-928b-53756b0b9c17", "fitness": 0.06810679587650345, "name": "AdaptiveMetaheuristic", "description": "Slightly increase the learning rate for adaptive strategy selection to enhance convergence speed.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.15  # Adjusted learning rate\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 57, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06811 with standard deviation 0.06375.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.16896979905050724, 0.1463991893523532, 0.14293587811571473, 0.05893511916019589, 0.046216267526542065, 0.04800490968321813, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "b983252e-1ec7-4294-8709-bb1eac6c59ad", "fitness": 0.06693144606247275, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy with Reward-Based Probability Adjustment for Dynamic Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with rewards\n            success = new_fitness < fitness\n            rewards = (fitness - new_fitness) / np.abs(fitness + 1e-9)  # Reward calculation\n            avg_reward = np.mean(rewards)\n            self.strategy_probabilities[strategy_idx] += self.learning_rate * avg_reward  # Reward-based adjustment\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx])  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 58, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06693 with standard deviation 0.06202.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13952513876757666, 0.1624167065600619, 0.14535219395732546, 0.05893511916019589, 0.04915336469689369, 0.045500491420201405, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "92c7fed5-8726-4773-a8de-8fee52506c07", "fitness": 0.06702272665352155, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy with Diversity Control and Adaptive Learning Rate for Efficient Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.diversity_threshold = 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with diversity control\n            improvement = new_fitness < fitness\n            diversity = np.mean(np.std(new_population, axis=0))\n            success_rate = improvement.mean()\n            if diversity < self.diversity_threshold:\n                self.learning_rate = min(self.learning_rate + 0.05, 1.0) # Increase learning rate if diversity is low\n            else:\n                self.learning_rate = max(self.learning_rate - 0.05, 0.01) # Decrease learning rate if diversity is sufficient\n                \n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success_rate)\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 59, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06702 with standard deviation 0.06157.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13952513876757666, 0.1624167065600619, 0.14293587811571473, 0.05904289539343155, 0.04915336469689369, 0.04863055634801561, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "652ad3c4-e7ff-4157-882c-27dc1340da3e", "fitness": 0.0673711446863818, "name": "AdaptiveMetaheuristic", "description": "Refined Adaptive Strategy with Boosted Probability Learning for Enhanced Convergence", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean() * 1.1)  # Boosted learning rate\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 60, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06737 with standard deviation 0.06057.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13364206119382438, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.05097133423587963, 0.05593920291175991, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "6baead72-5e5d-4c49-83cb-283b569b5f76", "fitness": 0.06736552402190886, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy Adjustment with Probability Decay for Efficient Exploration", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean() * 0.9)  # Added decay factor\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 61, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06737 with standard deviation 0.06224.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.050735229442571916, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "afd607ac-275b-486a-8213-c2b8f1ec985e", "fitness": 0.0677402309909253, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with an Improved Dynamic Strategy Selection Scheme.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            self.strategy_probabilities += 0.01  # Additional line to slightly adjust probabilities\n            self.strategy_probabilities /= self.strategy_probabilities.sum()  # Normalize again\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 62, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06774 with standard deviation 0.06230.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14507964862451983, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05229891600105574, 0.046472994668271705, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "e4c30839-f4ea-484f-9a67-2c2262e28562", "fitness": 0.06409049156808096, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Improved Strategy Selection and Dynamic Scaling for Population and Mutation", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 3)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.enhanced_gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.scaling_factor = 0.5\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, fitness, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with dynamic scaling\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Dynamic scaling adjustment based on performance\n            self.scaling_factor *= 0.95 if success.mean() < 0.5 else 1.05\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, fitness, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def enhanced_gradient_search(self, population, fitness, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * self.scaling_factor\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, fitness, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * self.scaling_factor * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 63, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06409 with standard deviation 0.05839.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1390089220223698, 0.14241713671933587, 0.14293587811571473, 0.04917000725760989, 0.05349174122617106, 0.04829073877152745, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "42e397db-5442-4d86-9dfa-28c7e3ecb94a", "fitness": 0.06670896509895098, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic using Biased Strategy Adaptation with Memory-Based Learning for Efficient Exploration and Exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.strategy_success_memory = np.zeros(len(self.strategy_pool))\n        self.memory_decay = 0.95\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with memory\n            improvement = fitness - new_fitness\n            success_rate = np.mean(improvement > 0)\n            self.strategy_success_memory[strategy_idx] = self.memory_decay * self.strategy_success_memory[strategy_idx] + (1 - self.memory_decay) * success_rate\n            self.strategy_probabilities = np.exp(self.strategy_success_memory) / np.sum(np.exp(self.strategy_success_memory))\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 64, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06671 with standard deviation 0.06167.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13952513876757666, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.049168240638636895, 0.045899602648372895, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "1d005df8-cbab-4812-a9cb-fa3762fe270c", "fitness": 0.06863461959166045, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Self-Adaptive Mutation Scaling and Diversity Preservation for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.mutation_scaling = 0.1\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Self-adaptive mutation scaling\n            self.mutation_scaling = max(0.01, self.mutation_scaling * (1.2 if success.mean() > 0.5 else 0.9))\n            \n            # Diversity preservation\n            diversity = np.std(population, axis=0).mean()\n            if diversity < 0.1:\n                population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                continue\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * self.mutation_scaling\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * self.mutation_scaling\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 65, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06863 with standard deviation 0.06374.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.15625471827671267, 0.1611229637237951, 0.14303704973392928, 0.056216940205522814, 0.05192903787381309, 0.04765086651117134, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "d00771d7-aa89-4007-9f9c-6c5ecdb21f9e", "fitness": 0.06436331946852934, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Search with Hybrid Exploration-Exploitation Balancing", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.hybrid_search]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (2 * success.mean() - 1))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def hybrid_search(self, population, func, lb, ub):\n        hybrid_population = self.crossover_mutation(population, func, lb, ub)\n        return self.gradient_search(hybrid_population, func, lb, ub)", "configspace": "", "generation": 66, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06436 with standard deviation 0.05861.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13733381780287068, 0.14348704029275272, 0.14493783936796556, 0.04913174669825349, 0.05588298105428258, 0.046996450000639234, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "bae3e718-8fab-487b-91c7-4f03c92e99e4", "fitness": 0.06688395835651217, "name": "EnhancedAdaptiveMetaheuristic", "description": "Introduce a multi-faceted learning mechanism with a history-based diversification strategy to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.history = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n\n            # History-based diversification\n            self.history.append((new_fitness.min(), strategy_idx))\n            if len(self.history) > self.population_size:\n                self.history.pop(0)\n            self._adjust_learning_rate()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def _adjust_learning_rate(self):\n        if len(self.history) < 2:\n            return\n        if self.history[-1][0] > self.history[-2][0]:\n            self.learning_rate *= 0.9\n        else:\n            self.learning_rate = min(0.2, self.learning_rate * 1.1)", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06688 with standard deviation 0.06130.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13754016744984032, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05229891600105574, 0.04630602213323309, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "0f8daf63-54ca-45d6-ab1c-b5994eed2cbb", "fitness": 0.06739175788783194, "name": "AdaptiveMetaheuristic", "description": "Improved Adaptive Strategy Selection through Enhanced Probability Update Mechanics.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = min(0.9, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 68, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06739 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "4203fd12-a776-420e-9920-d81530b32c69", "fitness": 0.06734258873536908, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced adaptive metaheuristic with improved local search and strategy selection refinement.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.improved_gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with reward decay for less effective strategies\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (success.mean() - 0.5))\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def improved_gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.05\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 69, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06734 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1455935009009126, 0.1611229637237951, 0.14293587811571473, 0.05857047739217114, 0.049195378106238086, 0.04716510037949029, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "504464a8-519c-42be-9051-1a5798de7c59", "fitness": 0.06739175788783194, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy with Momentum Mechanism for Improved Convergence.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.momentum = np.zeros(dim)  # Added line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n                self.momentum = 0.9 * self.momentum + 0.1 * (new_population[new_best_idx] - best_solution)  # Added line\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1 + self.momentum  # Modified line\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 70, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06739 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c7c66e3b-0593-465c-93ec-4a7c116b6a57", "fitness": 0.062172184022188, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Improved Strategy Selection and Diversity Maintenance", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.diversity_enhancement]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.sum() / len(success))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def diversity_enhancement(self, population, func, lb, ub):\n        mean_point = np.mean(population, axis=0)\n        return np.clip(mean_point + np.random.randn(*population.shape) * 0.1, lb, ub)", "configspace": "", "generation": 71, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06217 with standard deviation 0.05746.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13572657090042728, 0.13610600980164456, 0.1442760698826987, 0.04528518616018007, 0.05116791363984219, 0.045487905814899365, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "b1358ce2-b36a-4261-a7bc-ca2d661f8e0d", "fitness": 0.06545698235100122, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Improved Local Search and Dynamic Strategy Tuning", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.15  # Increased learning rate for faster adaptation\n        self.strategy_pool = [self.random_search, self.enhanced_local_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (2 * success.mean() - 1))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def enhanced_local_search(self, population, func, lb, ub):\n        perturbation = np.random.laplace(0, 0.05, population.shape)\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.laplace(0, 0.03, self.dim)  # Using Laplace distribution for mutation\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 72, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06546 with standard deviation 0.05940.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13494650739950131, 0.1498385313576559, 0.14674739821595995, 0.05272838709153427, 0.052196741865371155, 0.05115527522898855, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "cde54f17-ecff-4b5f-b300-1293bdeca819", "fitness": 0.0673711446863818, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Improved Strategy Selection and Memory of Best Solutions", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.best_so_far = None  # Memory to store best solutions\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        self.best_so_far = best_solution if self.best_so_far is None else self.best_so_far\n        \n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n                self.best_so_far = best_solution  # Update memory\n            \n            # Enhanced adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities *= 0.9  # Decay probabilities slightly\n            self.strategy_probabilities[strategy_idx] += self.learning_rate * success.mean()\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n\n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 73, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06737 with standard deviation 0.06057.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13364206119382438, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.05097133423587963, 0.05593920291175991, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "e46471f6-eac0-443c-8156-3f31c09f1a11", "fitness": 0.06671742724010599, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Population-Based Search with Dynamic Strategy Selection and Adaptive Mutation for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.mutation_strength = 0.05\n        self.mutation_decay = 0.99  # Decay factor for mutation strength\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Decay mutation strength\n            self.mutation_strength *= self.mutation_decay\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * self.mutation_strength\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 74, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06672 with standard deviation 0.06139.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13783502362119504, 0.1624167065600619, 0.14293587811571473, 0.058831573926575675, 0.0510692397089797, 0.04586842322842699, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "add13d96-b8a3-40e4-b113-fdce2b6739c4", "fitness": 0.06481077352176126, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Dynamic Adaptive Learning and Enhanced Crossover-Mutation Strategy", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.enhanced_crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            adaptive_factor = self.learning_rate * (1.0 + success.mean() - (best_fitness - np.min(fitness)) / best_fitness)\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + adaptive_factor)  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def enhanced_crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation_strength = np.random.rand() * 0.1\n            mutation = np.random.randn(self.dim) * mutation_strength\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 75, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06481 with standard deviation 0.05764.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13744076569549224, 0.13854269905035976, 0.14478509256542893, 0.05511989700615261, 0.05532574897787379, 0.05058275840054416, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "ed27460b-7c63-4218-b44b-3c1e6c584b97", "fitness": 0.06655349166685813, "name": "AdaptiveMetaheuristic", "description": "Enhanced Local Learning with a small adjustment in gradient perturbation for better exploration.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.12  # Changed from 0.1 to 0.12\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 76, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06655 with standard deviation 0.06096.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1365480977031488, 0.1611229637237951, 0.14293898593612753, 0.056555927168054576, 0.05463794600669836, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "df4d6717-645f-4959-a9ae-e2a2de61c483", "fitness": 0.06464329813934072, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Improved Strategy Selection and Learning Mechanism", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.2  # Increased the learning rate for faster adaptation\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.differential_evolution]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            improvement = fitness - new_fitness\n            avg_improvement = improvement.mean()\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (avg_improvement > 0))\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def differential_evolution(self, population, func, lb, ub):\n        offspring = []\n        for i in range(self.population_size):\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = population[idxs]\n            mutant = a + 0.8 * (b - c)\n            np.clip(mutant, lb, ub, out=mutant)\n            offspring.append(mutant)\n        return np.array(offspring)", "configspace": "", "generation": 77, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06464 with standard deviation 0.05891.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13876793087387762, 0.1432122088793505, 0.1457872458580055, 0.04474617927267088, 0.05605648532988783, 0.05171963304027427, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "594adeee-e5e2-4145-846c-808c6342c9cd", "fitness": 0.0667005791132273, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Self-Adjusting Population Size and Adaptive Learning Rate for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.adjust_factor = 1.05\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = self.base_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n                \n            # Dynamic population size adjustment\n            if new_fitness[new_best_idx] < fitness.mean():\n                population_size = min(int(population_size * self.adjust_factor), self.budget - evaluations)\n            else:\n                population_size = max(int(population_size / self.adjust_factor), 5)\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Adaptive learning rate adjustment\n            self.learning_rate *= self.adjust_factor if success.any() else 1 / self.adjust_factor\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, (len(population), self.dim))\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(len(population)):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 78, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06670 with standard deviation 0.06088.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1344062126474136, 0.1624167065600619, 0.14293587811571473, 0.05973605095469292, 0.05299913208364593, 0.046311231657516716, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "c794af79-588c-4ad8-a500-b53629af2a92", "fitness": 0.06479694740447359, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy Selection with Success-Based Weight Adjustments.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + 2 * self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 79, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06480 with standard deviation 0.05834.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1350767854097904, 0.1463991893523532, 0.14293587811571473, 0.059081839866765073, 0.05017392421242095, 0.04800490968321813, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "077e0a3a-68a0-4ef9-91b9-4fb62fa440b5", "fitness": 0.06714682844143753, "name": "AdaptiveMetaheuristic", "description": "Enhanced adaptive strategy by tuning learning rate for improved convergence.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.05  # Changed from 0.1 to 0.05\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 80, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06715 with standard deviation 0.06183.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14128445724000604, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.05028245243045093, 0.046966842466508396, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "dcc65afa-9dbe-4ce6-b3ff-89cfb6747f5b", "fitness": 0.06586373113564681, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Dynamic Learning Rates and Elite Preservation to Improve Search Efficiency and Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 3)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.best_solution = None\n        self.best_fitness = float('inf')\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update and preserve elite solution\n            self.update_best_solution(new_population, new_fitness)\n            \n            # Adaptive strategy selection with dynamic learning rate\n            success_rate = (new_fitness < fitness).mean()\n            self.strategy_probabilities[strategy_idx] += self.learning_rate * success_rate\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation with elite preservation\n            elite_indices = np.argsort(fitness)[:max(1, self.population_size // 10)]\n            elite_population = population[elite_indices]\n            elite_fitness = fitness[elite_indices]\n            population = np.vstack((new_population, elite_population))\n            fitness = np.hstack((new_fitness, elite_fitness))\n            best_idx = np.argmin(fitness)\n            population = population[np.argsort(fitness)][:self.population_size]\n            fitness = fitness[np.argsort(fitness)][:self.population_size]\n        \n        return self.best_solution\n\n    def update_best_solution(self, population, fitness):\n        best_idx = np.argmin(fitness)\n        if fitness[best_idx] < self.best_fitness:\n            self.best_fitness = fitness[best_idx]\n            self.best_solution = population[best_idx].copy()\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 81, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06586 with standard deviation 0.05913.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1427928127352347, 0.14388293869772273, 0.14434397571267554, 0.05056339498129292, 0.056362243389777356, 0.05332821470411819, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "5a7a6002-7083-45c2-a343-3457c9557174", "fitness": 0.06739175788783194, "name": "AdaptiveMetaheuristic", "description": "Enhanced adaptive strategy selection by incorporating a feedback mechanism based on recent performance trends to improve convergence.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.recent_success = np.zeros(len(self.strategy_pool))  # Track success rates\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            self.recent_success[strategy_idx] = 0.9 * self.recent_success[strategy_idx] + 0.1 * success.mean()  # Update recent success\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 82, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06739 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "0fc243da-8888-43d6-abf8-cf32057bade9", "fitness": 0.0635267621431791, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy with Competitive Co-evolutionary Dynamics for Optimized Search Efficiency.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.2  # Adjusted learning rate\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.elite_perturbation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Competitive co-evolutionary dynamics\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.15\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def elite_perturbation(self, population, func, lb, ub):\n        elite_idx = np.argmin([func(ind) for ind in population])\n        elite = population[elite_idx].copy()\n        perturbation = np.random.randn(self.dim) * 0.02\n        elite_perturbed = elite + perturbation\n        np.clip(elite_perturbed, lb, ub, out=elite_perturbed)\n        return np.array([elite_perturbed for _ in range(self.population_size)])", "configspace": "", "generation": 83, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06353 with standard deviation 0.05549.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.12428402567164665, 0.13804920536051724, 0.14293587811571473, 0.05491655728961775, 0.05837147861737424, 0.05168371423374141, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "f727af87-bf23-4c26-a2ee-a79d07e85937", "fitness": 0.06606593796599282, "name": "AdaptiveMetaheuristic", "description": "Improved Adaptive Strategy Selection and Enhanced Mutation for Robust Convergence.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (2 * success.mean() - 1))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.1  # Increased mutation strength\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 84, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06607 with standard deviation 0.06110.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1364106155051139, 0.1611229637237951, 0.14328413485192382, 0.056481069483724444, 0.04921809231616425, 0.04657656581321401, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "df1eb8fa-5d6a-4eb8-8ad2-9ce4e5c5917e", "fitness": 0.06739175788783194, "name": "AdaptiveMetaheuristic", "description": "Incremental adaptation of strategy probabilities to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            incremental_update = self.learning_rate * np.mean(success)\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + incremental_update)\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 85, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06739 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "a2a06210-3c2d-46c8-839a-c06f0a44079d", "fitness": 0.06739175788783194, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Memory-Based Strategy Adjustment and Diversity Promotion.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.memory_weight = 0.5\n        self.diversity_threshold = 0.2\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        diversity_measures = []\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with memory\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Update population diversity\n            diversity = np.mean(np.std(new_population, axis=0))\n            diversity_measures.append(diversity)\n            if len(diversity_measures) > 5:\n                diversity_measures.pop(0)\n            \n            if np.mean(diversity_measures) < self.diversity_threshold:\n                population = self.random_search(population, func, lb, ub)\n            else:\n                population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 86, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06739 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "5ecfab0d-8323-445e-877f-b1bcde48d1a6", "fitness": 0.06728837687087211, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic using Self-Adaptive Strategy Probabilities and Momentum-Based Learning for Improved Convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.05\n        self.momentum = 0.9\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.strategy_momentum = np.zeros(len(self.strategy_pool))\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with momentum\n            success = new_fitness < fitness\n            success_rate = success.mean()\n            self.strategy_momentum[strategy_idx] = (self.momentum * self.strategy_momentum[strategy_idx] + \n                                                    (1 - self.momentum) * success_rate)\n            self.strategy_probabilities[strategy_idx] += self.learning_rate * self.strategy_momentum[strategy_idx]\n            self.strategy_probabilities = np.clip(self.strategy_probabilities, 0.1, 1.0)  # Prevent zeroing out\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 87, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06729 with standard deviation 0.06151.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13952513876757666, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.053315706767791515, 0.046966842466508396, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "57876eec-6866-4eb5-a4d9-719e442ba35a", "fitness": 0.06479694740447359, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Improved Strategy Probability Update for Better Convergence.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + 0.2 * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 88, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06480 with standard deviation 0.05834.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1350767854097904, 0.1463991893523532, 0.14293587811571473, 0.059081839866765073, 0.05017392421242095, 0.04800490968321813, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "9f9201e8-33af-4d59-b4be-2f4d604451f6", "fitness": 0.06634737267684078, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Diversity Preservation through Dynamic Niching and Adaptive Mutation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, 2 * dim)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.diversity_threshold = 0.1  # Threshold for maintaining population diversity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n\n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n\n            # Update strategy probabilities\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, \n                self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n\n            # Diversity preservation\n            if self.measure_diversity(population) < self.diversity_threshold:\n                population = self.introduce_diversity(population, lb, ub)\n\n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n\n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n\n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n\n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * (0.05 + 0.1 * np.random.rand())  # Adaptive mutation\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def measure_diversity(self, population):\n        pairwise_distances = np.linalg.norm(population[:, np.newaxis] - population, axis=2)\n        avg_distance = np.mean(pairwise_distances)\n        return avg_distance\n\n    def introduce_diversity(self, population, lb, ub):\n        num_new_individuals = self.population_size // 2\n        new_individuals = np.random.uniform(lb, ub, (num_new_individuals, self.dim))\n        return np.vstack((population, new_individuals))", "configspace": "", "generation": 89, "feedback": "The algorithm EnhancedAdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06635 with standard deviation 0.05920.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14494800218595505, 0.13966193622197265, 0.14715044613849948, 0.05998397524369392, 0.052166604891251356, 0.05171538941019471, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "9e699516-7c1a-4fc5-bf29-5a6b97a02b25", "fitness": 0.06773177868924528, "name": "AdaptiveMetaheuristic", "description": "Enhancing Adaptive Metaheuristic with a dynamic mutation scaling factor to improve exploration.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        mutation_scale = 0.05 + (0.5 - 0.05) * np.random.rand()  # Dynamic mutation scale\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * mutation_scale\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 90, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06773 with standard deviation 0.06307.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.15102926811042494, 0.1614595116314489, 0.14293587811571473, 0.04904882728548088, 0.05274362247784958, 0.050868900582288656, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "71d92683-02ff-494c-9790-8acaa9ec4229", "fitness": 0.06701234593246182, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Strategy Selection with Re-evaluation and Reinforcement Learning for Precision Optimization.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (success.mean() + 0.1))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 91, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06701 with standard deviation 0.06069.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13374576523561799, 0.1624167065600619, 0.14293587811571473, 0.05893511916019589, 0.04763844140880613, 0.05593920291175991, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "52fd2bac-c164-4b26-949f-f1967647774d", "fitness": -Infinity, "name": "AdaptiveMetaheuristic", "description": "Enhanced strategy selection with memory-based learning and adaptive mutation rate.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.memory = np.zeros(len(self.strategy_pool))  # Line changed\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.memory[strategy_idx] = success.mean()  # Line changed\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * self.memory[strategy_idx])  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 92, "feedback": "An exception occurred: AttributeError(\"'AdaptiveMetaheuristic' object has no attribute 'strategy_pool'\").", "error": "AttributeError(\"'AdaptiveMetaheuristic' object has no attribute 'strategy_pool'\")", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "5b07912f-9825-4510-b3ed-0d919f338821", "fitness": 0.0669851335404957, "name": "AdaptiveMetaheuristic", "description": "Refinement of strategy probabilities and mutation scaling for improved exploration.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * (success.mean() - 0.5))  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.08\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 93, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06699 with standard deviation 0.06275.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14461823578329625, 0.161458916750427, 0.14597906884761347, 0.05760342785070949, 0.04747330410944495, 0.04423324852297039, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "2d235f43-8dbd-4b10-a876-634c3dcc8fb9", "fitness": -Infinity, "name": "EnhancedAdaptiveMetaheuristic", "description": "Enhanced Adaptive Metaheuristic with Dynamic Learning Rates and Fitness-based Selection", "code": "import numpy as np\n\nclass EnhancedAdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        self.dynamic_lr = 0.1  # Initial dynamic learning rate\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            # Select strategy based on fitness-based probability\n            fitness_based_probs = self.strategy_probabilities * (1 - fitness/fitness.sum())\n            fitness_based_probs /= fitness_based_probs.sum()\n            \n            strategy_idx = np.random.choice(len(self.strategy_pool), p=fitness_based_probs)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection with dynamic learning rate\n            success = new_fitness < fitness\n            self.dynamic_lr = min(0.5, self.dynamic_lr + 0.01 * success.mean())\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.dynamic_lr * success.mean())\n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 94, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (3,) (40,) ').", "error": "ValueError('operands could not be broadcast together with shapes (3,) (40,) ')", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {}}
{"id": "383ec66c-21e4-42af-a022-378000528bda", "fitness": 0.06927575002639452, "name": "AdaptiveMetaheuristic", "description": "Adaptive Population-Based Search with Dynamic Strategy Selection and Probability Normalization Enhancement", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum() if self.strategy_probabilities.sum() != 0 else 1\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 95, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06928 with standard deviation 0.06473.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1610223919132917, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "78a07cba-66be-480a-a244-9827f5f56238", "fitness": 0.07096266330022577, "name": "AdaptiveMetaheuristic", "description": "Improved Adaptive Metaheuristic with Enhanced Diversity and Memory-based Learning for Better Convergence.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.memory_based_search]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        memory = []\n\n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub, memory)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            population, fitness = new_population, new_fitness\n            memory.append(best_solution)\n            if len(memory) > 5:\n                memory.pop(0)\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub, memory):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub, memory):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub, memory):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n\n    def memory_based_search(self, population, func, lb, ub, memory):\n        if memory:\n            mem_best = np.array(memory).mean(axis=0)\n            candidates = mem_best + np.random.randn(*population.shape) * 0.1\n            np.clip(candidates, lb, ub, out=candidates)\n            return candidates\n        else:\n            return self.random_search(population, func, lb, ub, memory)", "configspace": "", "generation": 96, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.07096 with standard deviation 0.05833.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1347808656572429, 0.14872931446976256, 0.14410334081382092, 0.06567984328359522, 0.06130784471835471, 0.08256276075925573, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "2719798f-c48a-4a4d-989b-c807bd49737e", "fitness": 0.06385157046041844, "name": "AdaptiveMetaheuristic", "description": "Refining the AdaptiveMetaheuristic with enhanced crossover and mutation dynamics.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.enhanced_crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def enhanced_crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            alpha = np.random.uniform(0, 1)  # New dynamic crossover\n            child = alpha * parents[0] + (1 - alpha) * parents[1]\n            mutation = np.random.laplace(0, 0.05, self.dim)  # New Laplace mutation\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 97, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06385 with standard deviation 0.05793.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.13470759020485823, 0.1362904730075537, 0.1497535402686434, 0.04981570234169297, 0.05300800246371884, 0.04958882585729896, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "a81d9edc-6abd-42f3-b470-3cb7f7a176ca", "fitness": 0.06739175788783194, "name": "AdaptiveMetaheuristic", "description": "Modified Adaptive Population-Based Search with Enhanced Diversity through Gaussian Mutation Control for Improved Exploration", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.mutation_scale = 0.05  # Added mutation scale as a parameter\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.1, self.strategy_probabilities[strategy_idx] + self.learning_rate * success.mean())  \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * self.mutation_scale  # Use the new mutation scale\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)", "configspace": "", "generation": 98, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06739 with standard deviation 0.06223.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.14406646266622847, 0.1624167065600619, 0.14293587811571473, 0.05895793494870394, 0.05097133423587963, 0.045677504463898955, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
{"id": "4bceef08-1b8e-49e4-a556-5709763aba5a", "fitness": 0.06317596084711893, "name": "AdaptiveMetaheuristic", "description": "Enhanced Adaptive Population-Based Search with Dynamic Strategy Selection and Improved Local Learning.", "code": "import numpy as np\n\nclass AdaptiveMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, dim * 2)\n        self.learning_rate = 0.1\n        self.strategy_pool = [self.random_search, self.gradient_search, self.crossover_mutation, self.self_adaptive_mutation]\n        self.strategy_probabilities = np.ones(len(self.strategy_pool)) / len(self.strategy_pool)\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx].copy()\n        best_fitness = fitness[best_idx]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            strategy_idx = np.random.choice(len(self.strategy_pool), p=self.strategy_probabilities)\n            new_population = self.strategy_pool[strategy_idx](population, func, lb, ub)\n            new_fitness = np.array([func(ind) for ind in new_population])\n            evaluations += len(new_population)\n            \n            # Update best solution\n            new_best_idx = np.argmin(new_fitness)\n            if new_fitness[new_best_idx] < best_fitness:\n                best_fitness = new_fitness[new_best_idx]\n                best_solution = new_population[new_best_idx].copy()\n            \n            # Adaptive strategy selection\n            success = new_fitness < fitness\n            self.strategy_probabilities[strategy_idx] = max(0.05, self.strategy_probabilities[strategy_idx] + self.learning_rate * (2 * success.mean() - 1))  \n            self.strategy_probabilities = np.clip(self.strategy_probabilities, 0.05, 0.9) \n            self.strategy_probabilities /= self.strategy_probabilities.sum()\n            \n            # Move to the next generation\n            population, fitness = new_population, new_fitness\n        \n        return best_solution\n\n    def random_search(self, population, func, lb, ub):\n        return np.random.uniform(lb, ub, population.shape)\n    \n    def gradient_search(self, population, func, lb, ub):\n        perturbation = np.random.randn(*population.shape) * 0.1\n        candidates = population + perturbation\n        np.clip(candidates, lb, ub, out=candidates)\n        return candidates\n    \n    def crossover_mutation(self, population, func, lb, ub):\n        offspring = []\n        for _ in range(self.population_size):\n            parents = population[np.random.choice(len(population), 2, replace=False)]\n            cross_point = np.random.randint(1, self.dim - 1)\n            child = np.concatenate((parents[0][:cross_point], parents[1][cross_point:]))\n            mutation = np.random.randn(self.dim) * 0.05\n            child += mutation\n            np.clip(child, lb, ub, out=child)\n            offspring.append(child)\n        return np.array(offspring)\n    \n    def self_adaptive_mutation(self, population, func, lb, ub):\n        sigma = 0.1\n        offspring = np.array([np.clip(ind + sigma * np.random.randn(self.dim), lb, ub) for ind in population])\n        return offspring", "configspace": "", "generation": 99, "feedback": "The algorithm AdaptiveMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.06318 with standard deviation 0.05788.", "error": "", "parent_ids": ["60dab772-8aac-4a30-a274-1374a9858270"], "operator": null, "metadata": {"aucs": [0.1344613652689901, 0.14234921463873873, 0.1432796690384336, 0.04906953589935581, 0.04874372386156456, 0.0491801389169878, 0.0004999999999999449, 0.0004999999999999449, 0.0004999999999999449]}}
