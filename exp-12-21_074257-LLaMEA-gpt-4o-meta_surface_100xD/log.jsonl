{"id": "f538bfac-9d5b-4f12-b4c6-81ab03c7fba1", "fitness": 0.1421055995878127, "name": "AMESH", "description": "Adaptive Memory and Evolutionary Strategy Hybrid (AMESH) combines adaptive memory with evolutionary strategies to efficiently explore and exploit the search space.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.1\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    direction += np.random.normal(0, sigma_init, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:2]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 0, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14211 with standard deviation 0.00864.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15304765313000035, 0.13193215755186471, 0.14133698808157302]}}
{"id": "5b6ef199-5f6a-4f53-8e55-50f808432848", "fitness": 0.14884466050624864, "name": "AMESH", "description": "AMESH with Enhanced Mutation (AMESH-EM) integrates direction-based mutation with dynamic mutation scaling to improve exploration and convergence speed.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.1\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 1.5)  # Dynamic scaling for mutation\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)  # Adjusted mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:2]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 1, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14884 with standard deviation 0.01214.", "error": "", "parent_ids": ["f538bfac-9d5b-4f12-b4c6-81ab03c7fba1"], "operator": null, "metadata": {"aucs": [0.16511973117949807, 0.14544457804355637, 0.13596967229569146]}}
{"id": "39251b37-0fbc-4a6a-9d69-18d15028acc4", "fitness": 0.1472219566882292, "name": "AMESH", "description": "AMESH-EM with Enhanced Selection incorporates tournament selection for improved exploitation and convergence.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.1\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 1.5)  # Dynamic scaling for mutation\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)  # Adjusted mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            tournament_indices = np.random.choice(len(combined_population), (population_size, 2), replace=False)\n            best_indices = [i if combined_fitness[i] < combined_fitness[j] else j for i, j in tournament_indices]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:2]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 2, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14722 with standard deviation 0.01529.", "error": "", "parent_ids": ["5b6ef199-5f6a-4f53-8e55-50f808432848"], "operator": null, "metadata": {"aucs": [0.15204145437752659, 0.16307122382658668, 0.12655319186057434]}}
{"id": "494ed50f-7a42-43b6-8605-b08c6e16bbcf", "fitness": 0.14595580863446136, "name": "AMESH_EMPlus", "description": "AMESH-EM+ integrates adaptive memory with a two-phase strategy, enhancing both exploration and exploitation while dynamically adjusting mutation based on convergence trends.", "code": "import numpy as np\n\nclass AMESH_EMPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.1\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Two-phase strategy based on budget utilization\n            if self.budget > self.budget / 2:\n                phase = \"exploration\"\n            else:\n                phase = \"exploitation\"\n            \n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    if phase == \"exploration\":\n                        mutation_scale = np.random.uniform(0.8, 2.0)\n                    else:\n                        mutation_scale = np.random.uniform(0.3, 1.2)\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            for new_best in population[:2]:\n                worst_index = np.argmax([func(mem) for mem in self.memory])\n                if func(new_best) < func(self.memory[worst_index]):\n                    self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 3, "feedback": "The algorithm AMESH_EMPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14596 with standard deviation 0.01028.", "error": "", "parent_ids": ["5b6ef199-5f6a-4f53-8e55-50f808432848"], "operator": null, "metadata": {"aucs": [0.14176696863500204, 0.13599557672222629, 0.1601048805461558]}}
{"id": "d8af5b2b-ef26-4c66-8026-008e0a382d77", "fitness": 0.15356581246292655, "name": "AMESH", "description": "AMESH with Enhanced Mutation and Memory Update (AMESH-EMMU) improves convergence by adjusting memory update strategy and mutation rate.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.15\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 1.5)  # Dynamic scaling for mutation\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)  # Adjusted mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:3]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 4, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15357 with standard deviation 0.00426.", "error": "", "parent_ids": ["5b6ef199-5f6a-4f53-8e55-50f808432848"], "operator": null, "metadata": {"aucs": [0.15524176979270876, 0.15773380905133083, 0.14772185854474007]}}
{"id": "b702faa8-2e02-4cc8-8e2b-b1cfdae3d7ce", "fitness": 0.1517666684274532, "name": "AMESH", "description": "AMESH with Dynamic Adaptive Memory (AMESH-DAM) uses a dynamic memory adjustment and adaptive mutation scaling to enhance search exploration and convergence.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        base_mutation_rate = 0.15\n        sigma_init = 0.5\n        dynamic_memory_factor = 2\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with dynamic adaptive memory\n            mutation_rate = base_mutation_rate * (1 + np.exp(-self.budget / (self.budget + 1)))\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 2.0)  # Wider dynamic scaling for mutation\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)  # Adjusted mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with dynamic adjustment\n            if len(self.memory) < self.memory_size * dynamic_memory_factor:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:3]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 5, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15177 with standard deviation 0.00423.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.14717661005314586, 0.1507311118420006, 0.15739228338721312]}}
{"id": "88c72459-aaf6-4e14-afce-c3964d7cd587", "fitness": 0.1475799367329986, "name": "AMESH", "description": "Improved AMESH-EMMU with Adaptive Population Size adjusts the population size dynamically based on remaining budget and convergence.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_population_size = 10\n        mutation_rate = 0.15\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= initial_population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Adjust population size dynamically\n            population_size = max(5, int((self.budget / (initial_population_size + 1)) * 10))\n            \n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(initial_population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 1.5)  # Dynamic scaling for mutation\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)  # Adjusted mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:initial_population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:3]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 6, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14758 with standard deviation 0.00200.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.1496239696521503, 0.1482585757362328, 0.14485726481061267]}}
{"id": "36f083b3-3af9-4e1d-a5c6-0d5eef0c540f", "fitness": 0.14194021047969996, "name": "AMESH", "description": "AMESH with Adaptive Mutation and Dynamic Memory (AMESH-AMDM) enhances convergence by dynamically adjusting mutation rates and memory usage based on diversity.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.15\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Diversity-based mutation adjustment\n            diversity = np.std(population, axis=0).mean()\n            adaptive_mutation_rate = mutation_rate * (1 + diversity)\n            \n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < adaptive_mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 1.5)\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Dynamic memory update\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:3]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 7, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14194 with standard deviation 0.01827.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.16621495126722097, 0.1374574554837763, 0.1221482246881026]}}
{"id": "0f2e394c-3ccb-4d32-843a-fa9f1821a887", "fitness": 0.15231990798260922, "name": "AMESH_EMMU_AC", "description": "AMESH-EMMU with Adaptive Crossover (AMESH-EMMU-AC): Enhances convergence by integrating adaptive crossover mechanisms alongside memory updates and mutation, optimizing exploration and exploitation balance.", "code": "import numpy as np\n\nclass AMESH_EMMU_AC:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.15\n        sigma_init = 0.5\n        crossover_rate = 0.7  # Introduce crossover rate\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            offspring = []\n            for _ in range(population_size):\n                parent_idx = np.random.choice(population_size, 2, replace=False)\n                parent1, parent2 = population[parent_idx]\n\n                # Adaptive crossover\n                if np.random.rand() < crossover_rate:\n                    alpha = np.random.rand(self.dim)\n                    child = alpha * parent1 + (1 - alpha) * parent2\n                else:\n                    child = parent1\n\n                # Evolutionary Strategy with adaptive memory\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - child\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 1.5)\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)\n                child = np.clip(child + direction, lb, ub)\n                \n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:3]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 8, "feedback": "The algorithm AMESH_EMMU_AC got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15232 with standard deviation 0.00221.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.15520664731745537, 0.14983449180328323, 0.1519185848270891]}}
{"id": "d0bb0990-673e-4b25-a88c-668cb733fc56", "fitness": 0.1517721205573284, "name": "AMESH_ANSS", "description": "Introducing the Adaptive Neighborhood Search Strategy (ANSS) in AMESH leverages localized exploration to enhance solution diversity and convergence.", "code": "import numpy as np\n\nclass AMESH_ANSS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.15\n        sigma_init = 0.5\n        neighborhood_size = int(self.dim / 3)  # Adaptive neighborhood size\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                # Adaptive Neighborhood Search Strategy\n                neighborhood_indices = np.random.choice(self.dim, neighborhood_size, replace=False)\n                if np.random.rand() < mutation_rate:\n                    for idx in neighborhood_indices:\n                        direction[idx] += np.random.normal(0, sigma_init)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:3]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 9, "feedback": "The algorithm AMESH_ANSS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15177 with standard deviation 0.01620.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.1367575218808118, 0.17426807115713638, 0.144290768634037]}}
{"id": "b9527418-86b2-4d16-ac7b-6696bd566b16", "fitness": 0.13426921389688565, "name": "DMS_AMR", "description": "Enhanced AMESH with Dynamic Memory Size and Adaptive Mutation Rate (DMS-AMR) to improve convergence by dynamically adjusting memory size and mutation rate based on fitness variance.", "code": "import numpy as np\n\nclass DMS_AMR:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        initial_mutation_rate = 0.15\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory size dynamically\n        self.memory_size = max(5, int(np.sqrt(self.dim)))\n        \n        # Initialize memory with the best individuals\n        self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Calculate dynamic mutation rate based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_rate = initial_mutation_rate * (1 + fitness_variance / (1 + fitness_variance))\n\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 1.5)  # Dynamic scaling for mutation\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)  # Adjusted mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory dynamically\n            new_best_individuals = population[:self.memory_size]\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(new_best_individuals[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in new_best_individuals:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 10, "feedback": "The algorithm DMS_AMR got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13427 with standard deviation 0.01295.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.1352585583821735, 0.1496138002427888, 0.11793528306569467]}}
{"id": "d75995c6-fbe1-4c6c-b1ee-90acb8076de3", "fitness": 0.1371456053690835, "name": "AMESH", "description": "Enhanced AMESH with Dynamic Memory Adaptation (AMESH-DMA) improves convergence by dynamically adjusting memory size and mutation strategy based on performance feedback.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_memory_size = 5\n        self.memory_size = self.initial_memory_size\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.15\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        self.memory.extend(population[np.argsort(fitness)[:self.initial_memory_size]])\n        \n        previous_best = np.min(fitness)\n        stagnation_counter = 0\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 1.5)  # Dynamic scaling for mutation\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)  # Adjusted mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            current_best = np.min(fitness)\n            if current_best < previous_best:\n                previous_best = current_best\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # Dynamic memory adaptation\n            if stagnation_counter > 2:\n                self.memory_size = min(self.memory_size + 1, population_size)\n            elif stagnation_counter == 0:\n                self.memory_size = max(self.initial_memory_size, self.memory_size - 1)\n\n            sorted_indices = np.argsort(fitness)\n            for new_best in population[sorted_indices[:self.memory_size]]:\n                worst_index = np.argmax([func(mem) for mem in self.memory])\n                if func(new_best) < func(self.memory[worst_index]):\n                    self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 11, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13715 with standard deviation 0.00439.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.13597651502733388, 0.1430167168820634, 0.13244358419785318]}}
{"id": "f9da89f3-84c0-415b-97fc-9a8e637a6d85", "fitness": 0.13848449183081557, "name": "AMESH", "description": "Adaptive Mutation and Memory Strategy (AMESH-AMMS) enhances performance by dynamically adjusting mutation rate and incorporating elite-solution-based memory updates.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.1  # Adjusted mutation rate\n        sigma_init = 0.4  # Adjusted initial sigma\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.8, 1.2)  # Narrowed dynamic scaling\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)  # Adjusted mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with elite solutions\n            top_indices = np.argsort(fitness)[:3]\n            for i, new_best in enumerate(population[top_indices]):\n                if len(self.memory) < self.memory_size:\n                    self.memory.append(new_best)\n                else:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 12, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13848 with standard deviation 0.01891.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.16479846732224857, 0.12948723133229978, 0.12116777683789837]}}
{"id": "e015c3b7-d8d5-45fe-a915-da5a8d8f1e54", "fitness": -Infinity, "name": "AMESH", "description": "AMESH with Adaptive Learning (AMESH-AL) boosts exploration and exploitation by dynamically resizing population and adjusting mutation based on fitness variance.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.15\n        sigma_init = 0.5\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        \n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Adjust population size based on fitness variance\n            fitness_variance = np.var(fitness)\n            population_size = max(5, int(20 * (1 - fitness_variance)))  # New dynamic sizing\n\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.clip(1 / (1 + fitness_variance), 0.5, 1.5)  # Adjusting mutation\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:3]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 13, "feedback": "An exception occurred: IndexError('index 14 is out of bounds for axis 0 with size 10').", "error": "IndexError('index 14 is out of bounds for axis 0 with size 10')", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {}}
{"id": "70b24bf3-d3b1-4923-ba6d-b1f3f79bb8df", "fitness": 0.1494584381640127, "name": "AMESH_AMDM", "description": "AMESH with Adaptive Memory and Dynamic Mutation (AMESH-AMDM) enhances convergence by integrating adaptive memory updates and dynamically controlling mutation based on fitness diversity.", "code": "import numpy as np\n\nclass AMESH_AMDM:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.15\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            fitness_diversity = np.std(fitness)\n            adaptive_mutation_rate = mutation_rate + 0.1 * (1 - fitness_diversity / max(fitness_diversity, 1e-3))\n            \n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < adaptive_mutation_rate:\n                    mutation_scale = np.random.uniform(0.5, 1.5)\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:3]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 14, "feedback": "The algorithm AMESH_AMDM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.14946 with standard deviation 0.00554.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.14259498598588005, 0.1496210599805058, 0.1561592685256522]}}
{"id": "63158963-40e4-4267-93ec-7cd93a061bb3", "fitness": 0.1544964179913765, "name": "AMESH", "description": "Improved convergence by reducing mutation scale for more precise exploitation around memory samples.", "code": "import numpy as np\n\nclass AMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.15\n        sigma_init = 0.5\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(population[np.argsort(fitness)[:self.memory_size]])\n        \n        while self.budget > 0:\n            # Evolutionary Strategy with adaptive memory\n            offspring = []\n            for _ in range(population_size):\n                parent = population[np.random.choice(population_size)]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rate:\n                    mutation_scale = np.random.uniform(0.1, 0.5)  # Reduced dynamic scaling for mutation\n                    direction += np.random.normal(0, sigma_init * mutation_scale, self.dim)  # Adjusted mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory\n            if len(self.memory) < self.memory_size:\n                self.memory.extend(population[:self.memory_size - len(self.memory)])\n            else:\n                for new_best in population[:3]:\n                    worst_index = np.argmax([func(mem) for mem in self.memory])\n                    if func(new_best) < func(self.memory[worst_index]):\n                        self.memory[worst_index] = new_best\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 15, "feedback": "The algorithm AMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15450 with standard deviation 0.02167.", "error": "", "parent_ids": ["d8af5b2b-ef26-4c66-8026-008e0a382d77"], "operator": null, "metadata": {"aucs": [0.15068443957481148, 0.18273254966625163, 0.13007226473306643]}}
{"id": "9f67efca-8a30-415b-9f84-3c9c9e976f63", "fitness": 0.16530592042015477, "name": "EnhancedAMESH", "description": "Enhanced convergence by incorporating adaptive memory learning and dynamic mutation control for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation based on rank\n            offspring = []\n            for rank, parent in enumerate(population):\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                if np.random.rand() < adaptive_mutation_scale:\n                    direction += np.random.normal(0, sigma_init, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = list(best_individuals)", "configspace": "", "generation": 16, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.16531 with standard deviation 0.00576.", "error": "", "parent_ids": ["63158963-40e4-4267-93ec-7cd93a061bb3"], "operator": null, "metadata": {"aucs": [0.17324284154493774, 0.15973894195170413, 0.1629359777638224]}}
{"id": "17aace4b-0933-4751-a73c-880a218d2e08", "fitness": 0.156626089105501, "name": "EnhancedAMESH", "description": "Improved exploitation by incorporating tournament selection for parent selection and adaptive mutation adjustment.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation based on rank\n            offspring = []\n            for rank, parent in enumerate(population):\n                tournament_indices = np.random.choice(population_size, 3, replace=False)\n                tournament_fitness = fitness[tournament_indices]\n                parent = population[tournament_indices[np.argmin(tournament_fitness)]]\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                if np.random.rand() < adaptive_mutation_scale:\n                    direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = list(best_individuals)", "configspace": "", "generation": 17, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.15663 with standard deviation 0.02243.", "error": "", "parent_ids": ["9f67efca-8a30-415b-9f84-3c9c9e976f63"], "operator": null, "metadata": {"aucs": [0.1520620627242294, 0.1317226474079204, 0.18609355718435316]}}
{"id": "2e417e26-da19-46bc-ab05-e157d0ede306", "fitness": 0.13506302431222003, "name": "EnhancedAMESHRefined", "description": "Incorporates stochastic adaptive learning and self-adjusting mutation strategies for enhanced exploration and exploitation dynamics.", "code": "import numpy as np\n\nclass EnhancedAMESHRefined:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        initial_mutation_rate = 0.2\n        sigma_init = 0.3\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals\n        self.update_memory(population, fitness)\n        \n        # Dynamic mutation rate based on success history\n        success_hist = np.zeros(population_size)\n        mutation_rates = np.full(population_size, initial_mutation_rate)\n        \n        while self.budget > 0:\n            # Adapt mutation rate for each individual\n            for i in range(population_size):\n                if success_hist[i] > 0:\n                    mutation_rates[i] *= 1.1\n                else:\n                    mutation_rates[i] *= 0.9\n                mutation_rates[i] = np.clip(mutation_rates[i], 0.01, 0.5)\n            \n            # Generate offspring\n            offspring = []\n            for i, parent in enumerate(population):\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                if np.random.rand() < mutation_rates[i]:\n                    direction += np.random.normal(0, sigma_init, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Success history update\n            success_hist = offspring_fitness < fitness\n\n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = list(best_individuals)", "configspace": "", "generation": 18, "feedback": "The algorithm EnhancedAMESHRefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.13506 with standard deviation 0.01748.", "error": "", "parent_ids": ["9f67efca-8a30-415b-9f84-3c9c9e976f63"], "operator": null, "metadata": {"aucs": [0.11974119984352993, 0.125928121796701, 0.15951975129642915]}}
{"id": "b758bccd-15e7-4758-ba59-8e8d292320e0", "fitness": 0.18041229313141582, "name": "ImprovedAMESH", "description": "Improved AMESH: Enhanced search through dynamic memory update strategies and adaptive mutation scaling, focusing on diversity preservation and effective exploration-exploitation trade-off.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        initial_mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(initial_mutation_rate, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                if np.random.rand() < adaptive_mutation_scale:\n                    direction += np.random.normal(0, sigma_init, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and maintain diversity\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = list(best_individuals)", "configspace": "", "generation": 19, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18041 with standard deviation 0.00710.", "error": "", "parent_ids": ["9f67efca-8a30-415b-9f84-3c9c9e976f63"], "operator": null, "metadata": {"aucs": [0.18381944914509352, 0.17052843710906673, 0.18688899314008722]}}
{"id": "cdb62f97-e33b-4db8-9fa5-5cb1f68273b3", "fitness": 0.17291424949132125, "name": "AdvancedAMESH", "description": "AdvancedAMESH: Leverages dynamic memory and crossover strategies with adaptive learning rates to maximize exploration and exploitation simultaneously.", "code": "import numpy as np\n\nclass AdvancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        initial_mutation_rate = 0.2\n        sigma_init = 0.3\n        crossover_rate = 0.8\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals\n        self.update_memory(population, fitness)\n\n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(initial_mutation_rate, 0.1 / (diversity + 1e-6))\n\n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < crossover_rate:\n                    partner = population[np.random.choice(len(population))]\n                    child = self.crossover(parent, partner, lb, ub)\n                else:\n                    memory_sample = self.memory[np.random.choice(len(self.memory))]\n                    direction = memory_sample - parent\n                    adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                    if np.random.rand() < adaptive_mutation_scale:\n                        direction += np.random.normal(0, sigma_init, self.dim)\n                    child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n\n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Update memory with best individuals and maintain diversity\n            self.update_memory(population, fitness)\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n\n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = list(best_individuals)\n\n    def crossover(self, parent1, parent2, lb, ub):\n        alpha = np.random.uniform(0, 1, self.dim)\n        child = alpha * parent1 + (1 - alpha) * parent2\n        return np.clip(child, lb, ub)", "configspace": "", "generation": 20, "feedback": "The algorithm AdvancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17291 with standard deviation 0.00725.", "error": "", "parent_ids": ["b758bccd-15e7-4758-ba59-8e8d292320e0"], "operator": null, "metadata": {"aucs": [0.17158671558449623, 0.18238466666105602, 0.1647713662284115]}}
{"id": "dd700f0a-ed2b-4923-887a-982baa8448a9", "fitness": 0.1061177428579766, "name": "AdaptiveMESHE", "description": "Adaptive Memory and Evolutionary Strategy with Dynamic Exploration: Leverages adaptive memory and evolutionary strategies with dynamic exploration-exploitation balancing using crowding distance to enhance diversity and convergence.", "code": "import numpy as np\n\nclass AdaptiveMESHE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        initial_mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        \n        # Initialize memory with best individuals\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(initial_mutation_rate, diversity_threshold / (diversity + 1e-6))\n            \n            # Dynamic exploration based on crowding distance\n            crowding_distances = self.calculate_crowding_distances(population)\n            exploration_factor = np.max(crowding_distances) / (np.mean(crowding_distances) + 1e-6)\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                if np.random.rand() < adaptive_mutation_scale * exploration_factor:\n                    direction += np.random.normal(0, sigma_init, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and maintain diversity\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = list(best_individuals)\n\n    def calculate_crowding_distances(self, population):\n        distances = np.zeros(len(population))\n        for i in range(self.dim):\n            sorted_indices = np.argsort(population[:, i])\n            sorted_population = population[sorted_indices]\n            distances[sorted_indices[0]] = distances[sorted_indices[-1]] = np.inf\n            for j in range(1, len(population) - 1):\n                distances[sorted_indices[j]] += (sorted_population[j + 1, i] - sorted_population[j - 1, i])\n        return distances", "configspace": "", "generation": 21, "feedback": "The algorithm AdaptiveMESHE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.10612 with standard deviation 0.01391.", "error": "", "parent_ids": ["b758bccd-15e7-4758-ba59-8e8d292320e0"], "operator": null, "metadata": {"aucs": [0.1147157267189034, 0.11714256742317275, 0.08649493443185363]}}
{"id": "c789ebab-a093-4d12-95e1-7d124f914290", "fitness": 0.1912135366458201, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Implements strategic memory decay and adaptive mutation fluctuation mechanisms to bolster exploration while maintaining search diversity for improved convergence performance.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 5\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        initial_mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(initial_mutation_rate, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                memory_sample = self.memory[np.random.choice(len(self.memory))]\n                direction = memory_sample - parent\n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                if np.random.rand() < adaptive_mutation_scale:\n                    direction += np.random.normal(0, sigma_init * np.random.rand(), self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and maintain diversity\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[len(self.memory):])", "configspace": "", "generation": 22, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19121 with standard deviation 0.00424.", "error": "", "parent_ids": ["b758bccd-15e7-4758-ba59-8e8d292320e0"], "operator": null, "metadata": {"aucs": [0.1855210502284803, 0.19244321305383072, 0.19567634665514932]}}
{"id": "af0ef083-39a1-49af-a2cd-bd35659b60f1", "fitness": 0.19789921889266315, "name": "ImprovedAMESH", "description": "ImprovedAMESH: Integrates memory diversification and step-size adaptation to enhance exploration, balancing convergence speed with solution robustness.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 23, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19790 with standard deviation 0.00527.", "error": "", "parent_ids": ["c789ebab-a093-4d12-95e1-7d124f914290"], "operator": null, "metadata": {"aucs": [0.19254794424275457, 0.19607845254736633, 0.20507125988786856]}}
{"id": "e6d5a70e-d122-460f-868f-3c0856290283", "fitness": -Infinity, "name": "AdaptiveAMESH", "description": "AdaptiveAMESH: Enhances ImprovedAMESH by incorporating adaptive learning rates with dynamic memory adjustments to better balance exploration and exploitation in various optimization landscapes.", "code": "import numpy as np\n\nclass AdaptiveAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n        self.learning_rate = 0.1  # New learning rate for adaptive control\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity and learning\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness, diversity)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness, diversity):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor with learning rate\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * self.learning_rate * new) for old, new in zip(self.memory, best_individuals)]\n        # Adjust memory size based on diversity to ensure adaptability\n        if diversity < 0.05:  # If diversity is low, store fewer solutions\n            self.memory_size = max(5, self.memory_size - 1)\n        elif diversity > 0.2:  # If diversity is high, store more solutions\n            self.memory_size = min(20, self.memory_size + 1)\n        \n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 24, "feedback": "An exception occurred: TypeError(\"update_memory() missing 1 required positional argument: 'diversity'\").", "error": "TypeError(\"update_memory() missing 1 required positional argument: 'diversity'\")", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {}}
{"id": "096df96d-4831-4d81-96a7-69839dc3424d", "fitness": 0.19632427611612627, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Utilizes dynamic memory enhancement and self-adaptive population scaling to improve exploration and exploitation balance in black-box optimization.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n        self.dynamic_scale_factor = 1.5  # New factor for dynamic population scaling\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Dynamic scaling of mutation rate based on performance\n            best_fitness = np.min(fitness)\n            scale_factor = self.dynamic_scale_factor if best_fitness < np.median(fitness) else 1.0\n            mutation_rate = max(scale_factor * self.adapt_factor, diversity_threshold / (np.mean(np.std(population, axis=0)) + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 25, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19632 with standard deviation 0.00777.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.20289488184251492, 0.20066649570871853, 0.18541145079714538]}}
{"id": "8b0e0a28-9a7d-4ca0-95bb-f112fd71c79e", "fitness": 0.19158929469393626, "name": "AdaptiveAMESH", "description": "AdaptiveAMESH: Enhances memory utilization and introduces dynamic mutation strategies for improved exploration and convergence in optimization tasks.", "code": "import numpy as np\n\nclass AdaptiveAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.8  # Increased decay factor for fresher memory\n        self.adapt_factor = 0.98  # Enhanced adaptation for mutation scale\n        self.dynamic_mutation = True  # Flag to toggle dynamic mutation strategies\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 12  # Increased size for more exploration\n        mutation_rate = 0.25  # Enhanced starting mutation rate\n        sigma_init = 0.4  # Larger initial sigma for more diverse exploration\n        diversity_threshold = 0.05  # Lower threshold to encourage exploration\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        \n        # Initial memory update\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Calculate diversity and adjust mutation rate dynamically\n            diversity = np.mean(np.std(population, axis=0))\n            if self.dynamic_mutation:\n                mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.6 or not self.memory:  # Increased chance of using memory\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Memory update\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 26, "feedback": "The algorithm AdaptiveAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19159 with standard deviation 0.00884.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.1856668686687064, 0.1850137408890823, 0.2040872745240201]}}
{"id": "8133a951-0eb1-43d6-9816-cd4f229f6cec", "fitness": 0.1868947376550487, "name": "ImprovedAMESH", "description": "ImprovedAMESH with enhanced mutation scaling, incorporating dynamic sigma adjustment for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale * 1.1, self.dim)  # Changed line\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 27, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18689 with standard deviation 0.00571.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.1789615699153727, 0.18956000401846607, 0.19216263903130726]}}
{"id": "e9fee429-5f17-4f69-bc58-e848565f60f5", "fitness": 0.18952412303994395, "name": "ImprovedAMESH", "description": "ImprovedAMESH with Dynamic Neighborhoods: Enhances exploration and exploitation by dynamically adjusting neighborhood sizes based on population diversity and fitness variance.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n        self.dynamic_neighborhood_factor = 0.5  # New parameter for neighborhood size adjustment\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            # Calculate fitness variance for neighborhood size adjustment\n            fitness_variance = np.var(fitness)\n            neighborhood_size = self.dynamic_neighborhood_factor / (1 + fitness_variance)\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale * neighborhood_size, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 28, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18952 with standard deviation 0.00580.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.1977223613392992, 0.18558813389009732, 0.18526187389043536]}}
{"id": "9dec589e-16f6-4090-8a25-541c11590ae6", "fitness": 0.19195718833987527, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Introduces dynamic memory management and environment-driven mutation to improve adaptability and convergence in complex search spaces.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 15  # Increased memory to maintain more diverse historical information\n        self.memory = []\n        self.memory_decay_factor = 0.85  # Lower decay for more reliance on accumulated memory\n        self.adapt_factor = 0.98  # Slightly reduced adaptation to allow more gradual change\n        self.diversity_factor = 0.05  # Factor to enhance mutation based on diversity assessment\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 20  # Larger population for broader search\n        mutation_rate = 0.25\n        sigma_init = 0.35  # Increase to explore larger steps initially\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Dynamic adjustment of mutation rate based on population diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, self.diversity_factor / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n\n                # Environment-driven mutation scale\n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 29, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19196 with standard deviation 0.00276.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.1939293301445708, 0.19389426865442283, 0.18804796622063213]}}
{"id": "d15fd9f9-8fad-470f-bd86-a7ccc3811e17", "fitness": 0.18354630618439494, "name": "ImprovedAMESH", "description": "Modified ImprovedAMESH by adjusting the adaptive mutation scale initialization for enhanced exploration. ", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.25  # Change made here: adjusted sigma_init to 0.25\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 30, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18355 with standard deviation 0.00430.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.17808730246006688, 0.18860018222213537, 0.18395143387098256]}}
{"id": "2010c778-fc68-42ad-9c07-af1be20f74df", "fitness": -Infinity, "name": "ImprovedAMESH_v2", "description": "ImprovedAMESH_v2: Enhances diversity with adaptive crossover alongside memory-driven mutation, promoting exploration in high-dimensional search spaces.", "code": "import numpy as np\n\nclass ImprovedAMESH_v2:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        crossover_prob = 0.3\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < crossover_prob and len(self.memory) > 1:\n                    # Crossover from memory\n                    mate1, mate2 = np.random.choice(self.memory, 2, replace=False)\n                    child = 0.5 * (mate1 + mate2) + np.random.normal(0, sigma_init * mutation_rate, self.dim)\n                else:\n                    # Memory-driven mutation or pure exploration\n                    if np.random.rand() < 0.5 and self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                    \n                    adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                    direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                    child = parent + direction\n                \n                child = np.clip(child, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 31, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {}}
{"id": "abe6a32f-bd82-4b0e-a801-b4de8ccd1802", "fitness": 0.19145746611554013, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Introduces dynamic population size and annealed mutation to improve exploration and convergence.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.8  # Adjusted decay for better retention\n        self.adapt_factor = 0.98  # Further refined adaptation parameter\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_population_size = 10\n        population_size = initial_population_size\n        mutation_rate = 0.3  # Increased for better exploration\n        sigma_init = 0.2  # Reduced for finer local search\n        diversity_threshold = 0.05  # Lowered for tighter population control\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Dynamic population size adjustment based on remaining budget\n            population_size = max(2, int(initial_population_size * (self.budget / (self.budget + 10))))\n            \n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.4 or not self.memory:\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            self.update_memory(population, fitness)\n        \n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 32, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19146 with standard deviation 0.01096.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19565072364560498, 0.1764442508603874, 0.20227742384062797]}}
{"id": "baaee7f9-f61e-4308-8b7b-261f3aeeefd0", "fitness": 0.19789921889266315, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Incorporates dynamic population resizing and adaptive mutation strategies to better balance exploration and exploitation throughout the optimization process, improving convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n        self.dynamic_pop_size_factor = 0.5  # New factor for dynamic population resizing\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population_size = initial_population_size\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Dynamic population resizing based on remaining budget\n            remaining_budget_ratio = self.budget / (self.budget + population_size)\n            population_size = max(2, int(initial_population_size * (1 + self.dynamic_pop_size_factor * (1 - remaining_budget_ratio))))\n            \n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 33, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19790 with standard deviation 0.00527.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19254794424275457, 0.19607845254736633, 0.20507125988786856]}}
{"id": "c6bb6116-de1d-4048-a57d-959c20953991", "fitness": -Infinity, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Introduces dynamic memory reshuffling and adaptive mutation intensity for improved convergence in diverse search landscapes.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n        self.intensity_factor = 1.2  # New intensity factor to adjust mutation strength dynamically\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate * self.intensity_factor\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        reshuffle_probability = 0.3  # Probability to reshuffle the memory\n        for i, new in enumerate(best_individuals):\n            if np.random.rand() < reshuffle_probability:\n                idx = np.random.choice(len(self.memory))\n                self.memory[idx] = new\n            else:\n                if len(self.memory) < self.memory_size:\n                    self.memory.append(new)\n                else:\n                    self.memory[i % self.memory_size] = (self.memory_decay_factor * self.memory[i % self.memory_size] + \n                                                         (1 - self.memory_decay_factor) * new)", "configspace": "", "generation": 34, "feedback": "An exception occurred: ValueError('a must be greater than 0 unless no samples are taken').", "error": "ValueError('a must be greater than 0 unless no samples are taken')", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {}}
{"id": "20a5cd72-a40d-4320-b9fb-d55802c2431d", "fitness": 0.18507127904928014, "name": "DynamicAMESH", "description": "DynamicAMESH: Enhances ImprovedAMESH by integrating dynamic population adaptation and directional learning to efficiently balance exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n        self.dynamic_population_size = 10  # Start with a base population size\n        self.max_population_size = 20  # Allow dynamic growth\n        self.shrinkage_factor = 0.95  # Population shrinkage factor after exploration\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (self.dynamic_population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= self.dynamic_population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            # Dynamic population adjustment\n            if self.budget > self.dynamic_population_size:\n                self.dynamic_population_size = min(self.dynamic_population_size + 1, self.max_population_size)\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / self.dynamic_population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= self.dynamic_population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.dynamic_population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n            \n            # Shrink population size gradually\n            self.dynamic_population_size = max(int(self.dynamic_population_size * self.shrinkage_factor), 10)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 35, "feedback": "The algorithm DynamicAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18507 with standard deviation 0.00454.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.1789593069052069, 0.1864280561823033, 0.18982647406033026]}}
{"id": "6140ca47-3141-44b0-b91e-b54c1eea53a3", "fitness": 0.18429460389547547, "name": "ImprovedAMESH", "description": "Refines ImprovedAMESH by integrating weighted memory learning and dynamic population sizing for better balance between exploration and exploitation.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_weights = np.ones(self.memory_size)  # Added weights for memory slots\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  \n        self.dynamic_pop_factor = 0.1  # New dynamic population factor\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = max(5, int(10 + self.dynamic_pop_factor * (self.budget / (self.dim + 1))))  # Dynamic population size\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:\n                    if self.memory:\n                        memory_sample, weight = self.select_weighted_memory()\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else: \n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            self.update_memory(population, fitness)\n        \n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory_weights = self.memory_decay_factor * self.memory_weights + 1  # Dynamic weight adjustment\n        self.memory = [(weight * old + new) / (weight + 1) for old, new, weight in zip(self.memory, best_individuals, self.memory_weights)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]\n\n    def select_weighted_memory(self):\n        probabilities = self.memory_weights / np.sum(self.memory_weights)\n        index = np.random.choice(len(self.memory), p=probabilities)\n        return self.memory[index], self.memory_weights[index]", "configspace": "", "generation": 36, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18429 with standard deviation 0.00553.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18663420254924912, 0.18959244344989656, 0.17665716568728074]}}
{"id": "322f8776-840c-4bd7-a18b-11b534682261", "fitness": 0.18420341130052875, "name": "ImprovedAMESH", "description": "Enhanced exploration through dynamic population size scaling and refined adaptive mutation control.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale * (diversity + 1), self.dim)  # Refined mutation\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 37, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18420 with standard deviation 0.00424.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.17899191763246702, 0.1893886204634344, 0.18422969580568482]}}
{"id": "73b0245c-25f3-4a88-8350-005c3f7b9697", "fitness": 0.19081382679429806, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Incorporates a two-phase adaptive exploration-exploitation balance and dynamic memory updating to improve convergence and solution quality in black-box optimization.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.85  # Enhanced decay factor for rapid adaptation\n        self.adapt_factor = 0.98  # Refined adaptation parameter\n        self.phase_switch = budget // 2  # Switch from exploration to exploitation halfway\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.3\n        sigma_init = 0.2\n        diversity_threshold = 0.15\n        \n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            if self.budget > self.phase_switch:\n                # Exploration phase\n                mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (np.mean(np.std(population, axis=0)) + 1e-6))\n            else:\n                # Exploitation phase\n                mutation_rate = min(mutation_rate / self.adapt_factor, 0.5)\n\n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:\n                    memory_sample = self.memory[np.random.choice(len(self.memory))] if self.memory else np.zeros(self.dim)\n                    direction = memory_sample - parent\n                else:\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            self.update_memory(population, fitness)\n        \n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 38, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19081 with standard deviation 0.00289.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19255427154692117, 0.19315263607507882, 0.18673457276089422]}}
{"id": "6854cbb4-da18-4dba-b920-7db8bcff6364", "fitness": 0.19046070317705666, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Augments adaptive memory with a feedback loop for dynamic population sizing and targeted mutation for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n        self.dynamic_population_factor = 0.1  # New factor for dynamic population sizing\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = max(5, int(self.dynamic_population_factor * self.dim))\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adapt mutation rate based on diversity and budget usage\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            population_size = int(min(10, self.budget, self.dynamic_population_factor * self.dim))\n\n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 39, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19046 with standard deviation 0.01149.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19625811069516708, 0.1744097570056813, 0.2007142418303216]}}
{"id": "1b2bfcc2-45b9-4f23-818c-3aafa5fb3e01", "fitness": 0.18142174009862974, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Utilizes dynamic population sizing and multi-strategy adaptation to improve convergence and robustness through balanced exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 15\n        self.memory = []\n        self.memory_decay_factor = 0.85\n        self.adapt_factor = 0.98\n        self.dynamic_population_factor = 0.2  # New parameter for dynamic population sizing\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        base_population_size = 10\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize dynamic population size\n        population_size = int(base_population_size + self.dynamic_population_factor * self.dim)\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        \n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(self.adapt_factor * diversity, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 40, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18142 with standard deviation 0.00314.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.1837594715717471, 0.17698108912709987, 0.18352465959704223]}}
{"id": "04519332-222f-4f47-bb09-d2cfc0e089d5", "fitness": 0.18812926662386312, "name": "ImprovedAMESH", "description": "AdaptiveAMESH: Enhances exploration-exploitation balance with adaptive population size and mutation scaling based on convergence rate.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = self.dynamic_population_size()\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:\n                    direction = np.random.uniform(-1, 1, self.dim)\n\n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            self.update_memory(population, fitness)\n        \n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]\n\n    def dynamic_population_size(self):\n        return max(5, int(self.budget / 100))", "configspace": "", "generation": 41, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18813 with standard deviation 0.00758.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18835260547830734, 0.19729415149547536, 0.17874104289780668]}}
{"id": "32798219-b90b-4c34-a84b-e6ea1f2ad135", "fitness": 0.1923881213142339, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Incorporates multi-layered memory and adaptive elite selection to boost exploration and convergence efficiency in diverse optimization landscapes.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 15  # Increased memory size for diversity\n        self.memory = []\n        self.elite_memory = []  # Additional memory for elite solutions\n        self.memory_decay_factor = 0.85  # Slightly decreased to boost influence of new solutions\n        self.adapt_factor = 0.98  # Adaptation for mutation scale remains\n        self.elite_threshold = 0.2  # Proportion of elite individuals\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 12  # Slightly increased to improve exploration\n        mutation_rate = 0.25\n        sigma_init = 0.25\n        diversity_threshold = 0.15\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Update elite memory with top-performing individuals\n        elite_count = int(self.elite_threshold * self.memory_size)\n        elite_individuals = best_individuals[:elite_count]\n        \n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        self.elite_memory = elite_individuals\n        \n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 42, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19239 with standard deviation 0.00366.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19667625409509804, 0.18772692552008674, 0.192761184327517]}}
{"id": "c9e1273f-0440-4bb0-86ec-ef664c449332", "fitness": 0.18901933427954684, "name": "EnhancedAMESH", "description": "EnhancedAMESH integrates dynamic diversity management and adaptive memory reshaping to improve exploration efficiency and convergence precision.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.98  # Adjusted adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 12  # Slightly increased population size for better exploration\n        base_mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.15  # Adjusted threshold for diversity management\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(base_mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.6 or not self.memory:  # More exploration with memory\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 43, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18902 with standard deviation 0.01240.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.2038369644347906, 0.18973506930897088, 0.173485969094879]}}
{"id": "c19d11fd-68b8-4d04-b3b2-00e61756deab", "fitness": 0.18476097948336215, "name": "AdaptiveMemorySynergy", "description": "AdaptiveMemorySynergy (AMS): Introduces synergy between historical bests and current exploration to enhance convergence through adaptive memory weighting and dynamic mutation control.", "code": "import numpy as np\n\nclass AdaptiveMemorySynergy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.8\n        self.adapt_factor = 0.98  # Improved adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.05\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.7 or not self.memory:  # Higher chance to exploit memory synergy\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        if not self.memory:\n            self.memory = list(best_individuals)\n        else:\n            # Combine new bests with the existing memory, applying decay to older entries\n            self.memory = [\n                self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new \n                for old, new in zip(self.memory, best_individuals)\n            ]\n        # Ensure memory size is maintained\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 44, "feedback": "The algorithm AdaptiveMemorySynergy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18476 with standard deviation 0.00445.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18473654593615318, 0.19022680408654213, 0.17931958842739115]}}
{"id": "ef8cb019-7fbd-4b9c-aa9a-b1539835edbf", "fitness": 0.1835852987781845, "name": "ImprovedAMESH", "description": "ImprovedAMESH with refined adaptive mutation: Enhances exploration by introducing dynamic adaptation of mutation rate and increased memory capacity.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 15  # Changed from 10 to 15 to enhance memory\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.995  # Changed from 0.99 to 0.995 for more refined adaptation\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 45, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18359 with standard deviation 0.00562.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.17702888700312625, 0.1829689074626516, 0.19075810186877562]}}
{"id": "c2c1993d-d259-431d-9710-5e0056be1e0b", "fitness": 0.19412733300785864, "name": "HybridEvolutionaryLevySearch", "description": "Hybrid Evolutionary-Levy Search (HELS): Combines evolutionary strategies with Levy flight patterns to enhance exploration and exploitation across varied landscapes.", "code": "import numpy as np\n\nclass HybridEvolutionaryLevySearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 15\n        self.memory = []\n        self.memory_decay_factor = 0.85\n        self.adapt_factor = 0.98  # New adaptation parameter for mutation scale\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.25\n        sigma_init = 0.25\n        levy_alpha = 1.5\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n\n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity and memory\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity / (diversity + 1e-6))\n\n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                        direction = self.levy_flight(direction, levy_alpha)\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n\n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n\n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n\n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new)\n                       if old is not None else new\n                       for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]\n\n    def levy_flight(self, direction, alpha):\n        step = np.random.standard_normal(self.dim)\n        step_size = direction / (np.abs(step) ** (1 / alpha))\n        return step_size", "configspace": "", "generation": 46, "feedback": "The algorithm HybridEvolutionaryLevySearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19413 with standard deviation 0.00765.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18361249260833612, 0.19716142678803905, 0.20160807962720073]}}
{"id": "401ba261-6222-4a84-9bb2-00e7a3f06f21", "fitness": 0.18988074761540294, "name": "AdaptiveDirectionalEvolution", "description": "AdaptiveDirectionalEvolution (ADE): Enhances exploration by dynamically adjusting mutation scale and leveraging directionality from both memory and best individuals, aiming for rapid convergence and robustness.", "code": "import numpy as np\n\nclass AdaptiveDirectionalEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 15\n        self.memory = []\n        self.memory_decay_factor = 0.85\n        self.adapt_factor = 0.98\n        self.mutation_scale_decay = 0.95\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 12\n        mutation_rate = 0.25\n        sigma_init = 0.4\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Calculate diversity to adapt mutation scale\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.6 and self.memory:  # Exploration with memory\n                    memory_sample = self.memory[np.random.choice(len(self.memory))]\n                    direction = memory_sample - parent\n                else:  # Pure exploration or based on best individual\n                    best_individual = population[np.argmin(fitness)]\n                    direction = best_individual - parent if np.random.rand() < 0.5 else np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 47, "feedback": "The algorithm AdaptiveDirectionalEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18988 with standard deviation 0.00830.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.20023606795958238, 0.1799200372770149, 0.18948613760961153]}}
{"id": "8453cb95-36be-4215-b9fd-1861200dd089", "fitness": 0.18354630618439494, "name": "ImprovedAMESH", "description": "ImprovedAMESH+: Adjusts adaptive mutation scale initialization for enhanced convergence efficiency.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.25  # Adjusted initial mutation scale for improved convergence\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 48, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18355 with standard deviation 0.00430.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.17808730246006688, 0.18860018222213537, 0.18395143387098256]}}
{"id": "8b83cfe5-c55f-460a-8ca1-10decbb2b5d1", "fitness": 0.1825202378360097, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Leverages adaptive learning rates for mutation and diversified memory with optimal retention to balance between exploration and exploitation efficiently.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.85\n        self.adapt_factor = 0.95\n        self.learning_rate = 0.01\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new + self.learning_rate * np.random.uniform(-1, 1, self.dim))\n                       for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 49, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18252 with standard deviation 0.00447.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18388201535070037, 0.17649458534950935, 0.18718411280781944]}}
{"id": "39c859bd-9e81-455d-bcb4-78e213150af3", "fitness": -Infinity, "name": "ImprovedAMESH", "description": "EnhancedAMESH: Introduces adaptive population size based on convergence rate to refine exploration and exploitation balance.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            # Adjust population size based on convergence\n            population_size = max(5, min(20, int((ub - lb).mean() / diversity)))  # <-- Line modified\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 50, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {}}
{"id": "59a782d4-0303-4b02-870a-0f90e90dccd8", "fitness": 0.19120645388298393, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Utilizes a rotating neighborhood exploration mechanism and adaptive memory decay to improve convergence and diversity.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.95  # Increased decay for faster adaptability\n        self.adapt_factor = 0.98  # Adjust mutation adaptation for subtle adjustments\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                # Rotate direction for neighborhood exploration\n                rotation_matrix = np.eye(self.dim)\n                angle = np.pi / 6 * (rank % population_size)  # Rotate based on rank\n                rotation_matrix[0, 0] = np.cos(angle)\n                rotation_matrix[0, 1] = -np.sin(angle)\n                rotation_matrix[1, 0] = np.sin(angle)\n                rotation_matrix[1, 1] = np.cos(angle)\n                rotated_direction = np.dot(rotation_matrix, direction)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                rotated_direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + rotated_direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 51, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19121 with standard deviation 0.00469.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18480085789643919, 0.1929080378827599, 0.1959104658697527]}}
{"id": "06aaab8c-6ffa-45a7-944b-286067deaf51", "fitness": -Infinity, "name": "ImprovedAMESH", "description": "ImprovedAMESH with dynamic population size adjustment to enhance convergence.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            # Adjust population size based on remaining budget\n            population_size = max(1, self.budget // 10) # Adjust population size dynamically\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 52, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {}}
{"id": "40e5284f-23e6-4526-988d-87167abcaf0b", "fitness": -Infinity, "name": "ImprovedAMESH", "description": "Enhanced ImprovedAMESH by introducing an enhanced memory update mechanism to boost solution robustness.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        self.memory.extend([ind for ind in best_individuals if ind not in self.memory])\n        if len(self.memory) > self.memory_size:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 53, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {}}
{"id": "2242bcf3-8ef0-4118-8071-2202344db7e8", "fitness": 0.18896745968806927, "name": "ImprovedAMESH", "description": "EnhancedAMESH: Introduces dynamic memory decay adjustment for balanced exploration-exploitation, improving solution robustness with minimal code change.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply dynamic memory decay factor for better adaptability\n        self.memory_decay_factor = 0.9 * (self.budget / (self.budget + len(self.memory)))\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 54, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18897 with standard deviation 0.00324.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18727178078653084, 0.18612431310515487, 0.1935062851725221]}}
{"id": "ae87940d-afa7-458c-a725-633ba3142266", "fitness": 0.19168359122355136, "name": "AEMES", "description": "Adaptive Exploration and Memory-Enhanced Search (AEMES) leverages adaptive mutation strategy and dynamic memory filtering to refine solution quality and improve convergence in diverse optimization landscapes.", "code": "import numpy as np\n\nclass AEMES:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.98  # Slightly more aggressive adaptation\n        self.diversity_factor = 0.1  # Factor to dynamically adjust memory influence\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.select_memory_sample()\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Update memory with a weighted blend of old and new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]\n\n    def select_memory_sample(self):\n        # Dynamically adjust memory influence based on diversity factor\n        return self.memory[np.random.choice(len(self.memory))]", "configspace": "", "generation": 55, "feedback": "The algorithm AEMES got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19168 with standard deviation 0.00312.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18729633096070986, 0.19346061228132072, 0.19429383042862347]}}
{"id": "2e94f244-5944-498b-ae6b-215a49dac357", "fitness": 0.19789921889266315, "name": "ImprovedAMESH", "description": "ImprovedAMESH with enhanced mutation direction influenced by memory to better exploit promising regions.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 56, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19790 with standard deviation 0.00527.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19254794424275457, 0.19607845254736633, 0.20507125988786856]}}
{"id": "a9e3f1e8-ae0d-4a37-8fa9-bbf20f592acc", "fitness": 0.1936950801241117, "name": "ImprovedAMESHPlusPlus", "description": "ImprovedAMESH++: Introduces adaptive learning rates for enhanced mutation scale and diversified memory updates to further optimize exploration and exploitation balance.", "code": "import numpy as np\n\nclass ImprovedAMESHPlusPlus:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 15  # Increased memory size for more diversity\n        self.memory = []\n        self.memory_decay_factor = 0.85  # Slightly reduced decay for better memory retention\n        self.adapt_factor = 0.98  # Adjusted adaptation parameter for learning rate\n        self.learn_rate = 0.05  # New parameter for adaptive learning rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 15  # Increased population size for greater exploration\n        mutation_rate = 0.25\n        sigma_init = 0.3\n        diversity_threshold = 0.05  # More sensitive diversity threshold\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * (self.adapt_factor + self.learn_rate * diversity), \n                                diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) \n                       for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 57, "feedback": "The algorithm ImprovedAMESHPlusPlus got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19370 with standard deviation 0.00614.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.2019058574758842, 0.19205135220394942, 0.1871280306925015]}}
{"id": "e0654dc9-99fc-4365-a813-eb934bf82f87", "fitness": 0.18503922625164307, "name": "ImprovedAMESH", "description": "Integrates enhanced mutation strategy via memory-based direction scaling to optimize exploration-exploitation balance.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = (memory_sample - parent) * mutation_rate  # Modified line\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 58, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18504 with standard deviation 0.00961.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.17478540375195284, 0.18244075276204796, 0.19789152224092843]}}
{"id": "62eca306-6abb-4eb5-a360-fc533de8d4af", "fitness": 0.19168359122355136, "name": "ImprovedAMESH", "description": "Adjusted the mutation rate adaptation factor to potentially enhance exploration and convergence balance.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.98  # Changed adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 59, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19168 with standard deviation 0.00312.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18729633096070986, 0.19346061228132072, 0.19429383042862347]}}
{"id": "d6aa91fe-2431-4d1e-86a7-e83af1cbf2b7", "fitness": 0.18804345718276713, "name": "ImprovedAMESH", "description": "Enhanced memory reinforcement strategy by adjusting memory decay for improved convergence.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.95  # Adjusted decay factor for better memory retention\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 60, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18804 with standard deviation 0.01197.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18067829571794447, 0.17852474498294968, 0.20492733084740722]}}
{"id": "7320d663-f5e2-4085-8d97-e0b794ef7700", "fitness": 0.17772063333753296, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Introduces elite-based crossover and adaptive memory retention to improve exploration diversity and convergence precision.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Perform elite-based crossover\n            elite_indices = np.argsort(fitness)[:2]  # Select top two individuals\n            elite_offspring = self.crossover(elite_indices, population, lb, ub)\n            offspring = np.vstack((offspring, elite_offspring))\n            offspring_fitness = np.hstack((offspring_fitness, [func(ind) for ind in elite_offspring]))\n            self.budget -= len(elite_offspring)\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]\n    \n    def crossover(self, elite_indices, population, lb, ub):\n        parent1, parent2 = population[elite_indices]\n        crossover_point = np.random.randint(1, self.dim-1)\n        child1 = np.clip(np.concatenate((parent1[:crossover_point], parent2[crossover_point:])), lb, ub)\n        child2 = np.clip(np.concatenate((parent2[:crossover_point], parent1[crossover_point:])), lb, ub)\n        return [child1, child2]", "configspace": "", "generation": 61, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17772 with standard deviation 0.00137.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.17860765145324808, 0.17578388977522819, 0.17877035878412262]}}
{"id": "b669526b-6f7e-4634-a8cb-c97b30b6d95c", "fitness": 0.19172364491642624, "name": "AdvancedAMESH", "description": "AdvancedAMESH: Enhances adaptive memory with dynamic crossover and mutation scaling to improve exploration-exploitation balance and solution precision.", "code": "import numpy as np\n\nclass AdvancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # Adaptation parameter for mutation scale\n        self.crossover_rate = 0.7  # New parameter for crossover rate\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                # Crossover with memory individuals for enhanced search\n                if np.random.rand() < self.crossover_rate and self.memory:\n                    memory_sample = self.memory[np.random.choice(len(self.memory))]\n                    crossover_point = np.random.randint(1, self.dim)\n                    child = np.concatenate([parent[:crossover_point], memory_sample[crossover_point:]])\n                else:\n                    child = parent.copy()\n                    \n                # Mutation\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - child\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(child + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 62, "feedback": "The algorithm AdvancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19172 with standard deviation 0.00566.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.1862816475285094, 0.18935533055518028, 0.19953395666558904]}}
{"id": "caca5623-01ec-40d2-9c40-8408348f6c25", "fitness": 0.190147768876848, "name": "ImprovedAMESH", "description": "Enhanced Adaptive Mutation with Memory: Tweaks adaptive mutation rate to exploit memory more effectively, boosting exploration while retaining convergence robustness.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale * 1.05, self.dim)  # Slightly increased mutation scale\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 63, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19015 with standard deviation 0.00734.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19013716105674816, 0.19914540191141827, 0.18116074366237755]}}
{"id": "0a97e41f-9ab2-4b22-843b-10d286a29d3c", "fitness": -Infinity, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Employs adaptive dynamic population size and memory-based direction refinement to boost convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        adaptive_population_factor = 0.5  # New factor for adaptive population size\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            # Adaptive population size\n            adjusted_population_size = max(2, int(population_size * (1 + adaptive_population_factor * (1 - diversity))))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= len(offspring)\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:adjusted_population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 64, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {}}
{"id": "a3d04674-d650-46bd-9e8c-52bda7b7620c", "fitness": 0.18171788320361904, "name": "ImprovedAMESH", "description": "Enhancing adaptive mutation scale using a squared rank-based approach to improve exploration and convergence.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank**2 / population_size**2)) * mutation_rate  # Change here\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 65, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18172 with standard deviation 0.00344.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18148378880838345, 0.17763130257187165, 0.186038558230602]}}
{"id": "c4e4c8de-c91c-4111-909d-47bb4985ed35", "fitness": 0.19789921889266315, "name": "ImprovedAMESH", "description": "ImprovedAMESH: Enhances the exploitation phase by utilizing local search on the best solutions, improving convergence speed.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index] + np.random.normal(0, sigma_init * 0.1, self.dim)  # Added local search step\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 66, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19790 with standard deviation 0.00527.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19254794424275457, 0.19607845254736633, 0.20507125988786856]}}
{"id": "ae8090fb-217c-4ca0-9afd-25e10485a473", "fitness": 0.18629677964834004, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Introduces dynamic memory selection and adaptive exploration to improve convergence speed and solution quality.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 15  # Increased memory size for better exploration-exploitation balance\n        self.memory = []\n        self.memory_decay_factor = 0.8  # More weight to recent solutions\n        self.adapt_factor = 0.98  # Adaptive mutation factor for more refinement\n        self.adaptive_threshold = 1.5  # Dynamic threshold for diversity-based adaptation\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 15  # Larger population for increased diversity\n        mutation_rate = 0.25  # Initial mutation rate\n        sigma_init = 0.2  # Reduced initial mutation scale\n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity and a dynamic threshold\n            diversity = np.mean(np.std(population, axis=0))\n            adaptive_threshold = max(self.adaptive_threshold, diversity)\n            mutation_rate = max(mutation_rate * self.adapt_factor, adaptive_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Use memory-guided exploration\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure random exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 67, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18630 with standard deviation 0.00660.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19420913121920325, 0.17804731291877884, 0.18663389480703807]}}
{"id": "b1359f45-8b63-4a8d-bf8b-ad7b25cec5c2", "fitness": -Infinity, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Introduces a dynamic crossover mechanism and improved memory integration for heightened exploration and accelerated convergence.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 15\n        self.memory = []\n        self.memory_decay_factor = 0.85\n        self.adapt_factor = 0.98\n        self.crossover_rate = 0.7\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 12\n        mutation_rate = 0.15\n        sigma_init = 0.25\n        diversity_threshold = 0.05\n        \n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < self.crossover_rate and len(self.memory) >= 2:\n                    parent2, parent3 = np.random.choice(self.memory, 2, replace=False)\n                    direction = (parent2 + parent3) / 2 - parent\n                else:\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            self.update_memory(population, fitness)\n        \n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 68, "feedback": "An exception occurred: ValueError('a must be 1-dimensional').", "error": "ValueError('a must be 1-dimensional')", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {}}
{"id": "ffcc7761-bc8c-4cf6-b48c-86b02d86f8e7", "fitness": 0.1962931867368524, "name": "ImprovedAMESH", "description": "Integrates adaptive crossover and memory-based directional mutation to enhance exploration and maintain solution robustness.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                \n                # Adaptive crossover\n                if np.random.rand() < 0.5:\n                    partner = population[np.random.choice(population_size)]\n                    child = np.clip((parent + partner) / 2 + direction, lb, ub)\n                else:\n                    child = np.clip(parent + direction, lb, ub)\n                \n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 69, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19629 with standard deviation 0.00260.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19952937055667097, 0.19619229099297653, 0.1931578986609097]}}
{"id": "faa31329-4cea-4aa7-88b8-95b51081f4d9", "fitness": 0.17723026509701534, "name": "EnhancedAMESH", "description": "EnhancedAMESH: Introduces dynamic memory allocation and probabilistic diversity control to improve adaptability and convergence precision.", "code": "import numpy as np\n\nclass EnhancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.995  # Slightly higher adaptation to refine convergence\n        self.prob_diversify = 0.1  # Probability to use diversification for exploration\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < self.prob_diversify and self.memory:  # Exploration with diversity control\n                    memory_sample = self.memory[np.random.choice(len(self.memory))]\n                    direction = memory_sample - parent\n                else:  # Pure exploration or random direction\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and dynamically allocate memory\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) \n                       for old, new in zip(self.memory, best_individuals[:len(self.memory)])]\n        # Dynamically allocate new individuals if memory slots are available\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[len(self.memory):self.memory_size])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 70, "feedback": "The algorithm EnhancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17723 with standard deviation 0.00687.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.17147962313809972, 0.17332428079276696, 0.18688689136017933]}}
{"id": "5b89126c-4f35-4b93-80e7-ccc39ef227b2", "fitness": 0.17125290353790676, "name": "AdvancedAMESH", "description": "AdvancedAMESH: Enhances memory adaptation with dynamic mutation strategies and elite preservation to boost convergence efficiency and robustness.", "code": "import numpy as np\n\nclass AdvancedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.95  # Further reduced adaptation to allow more exploration\n        self.elite_rate = 0.2  # Elite rate for preserving top individuals\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(self.adapt_factor * diversity_threshold / (diversity + 1e-6), 0.01)\n            \n            offspring = []\n            elites = int(population_size * self.elite_rate)\n            \n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Preserve elite individuals\n            elite_population = population[:elites]\n            elite_fitness = fitness[:elites]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n            \n            # Include elite individuals back into the population\n            population[-elites:] = elite_population\n            fitness[-elites:] = elite_fitness\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 71, "feedback": "The algorithm AdvancedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.17125 with standard deviation 0.00619.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.16360185764258295, 0.17139299232140204, 0.17876386064973526]}}
{"id": "694dff22-0fbe-4fac-8732-6d08fbedb34e", "fitness": 0.1811528993526479, "name": "ImprovedAMESH", "description": "ImprovedAMESH*: Enhances the convergence rate by adjusting mutation scale adaptively, balancing exploration and exploitation effectively.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                # Modification: Increase sigma_init to 0.35 for improved exploration\n                direction += np.random.normal(0, 0.35 * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 72, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18115 with standard deviation 0.00212.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18320881190780058, 0.18201310255209147, 0.17823678359805162]}}
{"id": "9e3071d4-b374-4f66-a05e-a0830e4071f3", "fitness": 0.19058251563264636, "name": "ImprovedAMESH", "description": "ImprovedAMESH with dynamic mutation rate based on diversity and memory adaptation.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.995  # Adjusted adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 73, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19058 with standard deviation 0.00353.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.18818131467246713, 0.18799313710077448, 0.19557309512469745]}}
{"id": "59ebbfcb-38a3-4b22-8f07-abb5e672322d", "fitness": 0.18348266775734204, "name": "ImprovedAMESH", "description": "Adjusts mutation rate using a dynamic threshold to better balance exploration and exploitation, improving convergence efficiency.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.15  # Adjusted threshold for mutation\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 74, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.18348 with standard deviation 0.00523.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.17666920817857978, 0.1844110253217076, 0.18936776977173875]}}
{"id": "2d1d5398-82ae-4e0b-872b-6eaac985d1bc", "fitness": 0.19789921889266315, "name": "ImprovedAMESH", "description": "ImprovedAMESH2: Enhances convergence by increasing the initial mutation rate, promoting diverse exploration for robust solutions.", "code": "import numpy as np\n\nclass ImprovedAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99  # New adaptation parameter for mutation scale\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = 10\n        mutation_rate = 0.3  # Increased initial mutation rate for enhanced exploration\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Adaptive mutation rate based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:population_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 75, "feedback": "The algorithm ImprovedAMESH got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.19790 with standard deviation 0.00527.", "error": "", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {"aucs": [0.19254794424275457, 0.19607845254736633, 0.20507125988786856]}}
{"id": "5897ddf5-3cb5-46e5-a6b8-96bbb44ed187", "fitness": -Infinity, "name": "DynamicAMESH", "description": "DynamicAMESH: Incorporates dynamic population sizing and memory-based crossover to enhance adaptability and improve convergence over diverse optimization landscapes.", "code": "import numpy as np\n\nclass DynamicAMESH:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = 10\n        self.memory = []\n        self.memory_decay_factor = 0.9\n        self.adapt_factor = 0.99\n        self.initial_pop_size = 10\n        self.min_pop_size = 5\n        self.max_pop_size = 20\n        \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population_size = self.initial_pop_size\n        mutation_rate = 0.2\n        sigma_init = 0.3\n        diversity_threshold = 0.1\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.budget -= population_size\n\n        # Initialize memory with best individuals and empty slots\n        self.update_memory(population, fitness)\n        \n        while self.budget > 0:\n            # Dynamic population size based on diversity\n            diversity = np.mean(np.std(population, axis=0))\n            population_size = int(self.min_pop_size + (self.max_pop_size - self.min_pop_size) * (diversity / (diversity + diversity_threshold)))\n            population_size = min(population_size, self.budget)\n\n            # Adaptive mutation rate based on diversity\n            mutation_rate = max(mutation_rate * self.adapt_factor, diversity_threshold / (diversity + 1e-6))\n            \n            offspring = []\n            for rank, parent in enumerate(population):\n                if np.random.rand() < 0.5 or not self.memory:  # Exploration with memory or random direction\n                    if self.memory:\n                        memory_sample = self.memory[np.random.choice(len(self.memory))]\n                        direction = memory_sample - parent\n                    else:\n                        direction = np.random.uniform(-1, 1, self.dim)\n                    # Memory-based crossover\n                    child = parent + 0.5 * direction\n                else:  # Pure exploration\n                    direction = np.random.uniform(-1, 1, self.dim)\n                    child = parent + direction\n                \n                adaptive_mutation_scale = (1 - (rank / population_size)) * mutation_rate\n                direction += np.random.normal(0, sigma_init * adaptive_mutation_scale, self.dim)\n                child = np.clip(parent + direction, lb, ub)\n                offspring.append(child)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring[:population_size]])\n            self.budget -= population_size\n            \n            # Select the best individuals\n            combined_population = np.vstack((population, offspring[:population_size]))\n            combined_fitness = np.hstack((fitness, offspring_fitness))\n            best_indices = np.argsort(combined_fitness)[:self.initial_pop_size]\n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n            \n            # Update memory with best individuals and diversify\n            self.update_memory(population, fitness)\n        \n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]\n    \n    def update_memory(self, population, fitness):\n        best_individuals = population[np.argsort(fitness)[:self.memory_size]]\n        # Apply memory decay factor to reinforce new best individuals\n        self.memory = [(self.memory_decay_factor * old + (1 - self.memory_decay_factor) * new) for old, new in zip(self.memory, best_individuals)]\n        # Fill the memory with new individuals if size is not met\n        if len(self.memory) < self.memory_size:\n            self.memory.extend(best_individuals[:self.memory_size - len(self.memory)])\n        else:\n            self.memory = self.memory[:self.memory_size]", "configspace": "", "generation": 76, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_ids": ["af0ef083-39a1-49af-a2cd-bd35659b60f1"], "operator": null, "metadata": {}}
