{"id": "7c312d25-3bc4-4d80-90fb-1934bd33d0d9", "fitness": 0.673129186062608, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with Dynamic Parameter Control to efficiently explore and exploit the search space for black box optimization.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        F = 0.5  # differential weight\n        CR = 0.9  # crossover probability\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation\n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.7, 1.0)\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.67313 with standard deviation 0.10068.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.7962003519753238, 0.5495795373961827, 0.6736076688163177]}}
{"id": "3a73ded6-c9ca-40d8-854d-2b62c432e053", "fitness": 0.7775283870677353, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution using dynamic crossover and mutation strategies for improved convergence in black box optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.77753 with standard deviation 0.09370.", "error": "", "parent_ids": ["7c312d25-3bc4-4d80-90fb-1934bd33d0d9"], "operator": null, "metadata": {"aucs": [0.7794307648574108, 0.6618292517553119, 0.891325144590483]}}
{"id": "01169226-1427-465a-a7a3-a94409d36577", "fitness": 0.6520944213875817, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive population size adjustment based on convergence rate for improved efficiency in black box optimization.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptive population size based on convergence rate\n            if eval_count % (10 * self.dim) == 0:  # Checkpoint every 10 * dim evaluations\n                pop_size = max(4, pop_size - 1)  # Reduce population size to the minimum of 4 if stagnation is detected\n            \n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.65209 with standard deviation 0.06482.", "error": "", "parent_ids": ["3a73ded6-c9ca-40d8-854d-2b62c432e053"], "operator": null, "metadata": {"aucs": [0.7198417097183606, 0.6717069182271691, 0.5647346362172153]}}
{"id": "9dac118a-856f-4f58-ab8a-5917fd372c53", "fitness": 0.7775283870677353, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Introduced memory-based mutation strategy by leveraging historical best solutions to refine convergence dynamics.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        historical_best = population[np.argmin(fitness)].copy()  # Maintain historical best solution\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use historical best for mutation strategy\n                    mutant = np.clip(x0 + F * (historical_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_index]:\n                        historical_best = trial.copy()  # Update historical best if improved\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.77753 with standard deviation 0.09370.", "error": "", "parent_ids": ["3a73ded6-c9ca-40d8-854d-2b62c432e053"], "operator": null, "metadata": {"aucs": [0.7794307648574108, 0.6618292517553119, 0.891325144590483]}}
{"id": "2c79b240-2200-4e1d-9b13-3f06469e1ed5", "fitness": 0.8183943629289775, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with improved mutation strategy, leveraging the second-best individual for additional diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.81839 with standard deviation 0.03205.", "error": "", "parent_ids": ["3a73ded6-c9ca-40d8-854d-2b62c432e053"], "operator": null, "metadata": {"aucs": [0.7939144291084111, 0.7976034708199061, 0.8636651888586153]}}
{"id": "6c27a7af-cd1c-48b2-9297-e3710b232d7a", "fitness": 0.6182344537998036, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with modified crossover probability adjustment to dynamically increase diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.5, 1.0) + (0.5 - np.mean(fitness)/max(fitness)) # Modified CR\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.61823 with standard deviation 0.05926.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7017143610761257, 0.57011144013498, 0.5828775601883052]}}
{"id": "e840b086-88a5-4d15-892b-e5328a87f2c2", "fitness": 0.76056596388749, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with stochastic crossover probability for improved exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            CR = np.random.uniform(0.1, 1.0)  # Modified line: Changed crossover probability range for exploration\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.76057 with standard deviation 0.07371.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7334364437687961, 0.8613009809502987, 0.6869604669433752]}}
{"id": "4e238c1f-d1fd-498d-9d58-67fa94b27bc8", "fitness": 0.6754187724295603, "name": "DynamicDifferentialEvolution", "description": "Dynamic Differential Evolution with adaptive parameter control using historical success rates for enhanced convergence.", "code": "import numpy as np\n\nclass DynamicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n\n        # Tracking success history for parameter adaptation\n        success_history = []\n\n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters based on historical success\n            if success_history:\n                F = np.clip(0.5 + 0.3 * np.mean(success_history), 0.4, 0.9)\n                CR = np.clip(0.6 + 0.2 * np.mean(success_history), 0.5, 1.0)\n            else:\n                F = np.random.uniform(0.4, 0.9)\n                CR = np.random.uniform(0.5, 1.0)\n\n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1])  # second-best individual\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    success_history.append(1)\n                    population[i] = trial\n                    fitness[i] = f_trial\n                else:\n                    success_history.append(0)\n\n                # Maintain a fixed size for success history\n                if len(success_history) > 100:\n                    success_history.pop(0)\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 7, "feedback": "The algorithm DynamicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.67542 with standard deviation 0.22116.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.3649876576101596, 0.7976034708199061, 0.8636651888586153]}}
{"id": "b07e57de-b681-4e2c-9ce6-e896a0085885", "fitness": 0.7271563774071913, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced parameter adaptation by dynamically adjusting the mutation factor (F) based on population diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9) * (np.std(fitness) / np.mean(fitness))  # Adjust mutation factor based on diversity\n            CR = np.random.uniform(0.5, 1.0)\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.72716 with standard deviation 0.01136.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7193416995091598, 0.7189018926658048, 0.7432255400466092]}}
{"id": "bb27ca2d-6cc7-475e-8875-ed7cbefd1e14", "fitness": 0.635638512482024, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with adaptive crossover probability based on fitness variance for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize parameters\n        pop_size = 10 * self.dim\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        \n        # Initialize population\n        population = np.random.rand(pop_size, self.dim)\n        for i in range(self.dim):\n            population[:, i] = bounds[i, 0] + population[:, i] * (bounds[i, 1] - bounds[i, 0])\n        \n        # Evaluate initial population\n        fitness = np.array([func(ind) for ind in population])\n        eval_count = pop_size\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Adaptation of parameters\n            F = np.random.uniform(0.4, 0.9)\n            \n            # New line: Adaptive CR based on fitness variance\n            CR = np.random.uniform(0.3, 1.0 - (np.std(fitness) / np.mean(fitness)))\n            \n            for i in range(pop_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation with dynamic strategy based on current best fitness\n                best_index = np.argmin(fitness)\n                second_best_index = (fitness.argsort()[1]) # Identify second-best individual for mutation strategy\n                x_best = population[best_index]\n                x_second_best = population[second_best_index]\n                \n                indices = np.random.choice(pop_size, 3, replace=False)\n                x0, x1, x2 = population[indices]\n                if np.random.rand() < 0.7:\n                    # Use current best for mutation strategy\n                    mutant = np.clip(x0 + F * (x_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                else:\n                    # New line: Use second best for additional diversity\n                    mutant = np.clip(x0 + F * (x_second_best - x0) + F * (x1 - x2), bounds[:, 0], bounds[:, 1])\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n        # Best solution\n        best_index = np.argmin(fitness)\n        return population[best_index]", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.63564 with standard deviation 0.18795.", "error": "", "parent_ids": ["2c79b240-2200-4e1d-9b13-3f06469e1ed5"], "operator": null, "metadata": {"aucs": [0.7474090797825712, 0.7886042443201258, 0.3709022133433749]}}
